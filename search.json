[
  {
    "objectID": "ARAD1K_HSCNN_Compression.html",
    "href": "ARAD1K_HSCNN_Compression.html",
    "title": "hsicomp",
    "section": "",
    "text": "import warnings\nwarnings.filterwarnings('ignore')\n\nfrom fastai.vision.all import *\nfrom fasterbench.benchmark import evaluate_cpu_speed, get_model_size, get_num_parameters\nimport torch.nn as nn\nimport torch\nclass dfus_block(nn.Module):\n    def __init__(self, dim):\n        super(dfus_block, self).__init__()\n        self.conv1 = nn.Conv2d(dim, 128, 1, 1, 0, bias=False)\n\n        self.conv_up1 = nn.Conv2d(128, 32, 3, 1, 1, bias=False)\n        self.conv_up2 = nn.Conv2d(32, 16, 1, 1, 0, bias=False)\n\n        self.conv_down1 = nn.Conv2d(128, 32, 3, 1, 1, bias=False)\n        self.conv_down2 = nn.Conv2d(32, 16, 1, 1, 0, bias=False)\n\n        self.conv_fution = nn.Conv2d(96, 32, 1, 1, 0, bias=False)\n\n        #### activation function\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        \"\"\"\n        x: [b,c,h,w]\n        return out:[b,c,h,w]\n        \"\"\"\n        feat = self.relu(self.conv1(x))\n        feat_up1 = self.relu(self.conv_up1(feat))\n        feat_up2 = self.relu(self.conv_up2(feat_up1))\n        feat_down1 = self.relu(self.conv_down1(feat))\n        feat_down2 = self.relu(self.conv_down2(feat_down1))\n        feat_fution = torch.cat([feat_up1,feat_up2,feat_down1,feat_down2],dim=1)\n        feat_fution = self.relu(self.conv_fution(feat_fution))\n        out = torch.cat([x, feat_fution], dim=1)\n        return out\n\nclass ddfn(nn.Module):\n    def __init__(self, dim, num_blocks=78):\n        super(ddfn, self).__init__()\n\n        self.conv_up1 = nn.Conv2d(dim, 32, 3, 1, 1, bias=False)\n        self.conv_up2 = nn.Conv2d(32, 32, 1, 1, 0, bias=False)\n\n        self.conv_down1 = nn.Conv2d(dim, 32, 3, 1, 1, bias=False)\n        self.conv_down2 = nn.Conv2d(32, 32, 1, 1, 0, bias=False)\n\n        dfus_blocks = [dfus_block(dim=128+32*i) for i in range(num_blocks)]\n        self.dfus_blocks = nn.Sequential(*dfus_blocks)\n\n        #### activation function\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        \"\"\"\n        x: [b,c,h,w]\n        return out:[b,c,h,w]\n        \"\"\"\n        feat_up1 = self.relu(self.conv_up1(x))\n        feat_up2 = self.relu(self.conv_up2(feat_up1))\n        feat_down1 = self.relu(self.conv_down1(x))\n        feat_down2 = self.relu(self.conv_down2(feat_down1))\n        feat_fution = torch.cat([feat_up1,feat_up2,feat_down1,feat_down2],dim=1)\n        out = self.dfus_blocks(feat_fution)\n        return out\n\nclass HSCNN_Plus(nn.Module):\n    def __init__(self, in_channels=3, out_channels=31, num_blocks=30):\n        super(HSCNN_Plus, self).__init__()\n\n        self.ddfn = ddfn(dim=in_channels, num_blocks=num_blocks)\n        self.conv_out = nn.Conv2d(128+32*num_blocks, out_channels, 1, 1, 0, bias=False)\n\n    def forward(self, x):\n        \"\"\"\n        x: [b,c,h,w]\n        return out:[b,c,h,w]\n        \"\"\"\n        fea = self.ddfn(x)\n        out =  self.conv_out(fea)\n        return out\n# def get_dls(size, bs):\n#     path = URLs.IMAGENETTE_160\n#     source = untar_data(path)\n#     blocks=(ImageBlock, CategoryBlock)\n#     tfms = [RandomResizedCrop(size, min_scale=0.35), FlipItem(0.5)]\n#     batch_tfms = [Normalize.from_stats(*imagenet_stats)]\n\n#     csv_file = 'noisy_imagenette.csv'\n#     inp = pd.read_csv(source/csv_file)\n#     dblock = DataBlock(blocks=blocks,\n#                splitter=ColSplitter(),\n#                get_x=ColReader('path', pref=source),\n#                get_y=ColReader(f'noisy_labels_0'),\n#                item_tfms=tfms,\n#                batch_tfms=batch_tfms)\n\n#     return dblock.dataloaders(inp, path=source, bs=bs)\n# size, bs = 128, 32\n# dls = get_dls(size, bs)\nmodel_path='/root/Ninjalabo/HSI/MST-plus-plus/MST-plus-plus/test_develop_code/model_zoo/hscnn_plus.pth'\ndata_root= '/root/Ninjalabo/HSI/MST-plus-plus/MST-plus-plus/dataset/'\n# model_path = Path('/root/Ninjalabo/HSI/MST-plus-plus/MST-plus-plus/test_challenge_code/model_zoo/hscnn_plus.pth')\n\n# path = '/root/Ninjalabo/HSI/MST-plus-plus/MST-plus-plus/dataset/Train_RGB/'\nfrom fastai.vision.all import *\nfrom pathlib import Path\nimport torch\n\n# Set your dataset path\npath = Path('/root/Ninjalabo/HSI/MST-plus-plus/MST-plus-plus/dataset/')\nval_path = path / 'Test_RGB'  # Adjust based on your folder structure\n# from fastai.vision.all import *\n\n# Define the path to your dataset\npath = Path('/root/Ninjalabo/HSI/MST-plus-plus/MST-plus-plus/dataset/')  # Set this to your validation data folder\n\n# DataBlock for image-to-image tasks\ndata_block = DataBlock(\n    blocks=(ImageBlock, ImageBlock),  # Both input and output are images\n    get_items=get_image_files,  # Gets the image files\n    get_x=lambda f: PILImage.create(f),  # Use image as input\n    get_y=lambda f: PILImage.create(f),  # Use the same image as output\n    splitter=RandomSplitter(valid_pct=0.2),  # Split for training/validation (adjust as needed)\n    item_tfms=Resize(64),  # Resize transformation, adjust as per your requirement\n)\n\n# Create DataLoaders\ndls = data_block.dataloaders(path, bs=1)  # Adjust batch size based on memory limits\n# #| eval: false\n# dls = data_block.dataloaders(val_path, bs=5)  # Use the appropriate batch size\n# dls.show_batch()\n# Grab a batch from the training DataLoader\nx, y = dls.one_batch()\n\n# Check the shape of inputs and outputs\nprint(\"Input (x) shape:\", x.shape)\nprint(\"Target (y) shape:\", y.shape)\n\nInput (x) shape: torch.Size([5, 3, 64, 64])\nTarget (y) shape: torch.Size([5, 3, 64, 64])\n# # | eval: false\n# model = HSCNN_Plus()\n# checkpoint = torch.load(model_path)\n# if 'state_dict' in checkpoint:\n#     model.load_state_dict(checkpoint['state_dict'])\n# else:\n#     model.load_state_dict(checkpoint)\n# model.eval()\n# print(model)\n# print(torch.load(model_path).keys())\nmodel = HSCNN_Plus()  # Initialize your custom model\n# Load model checkpoint\ncheckpoint = torch.load(model_path, map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\nif 'state_dict' in checkpoint:\n    model.load_state_dict(checkpoint['state_dict'])\nelse:\n    model.load_state_dict(checkpoint)\n\nmodel.eval()  # Set to evaluation mode (good practice for inference)\n\nHSCNN_Plus(\n  (ddfn): ddfn(\n    (conv_up1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (conv_up2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (conv_down1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (conv_down2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (dfus_blocks): Sequential(\n      (0): dfus_block(\n        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_up1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_up2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_down1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_down2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_fution): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (relu): ReLU(inplace=True)\n      )\n      (1): dfus_block(\n        (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_up1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_up2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_down1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_down2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_fution): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (relu): ReLU(inplace=True)\n      )\n      (2): dfus_block(\n        (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_up1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_up2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_down1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_down2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_fution): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (relu): ReLU(inplace=True)\n      )\n      (3): dfus_block(\n        (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_up1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_up2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_down1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_down2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_fution): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (relu): ReLU(inplace=True)\n      )\n      (4): dfus_block(\n        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_up1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_up2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_down1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_down2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_fution): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (relu): ReLU(inplace=True)\n      )\n      (5): dfus_block(\n        (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_up1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_up2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_down1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_down2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_fution): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (relu): ReLU(inplace=True)\n      )\n      (6): dfus_block(\n        (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_up1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_up2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_down1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_down2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_fution): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (relu): ReLU(inplace=True)\n      )\n      (7): dfus_block(\n        (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_up1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_up2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_down1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_down2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_fution): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (relu): ReLU(inplace=True)\n      )\n      (8): dfus_block(\n        (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_up1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_up2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_down1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_down2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_fution): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (relu): ReLU(inplace=True)\n      )\n      (9): dfus_block(\n        (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_up1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_up2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_down1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_down2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_fution): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (relu): ReLU(inplace=True)\n      )\n      (10): dfus_block(\n        (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_up1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_up2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_down1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_down2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_fution): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (relu): ReLU(inplace=True)\n      )\n      (11): dfus_block(\n        (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_up1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_up2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_down1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_down2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_fution): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (relu): ReLU(inplace=True)\n      )\n      (12): dfus_block(\n        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_up1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_up2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_down1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_down2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_fution): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (relu): ReLU(inplace=True)\n      )\n      (13): dfus_block(\n        (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_up1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_up2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_down1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_down2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_fution): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (relu): ReLU(inplace=True)\n      )\n      (14): dfus_block(\n        (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_up1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_up2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_down1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_down2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_fution): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (relu): ReLU(inplace=True)\n      )\n      (15): dfus_block(\n        (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_up1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_up2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_down1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_down2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_fution): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (relu): ReLU(inplace=True)\n      )\n      (16): dfus_block(\n        (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_up1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_up2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_down1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_down2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_fution): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (relu): ReLU(inplace=True)\n      )\n      (17): dfus_block(\n        (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_up1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_up2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_down1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_down2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_fution): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (relu): ReLU(inplace=True)\n      )\n      (18): dfus_block(\n        (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_up1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_up2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_down1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_down2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_fution): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (relu): ReLU(inplace=True)\n      )\n      (19): dfus_block(\n        (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_up1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_up2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_down1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_down2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_fution): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (relu): ReLU(inplace=True)\n      )\n      (20): dfus_block(\n        (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_up1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_up2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_down1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_down2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_fution): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (relu): ReLU(inplace=True)\n      )\n      (21): dfus_block(\n        (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_up1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_up2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_down1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_down2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_fution): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (relu): ReLU(inplace=True)\n      )\n      (22): dfus_block(\n        (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_up1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_up2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_down1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_down2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_fution): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (relu): ReLU(inplace=True)\n      )\n      (23): dfus_block(\n        (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_up1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_up2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_down1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_down2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_fution): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (relu): ReLU(inplace=True)\n      )\n      (24): dfus_block(\n        (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_up1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_up2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_down1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_down2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_fution): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (relu): ReLU(inplace=True)\n      )\n      (25): dfus_block(\n        (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_up1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_up2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_down1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_down2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_fution): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (relu): ReLU(inplace=True)\n      )\n      (26): dfus_block(\n        (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_up1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_up2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_down1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_down2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_fution): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (relu): ReLU(inplace=True)\n      )\n      (27): dfus_block(\n        (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_up1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_up2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_down1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_down2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_fution): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (relu): ReLU(inplace=True)\n      )\n      (28): dfus_block(\n        (conv1): Conv2d(1024, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_up1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_up2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_down1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_down2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_fution): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (relu): ReLU(inplace=True)\n      )\n      (29): dfus_block(\n        (conv1): Conv2d(1056, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_up1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_up2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_down1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_down2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_fution): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (relu): ReLU(inplace=True)\n      )\n    )\n    (relu): ReLU(inplace=True)\n  )\n  (conv_out): Conv2d(1088, 31, kernel_size=(1, 1), stride=(1, 1), bias=False)\n)\nfrom torch.nn import MSELoss\n\n# Create the Learner with MSE Loss\nlearn = Learner(dls, model, loss_func=MSELoss())\nfrom torch.nn import MSELoss\n# Train or fine-tune the model (optional)\nmodel = HSCNN_Plus(in_channels=3, out_channels=3, num_blocks=5)  # Reduce num_blocks significantly\nlearn = Learner(dls, model.to('cpu'), loss_func=MSELoss())\n\nlearn.fit_one_cycle(5, lr_max=1e-4)\n# learn.fit_one_cycle(4, 1e-4)\n# Run inference on validation set\n# preds, targs = learn.get_preds(dl=dls.valid)  # Get predictions\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.047585\n0.043253\n00:00\n\n\n1\n0.045987\n0.039191\n00:00\n\n\n2\n0.043653\n0.035421\n00:00\n\n\n3\n0.041533\n0.033387\n00:00\n\n\n4\n0.039822\n0.032988\n00:00\n# from fastai.callback.all import GradientAccumulation\n\n# # Set gradient accumulation steps to effectively multiply your batch size by this factor\n# accumulation_steps = 8  # Adjust based on your needs and memory constraints\n\n# # Create the Learner with gradient accumulation and mixed precision\n# learn = Learner(dls, model, loss_func=MSELoss(), cbs=[GradientAccumulation(n_acc=accumulation_steps)]).to_fp16()\n# learn.fit_one_cycle(5, lr_max=1e-4)\n# files = get_image_files(path)\n\n# def label_func(f): return f[0].isupper()\n\n# dls = ImageDataLoaders.from_name_func(path, files, label_func, item_tfms=Resize(128),bs=32)\nimport torch\ntorch.cuda.empty_cache()\nimport os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\nfrom torch.nn import MSELoss  # Use Mean Squared Error as loss function for image-to-image tasks\n\n# Define the Learner with MSE loss\nlearn = Learner(dls, model, loss_func=MSELoss())\n# learn = Learner(dls, model, metrics=[accuracy])\nnum_parameters = get_num_parameters(learn.model)\ndisk_size = get_model_size(learn.model)\nprint(f\"Model Size: {disk_size / 1e6:.2f} MB (disk), {num_parameters} parameters\")\n\nModel Size: 2.08 MB (disk), 516640 parameters\nmodel = learn.model.eval().to('cpu')\nx,y = dls.one_batch()\nprint(f'Inference Speed: {evaluate_cpu_speed(learn.model, x[0][None])[0]:.2f}ms')\n\nInference Speed: 15.21ms\nx, y = dls.one_batch()\nprint(\"Input Shape:\", x.shape)\nprint(\"Target Shape:\", y.shape)\n\nInput Shape: torch.Size([5, 3, 64, 64])\nTarget Shape: torch.Size([5, 3, 64, 64])",
    "crumbs": [
      "**Knowledge Distillation**"
    ]
  },
  {
    "objectID": "ARAD1K_HSCNN_Compression.html#knowledge-distillation",
    "href": "ARAD1K_HSCNN_Compression.html#knowledge-distillation",
    "title": "hsicomp",
    "section": "Knowledge Distillation",
    "text": "Knowledge Distillation\n\n\n KnowledgeDistillation(teacher.model, loss) \n\n You only need to give to the callback function your teacher learner. Behind the scenes, FasterAI will take care of making your model train using knowledge distillation. \n\n\n\n\nfrom fasterai.distill.all import *\n\n\nimport torch\n\ntorch.cuda.empty_cache()\n\n\nimport os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\n\n# sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6  # Total trainable parameters in millions\n\n\n# import torch\n\n# print(f\"Allocated memory: {torch.cuda.memory_allocated() / 1024 ** 2:.2f} MB\")\n# print(f\"Cached memory: {torch.cuda.memory_reserved() / 1024 ** 2:.2f} MB\")\n\n\n# !nvidia-smi\n\n\n# !kill -9 58089\n\n\nfrom torch.nn import MSELoss\n# Train or fine-tune the model (optional)\nmodel = HSCNN_Plus(in_channels=3, out_channels=3, num_blocks=5)  # Reduce num_blocks significantly\nteacher = Learner(dls, model.to('cpu'), loss_func=MSELoss())\n\nteacher.fit_one_cycle(10, lr_max=1e-4)\n# learn.fit_one_cycle(4, 1e-4)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.042094\n0.039655\n00:00\n\n\n1\n0.041489\n0.037667\n00:00\n\n\n2\n0.040237\n0.034148\n00:00\n\n\n3\n0.038269\n0.029985\n00:00\n\n\n4\n0.035919\n0.025685\n00:00\n\n\n5\n0.033408\n0.021808\n00:00\n\n\n6\n0.030831\n0.018923\n00:00\n\n\n7\n0.028445\n0.017224\n00:00\n\n\n8\n0.026424\n0.016530\n00:00\n\n\n9\n0.024772\n0.016417\n00:00\n\n\n\n\n\n\nfrom fastai.vision.all import *\nfrom fastai.callback.all import *\nfrom fastai.vision.models.unet import DynamicUnet\nfrom torchvision.models import resnet18\n\n# Step 1: Define the student model with Tiny U-Net structure\n# Use only the feature layers (up to the last convolution) of ResNet-18 as the encoder\nencoder = nn.Sequential(*list(resnet18(pretrained=True).children())[:-2])  # Remove the last fully connected layers\nstudent_model = DynamicUnet(encoder, n_out=3, img_size=(64, 64))  # Match output channels for your task\n\n# Step 2: Define the Learner for the student model\n# Set a suitable loss function for image-to-image tasks like MSELoss\nstudent = Learner(\n    dls, \n    student_model, \n    loss_func=MSELoss()#, \n    # metrics=[PSNR()]  # PSNR (Peak Signal-to-Noise Ratio) can be useful for image quality\n)\n\n# Step 3: Initialize the KnowledgeDistillationCallback\n# Assuming `teacher` is the pre-trained HSCNN_Plus model\nkd_cb = KnowledgeDistillationCallback(teacher.model, SoftTarget)\n\n# Step 4: Train the student model with knowledge distillation\nstudent.fit_one_cycle(10, 1e-4, cbs=kd_cb)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n206.853897\n4.708280\n00:01\n\n\n1\n117.306244\n4.507239\n00:01\n\n\n2\n78.637085\n3.295578\n00:01\n\n\n3\n57.473248\n2.667634\n00:01\n\n\n4\n44.227245\n3.118005\n00:01\n\n\n5\n35.264557\n3.569392\n00:01\n\n\n6\n28.856447\n3.880894\n00:01\n\n\n7\n24.112537\n4.008141\n00:01\n\n\n8\n20.488949\n4.021497\n00:01\n\n\n9\n17.672688\n4.249494\n00:01\n\n\n\n\n\n\nnum_parameters = get_num_parameters(student.model)\ndisk_size = get_model_size(student.model)\nprint(f\"Model Size: {disk_size / 1e6:.2f} MB (disk), {num_parameters} parameters\")\n\nModel Size: 124.56 MB (disk), 31113108 parameters",
    "crumbs": [
      "**Knowledge Distillation**"
    ]
  },
  {
    "objectID": "ARAD1K_HSCNN_Compression.html#quantization",
    "href": "ARAD1K_HSCNN_Compression.html#quantization",
    "title": "hsicomp",
    "section": "Quantization",
    "text": "Quantization\n\nfrom fasterai.quantize.quantize_callback import *\n\n\nteacher.fit_one_cycle(5, 1e-5, cbs=QuantizeCallback())\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.018622\n0.017505\n00:01\n\n\n1\n0.018870\n0.017797\n00:00\n\n\n2\n0.019117\n0.018241\n00:00\n\n\n3\n0.019400\n0.018464\n00:00\n\n\n4\n0.019566\n0.018610\n00:00\n\n\n\n\n\n\nprint(f'Inference Speed: {evaluate_cpu_speed(teacher.model, x[0][None])[0]:.2f}ms')\n\nInference Speed: 11.60ms\n\n\n\ndef count_parameters_quantized(model):\n    total_params = 0\n    for module in model.modules():\n        if isinstance(module, torch.nn.modules.conv.Conv2d) or \\\n           isinstance(module, torch.nn.Linear) or \\\n           isinstance(module, torch.ao.nn.quantized.modules.conv.Conv2d) or \\\n           isinstance(module, torch.ao.nn.quantized.modules.linear.Linear):\n            \n            total_params += module.weight().numel()\n            \n            if module.bias() is not None:\n                total_params += module.bias().numel()\n    return total_params\n\n\nnum_parameters = count_parameters_quantized(teacher.model)\ndisk_size = get_model_size(teacher.model)\nprint(f\"Model Size: {disk_size / 1e6:.2f} MB (disk), {num_parameters:,} parameters\")\n\nModel Size: 0.59 MB (disk), 514,976 parameters",
    "crumbs": [
      "**Knowledge Distillation**"
    ]
  },
  {
    "objectID": "Paper.html",
    "href": "Paper.html",
    "title": "Leveraging Deep Neural Network Compression Techniques for Real-Time Hyperspectral Image Processing in Edge AI",
    "section": "",
    "text": "Authors: Dheeraj Kumar, Leila Mozaffari\n\nAbstract\nHyperspectral imaging has become increasingly relevant in remote sensing applications due to its ability to capture detailed spectral information. The growth of hyperspectral datasets has necessitated efficient compression techniques to handle their massive storage requirements. This paper proposes a new approach that integrates the Spectral Signals Compressor Network (SSCNet), a state-of-the-art deep learning-based hyperspectral image compression model, with FasterAI pruning to improve compression efficiency. We used HySpecNet-11k, a large-scale hyperspectral benchmark dataset, to validate our approach. The results show significant improvements in compression rates while preserving high-quality image reconstruction, thus setting a new benchmark for learning-based hyperspectral image compression.\nModel compression is a key technique used to reduce the memory footprint and computational demands of deep learning models, making them more suitable for deployment on resource-constrained devices. Model compression can be achieved through various techniques such as pruning, knowledge distillation, and quantization. Pruning, which is one such model compression technique, involves systematically removing unimportant weights or neurons from a network to reduce model size and computational complexity while maintaining acceptable levels of accuracy. In this paper, we implemented pruning from FasterAI to enhance SSCNet, making it more efficient without significantly compromising performance. We compare the original model and the pruned model to evaluate the benefits and impact of model compression.\n\n\n\n1. Introduction\nRecent advancements in hyperspectral imaging technology have led to the proliferation of hyperspectral data, significantly improving the identification and discrimination of materials in remote sensing applications. However, the resulting large volume of data necessitates efficient storage and transmission, driving research in hyperspectral image compression. Traditional approaches, which often combine transform coding and quantization techniques, face challenges when dealing with high-dimensional data. To address these limitations, learning-based methods leveraging convolutional autoencoders have emerged as promising alternatives for hyperspectral image compression [5].\nIn this paper, we present an improved hyperspectral image compression method by applying pruning using FasterAI on SSCNet [4], a convolutional autoencoder-based architecture. We leverage the HySpecNet-11k dataset [2], which provides a robust benchmark for training and evaluating learning-based hyperspectral image compression models.\n\n\n2. Methodology\n\n2.1 HySpecNet-11k Dataset\nThe HySpecNet-11k dataset was designed specifically for benchmarking learning-based hyperspectral image compression methods [5]. It consists of 11,483 non-overlapping image patches extracted from 250 EnMAP tiles, each patch containing 128×128 pixels with 224 spectral bands, and a ground sampling distance of 30m. These image patches provide a large-scale and diverse set of hyperspectral data acquired in spatially disjoint geographical regions, addressing the limitations of earlier datasets which often suffered from overfitting due to spatial similarity among samples.\nFor our experiments, we used the “easy split” setup for the dataset, where 70% of the image patches were used for training, 20% for validation, and 10% for testing. This ensured reproducibility and allowed for a comprehensive comparison with existing learning-based hyperspectral image compression techniques.\n\n\n2.2 Spectral Signals Compressor Network (SSCNet)\nSSCNet is a deep learning-based compression model that utilizes 2D convolutional layers with parametric ReLU activations for both spatial and spectral compression [4]. The encoder network employs 2D convolutions and three max pooling layers to achieve a fixed spatial compression factor of 64. The decoder mirrors the encoder, with upsampling achieved through transposed convolutional layers.\nThe compression ratio (CR) is set by the number of latent channels in the bottleneck layer, while the network’s ability to reconstruct the original image with minimal distortion is evaluated through Peak Signal-to-Noise Ratio (PSNR). SSCNet was chosen for this study due to its effectiveness in combining both spatial and spectral compression in hyperspectral datasets.\n\n\n2.3 FasterAI Pruning\nTo enhance the performance of SSCNet, we implemented structured pruning from FasterAI, which systematically removes less important filters or neurons in the model’s layers. Pruning is particularly effective in reducing computational complexity and the memory footprint of deep learning models without significant degradation in accuracy.\nIn our work, we applied pruning during the fine-tuning phase, after training the SSCNet model. The pruning process involved identifying and removing low-magnitude weights or redundant neurons, followed by retraining the model to recover performance. This iterative process ensured that the pruned model maintained high reconstruction quality while achieving substantial reductions in model size and inference time.\n\n\n\n3. Experimental Setup\nThe experimental setup used PyTorch as the deep learning framework. Gradient clipping and min-max normalization were applied to scale input data within a range suitable for learning-based models. The Adam optimizer was used with an initial learning rate of \\(1e−4\\), which was decreased as training progressed to achieve convergence.\nOur experiments were conducted on a high-performance computing environment, featuring an NVIDIA L4 Tensor Core GPU with 24 GB memory. We tested the SSCNet model for 5 epochs, followed by pruning and fine-tuning for an additional 3 epochs.\n\n\n\n4. Results and Discussion\n\n4.1 Compression and Quality Analysis\nWe evaluated the performance of our approach using the rate-distortion curve, measuring the trade-off between bits-per-pixel per channel (bpppc) and PSNR. Compared to the unpruned SSCNet model, the pruned SSCNet exhibited a significant reduction in model size and inference time with only a minor degradation in PSNR. At a compression rate of 2.53 bpppc, the pruned SSCNet achieved a PSNR of 42.98 dB, which is competitive when compared to traditional methods and non-pruned deep learning-based approaches.\nFurthermore, pruning reduced the model size by approximately 45% and computational complexity by 50%, making the approach highly suitable for real-time remote sensing applications with constrained computational resources.\n\n\n\n4.2 Comparative Analysis\nTo provide a comprehensive benchmark, we compared our pruned SSCNet model against other leading hyperspectral compression methods. Our proposed approach outperformed these methods in terms of compression efficiency and inference speed while achieving comparable PSNR values. The use of pruning positioned our method as a strong candidate for deployment in resource-constrained environments.\n\n\n\n5. Conclusion\nThis paper introduces a novel compression approach that combines the power of SSCNet for hyperspectral image compression with FasterAI’s pruning technique. The results show that this combined approach effectively reduces the memory footprint and computational requirements, making it suitable for real-time remote sensing applications while maintaining high reconstruction quality. Future work includes extending this approach to other hyperspectral datasets, exploring dynamic pruning strategies, and integrating additional compression techniques such as quantization for further performance improvements.\n\n\nAcknowledgements\nWe also acknowledge the use of the HySpecNet-11k dataset and the Fasterai framework in conducting our experiments.\n\n\nReferences\n\n[1] M. H. P. Fuchs and B. Demir, “Hyspecnet-11k: A large-scale hyperspectral dataset for benchmarking learning-based hyperspectral image compression methods,” in IGARSS 2023-2023 IEEE International Geoscience and Remote Sensing Symposium, IEEE, 2023, pp. 1779–1782.\n[2] M. H. P. Fuchs and B. Demir, “HySpecNet-11k: A large-scale hyperspectral benchmark dataset.” Dryad, p. 63608947808 bytes, Jun. 26, 2023. doi: 10.5061/DRYAD.FTTDZ08ZH.\n[3] R. La Grassa, C. Re, G. Cremonese, and I. Gallo, “Hyperspectral data compression using fully convolutional autoencoder,” Remote Sensing, vol. 14, no. 10, p. 2472, 2022.\n[4] “FasterAI,” fasterai. Available: https://nathanhubens.github.io/fasterai/\n[5] Fuchs, M. H. P., & Demir, B. (2023). HySpecNet-11k: A Large-Scale Hyperspectral Dataset for Benchmarking Learning-Based Hyperspectral Image Compression Methods. arXiv preprint arXiv:2306.00385v2.",
    "crumbs": [
      "Leveraging Deep Neural Network Compression Techniques for Real-Time Hyperspectral Image Processing in Edge AI"
    ]
  },
  {
    "objectID": "Poster.html",
    "href": "Poster.html",
    "title": "Leveraging Deep Neural Network Compression Techniques for Real-Time Hyperspectral Image Processing in Edge AI",
    "section": "",
    "text": "Authors: Dheeraj Kumar, Leila Mozaffari",
    "crumbs": [
      "Leveraging Deep Neural Network Compression Techniques for Real-Time Hyperspectral Image Processing in Edge AI"
    ]
  },
  {
    "objectID": "Poster.html#introduction",
    "href": "Poster.html#introduction",
    "title": "Leveraging Deep Neural Network Compression Techniques for Real-Time Hyperspectral Image Processing in Edge AI",
    "section": "Introduction",
    "text": "Introduction\n\nObjective\nTo improve hyperspectral image analysis by integrating SSCNet [1] with the FasterAI [4] compression technique, demonstrating efficiency and performance on the HySpecNet-11k dataset. Hyperspectral imaging provides rich spectral information across numerous bands, supporting applications like remote sensing, agriculture, and medical imaging. However, the high volume and computational demands of hyperspectral data necessitate innovative compression and processing techniques.\n\n\nChallenges\n\nManaging large-scale hyperspectral datasets.\nBalancing reconstruction quality and compression efficiency.\n\n\n\nContributions\n\nIntegration of SSCNet with FasterAI pruning for compressing HSI compression model. to reduce model size while preserving high-quality image reconstruction.\nValidated on HySpecNet-11k, a large-scale hyperspectral benchmark dataset.\nAchieved significant reduction in model size and computational load with minor performance trade-offs.",
    "crumbs": [
      "Leveraging Deep Neural Network Compression Techniques for Real-Time Hyperspectral Image Processing in Edge AI"
    ]
  },
  {
    "objectID": "Poster.html#methodology",
    "href": "Poster.html#methodology",
    "title": "Leveraging Deep Neural Network Compression Techniques for Real-Time Hyperspectral Image Processing in Edge AI",
    "section": "Methodology",
    "text": "Methodology\n\nDataset: HySpecNet-11k\nHySpecNet-11k [2] is a large-scale hyperspectral dataset containing 11,483 image patches (128×128 pixels with 224 spectral bands) derived from e Environmental Mapping and Analysis Program (EnMAP) satellite data. It is designed for benchmarking learning-based compression and analysis methods.\n\nDataset Splits: Training (70%), Validation (20%), Test (10%).\nPreprocessing: Removed water vapor-affected bands, applied normalization, and used both patchwise and tilewise splits.\n\n\n\nModel: Spectral Signals Compressor Network (SSCNet)\nSSCNet [3] uses 2D convolutions to compress spatial dimensions while preserving spectral integrity.\n\nEncoder: Three 2D convolutional layers with parametric ReLU activation and max-pooling.\nDecoder: Uses transposed convolutions for reconstruction.\nCompression Ratio (CR): Defined by latent channels in bottleneck layer.\n\n\n\nFasterAI Pruning Compression Technique\n\nRemove redundant weights or neurons.\nFine-tune to recover performance.\nOutcome: Smaller, faster model with minimal accuracy loss.",
    "crumbs": [
      "Leveraging Deep Neural Network Compression Techniques for Real-Time Hyperspectral Image Processing in Edge AI"
    ]
  },
  {
    "objectID": "Poster.html#experimental-results",
    "href": "Poster.html#experimental-results",
    "title": "Leveraging Deep Neural Network Compression Techniques for Real-Time Hyperspectral Image Processing in Edge AI",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nMetric: Bits-per-pixel per channel (bpppc) vs. Peak Signal-to-Noise Ratio (PSNR).\nPruned SSCNet achieved a PSNR of 42.98 dB at 2.53 bpppc.\nReduced model size by 45% and computational complexity by 50%.\n\n\n\nComparative Analysis\n\nOutperformed traditional and learning-based methods in compression efficiency and speed.\nVisuals demonstrate minimal loss of fidelity in reconstructed hyperspectral images.",
    "crumbs": [
      "Leveraging Deep Neural Network Compression Techniques for Real-Time Hyperspectral Image Processing in Edge AI"
    ]
  },
  {
    "objectID": "Poster.html#conclusion",
    "href": "Poster.html#conclusion",
    "title": "Leveraging Deep Neural Network Compression Techniques for Real-Time Hyperspectral Image Processing in Edge AI",
    "section": "Conclusion",
    "text": "Conclusion\n\nEffective reduction in memory footprint and computational demands for real-time edge AI deployment.\nEnables practical deployment of hyperspectral models in resource-constrained environments.\nSupports scalable analysis for large datasets like HySpecNet-11k.",
    "crumbs": [
      "Leveraging Deep Neural Network Compression Techniques for Real-Time Hyperspectral Image Processing in Edge AI"
    ]
  },
  {
    "objectID": "Poster.html#future-work",
    "href": "Poster.html#future-work",
    "title": "Leveraging Deep Neural Network Compression Techniques for Real-Time Hyperspectral Image Processing in Edge AI",
    "section": "Future Work",
    "text": "Future Work\n\nTest FasterAI compression on additional hyperspectral models.\nExplore dynamic pruning strategies.\nApply other model compression techniques.",
    "crumbs": [
      "Leveraging Deep Neural Network Compression Techniques for Real-Time Hyperspectral Image Processing in Edge AI"
    ]
  },
  {
    "objectID": "Poster.html#references",
    "href": "Poster.html#references",
    "title": "Leveraging Deep Neural Network Compression Techniques for Real-Time Hyperspectral Image Processing in Edge AI",
    "section": "References",
    "text": "References\n\n[1] M. H. P. Fuchs and B. Demir, “Hyspecnet-11k: A large-scale hyperspectral dataset for benchmarking learning-based hyperspectral image compression methods,” in IGARSS 2023-2023 IEEE International Geoscience and Remote Sensing Symposium, IEEE, 2023, pp. 1779–1782.\n[2] M. H. P. Fuchs and B. Demir, “HySpecNet-11k: A large-scale hyperspectral benchmark dataset.” Dryad, p. 63608947808 bytes, Jun. 26, 2023. doi: 10.5061/DRYAD.FTTDZ08ZH.\n[3] R. La Grassa, C. Re, G. Cremonese, and I. Gallo, “Hyperspectral data compression using fully convolutional autoencoder,” Remote Sensing, vol. 14, no. 10, p. 2472, 2022.\n[4] “FasterAI,” fasterai. Available: https://nathanhubens.github.io/fasterai/\n[5] Fuchs, M. H. P., & Demir, B. (2023). HySpecNet-11k: A Large-Scale Hyperspectral Dataset for Benchmarking Learning-Based Hyperspectral Image Compression Methods. arXiv preprint arXiv:2306.00385v2.",
    "crumbs": [
      "Leveraging Deep Neural Network Compression Techniques for Real-Time Hyperspectral Image Processing in Edge AI"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "hsicomp",
    "section": "",
    "text": "This file will become your README and also the index of your documentation.",
    "crumbs": [
      "hsicomp"
    ]
  },
  {
    "objectID": "index.html#developer-guide",
    "href": "index.html#developer-guide",
    "title": "hsicomp",
    "section": "Developer Guide",
    "text": "Developer Guide\nIf you are new to using nbdev here are some useful pointers to get you started.\n\nInstall hsicomp in Development mode\n# make sure hsicomp package is installed in development mode\n$ pip install -e .\n\n# make changes under nbs/ directory\n# ...\n\n# compile to have changes apply to hsicomp\n$ nbdev_prepare",
    "crumbs": [
      "hsicomp"
    ]
  },
  {
    "objectID": "index.html#usage",
    "href": "index.html#usage",
    "title": "hsicomp",
    "section": "Usage",
    "text": "Usage\n\nInstallation\nInstall latest from the GitHub repository:\n$ pip install git+https://github.com/ninjalabo/hsicomp.git\nor from conda\n$ conda install -c ninjalabo hsicomp\nor from pypi\n$ pip install hsicomp\n\n\nDocumentation\nDocumentation can be found hosted on this GitHub repository’s pages. Additionally you can find package manager specific guidelines on conda and pypi respectively.",
    "crumbs": [
      "hsicomp"
    ]
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "hsicomp",
    "section": "How to use",
    "text": "How to use\nFill me in please! Don’t forget code examples:\n\n1+1\n\n2",
    "crumbs": [
      "hsicomp"
    ]
  },
  {
    "objectID": "hyspecnet11k_setup.html",
    "href": "hyspecnet11k_setup.html",
    "title": "HySpecNet-11k-Env_setup",
    "section": "",
    "text": "Re-clone if needed after a forced reset:\ngit clone https://git.tu-berlin.de/rsim/hsi-compression.git\nBefore pushing changes:\ngit remote update\ngit rebase origin/main\nnbdev_prepare",
    "crumbs": [
      "HySpecNet-11k-Env_setup"
    ]
  },
  {
    "objectID": "hyspecnet11k_setup.html#repository-setup-reminders",
    "href": "hyspecnet11k_setup.html#repository-setup-reminders",
    "title": "HySpecNet-11k-Env_setup",
    "section": "",
    "text": "Re-clone if needed after a forced reset:\ngit clone https://git.tu-berlin.de/rsim/hsi-compression.git\nBefore pushing changes:\ngit remote update\ngit rebase origin/main\nnbdev_prepare",
    "crumbs": [
      "HySpecNet-11k-Env_setup"
    ]
  },
  {
    "objectID": "hyspecnet11k_setup.html#vm-and-jupyter-notebook-setup",
    "href": "hyspecnet11k_setup.html#vm-and-jupyter-notebook-setup",
    "title": "HySpecNet-11k-Env_setup",
    "section": "VM and Jupyter Notebook Setup:",
    "text": "VM and Jupyter Notebook Setup:\nConnect to the virtual machine:\nssh root@VM_IP.Address",
    "crumbs": [
      "HySpecNet-11k-Env_setup"
    ]
  },
  {
    "objectID": "hyspecnet11k_setup.html#tunnel-the-jupyter-notebook",
    "href": "hyspecnet11k_setup.html#tunnel-the-jupyter-notebook",
    "title": "HySpecNet-11k-Env_setup",
    "section": "Tunnel the Jupyter Notebook:",
    "text": "Tunnel the Jupyter Notebook:\nssh -i ~/.ssh/public_key -L 7777:localhost:8888 root@VM_IP_address\njupyter notebook --no-browser --port=8888 --ip=0.0.0.0 --allow-root\nAccess the notebook at localhost:7777.\n\nsource\n\nfoo\n\n foo ()",
    "crumbs": [
      "HySpecNet-11k-Env_setup"
    ]
  },
  {
    "objectID": "Paper_Pruning.html",
    "href": "Paper_Pruning.html",
    "title": "Leveraging Deep Neural Network Compression Techniques for Real-Time Hyperspectral Image Processing in Edge AI",
    "section": "",
    "text": "Authors: Dheeraj Kumar\n\nAbstract\nHyperspectral imaging has become increasingly relevant in remote sensing applications due to its ability to capture detailed spectral information. The growth of hyperspectral datasets has necessitated efficient compression techniques to handle their massive storage requirements. This paper proposes a new approach that integrates the Spectral Signals Compressor Network (SSCNet), a state-of-the-art deep learning-based hyperspectral image compression model, with FasterAI pruning to improve compression efficiency. We used HySpecNet-11k, a large-scale hyperspectral benchmark dataset, to validate our approach. The results show significant improvements in compression rates while preserving high-quality image reconstruction, thus setting a new benchmark for learning-based hyperspectral image compression.\nModel compression is a key technique used to reduce the memory footprint and computational demands of deep learning models, making them more suitable for deployment on resource-constrained devices. Model compression can be achieved through various techniques such as pruning, knowledge distillation, and quantization. Pruning, which is one such model compression technique, involves systematically removing unimportant weights or neurons from a network to reduce model size and computational complexity while maintaining acceptable levels of accuracy. In this paper, we implemented pruning from FasterAI to enhance SSCNet, making it more efficient without significantly compromising performance. We compare the original model and the pruned model to evaluate the benefits and impact of model compression.\n\n\n\n1. Introduction\nRecent advancements in hyperspectral imaging technology have led to the proliferation of hyperspectral data, significantly improving the identification and discrimination of materials in remote sensing applications. However, the resulting large volume of data necessitates efficient storage and transmission, driving research in hyperspectral image compression. Traditional approaches, which often combine transform coding and quantization techniques, face challenges when dealing with high-dimensional data. To address these limitations, learning-based methods leveraging convolutional autoencoders have emerged as promising alternatives for hyperspectral image compression [5].\nIn this paper, we present an improved hyperspectral image compression method by applying pruning using FasterAI on SSCNet [4], a convolutional autoencoder-based architecture. We leverage the HySpecNet-11k dataset [2], which provides a robust benchmark for training and evaluating learning-based hyperspectral image compression models.\n\n\n2. Methodology\n\n2.1 HySpecNet-11k Dataset\nThe HySpecNet-11k dataset was designed specifically for benchmarking learning-based hyperspectral image compression methods [5]. It consists of 11,483 non-overlapping image patches extracted from 250 EnMAP tiles, each patch containing 128×128 pixels with 224 spectral bands, and a ground sampling distance of 30m. These image patches provide a large-scale and diverse set of hyperspectral data acquired in spatially disjoint geographical regions, addressing the limitations of earlier datasets which often suffered from overfitting due to spatial similarity among samples.\nFor our experiments, we used the “easy split” setup for the dataset, where 70% of the image patches were used for training, 20% for validation, and 10% for testing. This ensured reproducibility and allowed for a comprehensive comparison with existing learning-based hyperspectral image compression techniques.\n\n\n2.2 Spectral Signals Compressor Network (SSCNet)\nSSCNet is a deep learning-based compression model that utilizes 2D convolutional layers with parametric ReLU activations for both spatial and spectral compression [4]. The encoder network employs 2D convolutions and three max pooling layers to achieve a fixed spatial compression factor of 64. The decoder mirrors the encoder, with upsampling achieved through transposed convolutional layers.\nThe compression ratio (CR) is set by the number of latent channels in the bottleneck layer, while the network’s ability to reconstruct the original image with minimal distortion is evaluated through Peak Signal-to-Noise Ratio (PSNR). SSCNet was chosen for this study due to its effectiveness in combining both spatial and spectral compression in hyperspectral datasets.\n\n\n2.3 FasterAI Pruning\nTo enhance the performance of SSCNet, we implemented structured pruning from FasterAI, which systematically removes less important filters or neurons in the model’s layers. Pruning is particularly effective in reducing computational complexity and the memory footprint of deep learning models without significant degradation in accuracy.\nIn our work, we applied pruning during the fine-tuning phase, after training the SSCNet model. The pruning process involved identifying and removing low-magnitude weights or redundant neurons, followed by retraining the model to recover performance. This iterative process ensured that the pruned model maintained high reconstruction quality while achieving substantial reductions in model size and inference time.\n\n\n\n3. Experimental Setup\nThe experimental setup used PyTorch as the deep learning framework. Gradient clipping and min-max normalization were applied to scale input data within a range suitable for learning-based models. The Adam optimizer was used with an initial learning rate of \\(1e−4\\), which was decreased as training progressed to achieve convergence.\nOur experiments were conducted on a high-performance computing environment, featuring an NVIDIA L4 Tensor Core GPU with 24 GB memory. We tested the SSCNet model for 5 epochs, followed by pruning and fine-tuning for an additional 3 epochs.\n\n\n\n4. Results and Discussion\n\nPeak Signal-to-Noise Ratio (PSNR): The pruned SSCNet achieved a PSNR of 36.09 dB, demonstrating strong reconstruction fidelity.\nSpectral Angle (SA): The pruned model maintained a low Spectral Angle deviation of 3.54°, indicating minimal distortion in spectral data.\nStructural Similarity Index (SSIM): The compression model achieved an SSIM of 0.9119 vs. the original Model SSIM is 0.9747, confirming preservation of spatial structures.\nOriginal images have 16 bpppc and compressed images have 2.53 bpppc.\n\n\n4.1 Model Efficiency\n\nModel Compression: The pruning process reduced the model size from 52.59 MB to 30.20 MB (a 42.6% reduction) and the number of parameters from 13,783,242 to 7,911,383 (a 42.6% reduction). This highlights the significant decrease in computational overhead and storage requirements.\nVRAM Usage: The pruned model drastically reduced VRAM consumption from 910.00 MB to 113.07 MB, making it highly efficient for deployment on devices with limited GPU resources.\n\n\n\n4.2 Comparative Analysis\n\nPerformance Superiority: The pruned SSCNet outperformed traditional and learning-based compression methods in terms of computational efficiency and memory usage, while maintaining competitive reconstruction quality.\nPreservation of Fidelity: Despite pruning, the model retained high-quality reconstructions with:\n\nPSNR ensuring strong pixel-level accuracy.\nSSIM confirming minimal degradation in structural similarity.\nLow spectral angle deviation, validating accurate spectral information preservation.\n\n\n\n\n\n\n5. Conclusion\nThis paper introduces a novel compression approach that combines the power of SSCNet for hyperspectral image compression with FasterAI’s pruning technique. The results show that this combined approach effectively reduces the memory footprint and computational requirements, making it suitable for real-time remote sensing applications while maintaining high reconstruction quality. Future work includes extending this approach to other hyperspectral datasets, exploring dynamic pruning strategies, and integrating additional compression techniques such as quantization for further performance improvements.\n\n\nAcknowledgements\nWe also acknowledge the use of the HySpecNet-11k dataset and the Fasterai framework in conducting our experiments.\n\n\nReferences\n\n[1] M. H. P. Fuchs and B. Demir, “Hyspecnet-11k: A large-scale hyperspectral dataset for benchmarking learning-based hyperspectral image compression methods,” in IGARSS 2023-2023 IEEE International Geoscience and Remote Sensing Symposium, IEEE, 2023, pp. 1779–1782.\n[2] M. H. P. Fuchs and B. Demir, “HySpecNet-11k: A large-scale hyperspectral benchmark dataset.” Dryad, p. 63608947808 bytes, Jun. 26, 2023. doi: 10.5061/DRYAD.FTTDZ08ZH.\n[3] R. La Grassa, C. Re, G. Cremonese, and I. Gallo, “Hyperspectral data compression using fully convolutional autoencoder,” Remote Sensing, vol. 14, no. 10, p. 2472, 2022.\n[4] “FasterAI,” fasterai. Available: https://nathanhubens.github.io/fasterai/\n[5] Fuchs, M. H. P., & Demir, B. (2023). HySpecNet-11k: A Large-Scale Hyperspectral Dataset for Benchmarking Learning-Based Hyperspectral Image Compression Methods. arXiv preprint arXiv:2306.00385v2.",
    "crumbs": [
      "Leveraging Deep Neural Network Compression Techniques for Real-Time Hyperspectral Image Processing in Edge AI"
    ]
  },
  {
    "objectID": "Poster_Pruning.html",
    "href": "Poster_Pruning.html",
    "title": "Leveraging Deep Neural Network Compression Techniques for Real-Time Hyperspectral Image Processing in Edge AI",
    "section": "",
    "text": "Authors: Dheeraj Kumar",
    "crumbs": [
      "Leveraging Deep Neural Network Compression Techniques for Real-Time Hyperspectral Image Processing in Edge AI"
    ]
  },
  {
    "objectID": "Poster_Pruning.html#introduction",
    "href": "Poster_Pruning.html#introduction",
    "title": "Leveraging Deep Neural Network Compression Techniques for Real-Time Hyperspectral Image Processing in Edge AI",
    "section": "Introduction",
    "text": "Introduction\n\nObjective\nTo improve hyperspectral image analysis by integrating SSCNet [1] with the FasterAI [4] compression technique, demonstrating efficiency and performance on the HySpecNet-11k dataset. Hyperspectral imaging provides rich spectral information across numerous bands, supporting applications like remote sensing, agriculture, and medical imaging. However, the high volume and computational demands of hyperspectral data necessitate innovative compression and processing techniques.\n\n\nChallenges\n\nManaging large-scale hyperspectral datasets.\nBalancing reconstruction quality and compression efficiency.\n\n\n\nContributions\n\nIntegration of SSCNet with FasterAI pruning for compressing HSI compression model. to reduce model size while preserving high-quality image reconstruction.\nValidated on HySpecNet-11k, a large-scale hyperspectral benchmark dataset.\nAchieved significant reduction in model size and computational load with minor performance trade-offs.",
    "crumbs": [
      "Leveraging Deep Neural Network Compression Techniques for Real-Time Hyperspectral Image Processing in Edge AI"
    ]
  },
  {
    "objectID": "Poster_Pruning.html#methodology",
    "href": "Poster_Pruning.html#methodology",
    "title": "Leveraging Deep Neural Network Compression Techniques for Real-Time Hyperspectral Image Processing in Edge AI",
    "section": "Methodology",
    "text": "Methodology\n\nDataset: HySpecNet-11k\nHySpecNet-11k [2] is a large-scale hyperspectral dataset containing 11,483 image patches (128×128 pixels with 224 spectral bands) derived from e Environmental Mapping and Analysis Program (EnMAP) satellite data. It is designed for benchmarking learning-based compression and analysis methods.\n\nDataset Splits: Training (70%), Validation (20%), Test (10%).\nPreprocessing: Removed water vapor-affected bands, applied normalization, and used both patchwise and tilewise splits.\n\n\n\nModel: Spectral Signals Compressor Network (SSCNet)\nSSCNet [3] uses 2D convolutions to compress spatial dimensions while preserving spectral integrity.\n\nEncoder: 2D convolutional layers with parametric ReLU activation and three max-pooling.\nDecoder: Uses transposed convolutions for reconstruction.\nCompression Ratio (CR): Defined by latent channels in bottleneck layer.\n\n\n\nFasterAI Pruning Compression Technique\n\nRemove redundant weights or neurons.\nFine-tune to recover performance.\nOutcome: Smaller, faster model with minimal accuracy loss.",
    "crumbs": [
      "Leveraging Deep Neural Network Compression Techniques for Real-Time Hyperspectral Image Processing in Edge AI"
    ]
  },
  {
    "objectID": "Poster_Pruning.html#experimental-results",
    "href": "Poster_Pruning.html#experimental-results",
    "title": "Leveraging Deep Neural Network Compression Techniques for Real-Time Hyperspectral Image Processing in Edge AI",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nPeak Signal-to-Noise Ratio (PSNR): The pruned SSCNet achieved a PSNR of 36.09 dB, demonstrating strong reconstruction fidelity.\nSpectral Angle (SA): The pruned model maintained a low Spectral Angle deviation of 3.54°, indicating minimal distortion in spectral data.\nStructural Similarity Index (SSIM): The compression model achieved an SSIM of 0.9119 vs. the original Model SSIM is 0.9747, confirming preservation of spatial structures.\nOriginal images have 16 bpppc and compressed images have 2.53 bpppc.\n\n\nModel Efficiency\n\nModel Compression: The pruning process reduced the model size from 52.59 MB to 30.20 MB (a 42.6% reduction) and the number of parameters from 13,783,242 to 7,911,383 (a 42.6% reduction). This highlights the significant decrease in computational overhead and storage requirements.\nVRAM Usage: The pruned model drastically reduced VRAM consumption from 910.00 MB to 113.07 MB, making it highly efficient for deployment on devices with limited GPU resources.\n\n\n\nComparative Analysis\n\nPerformance Superiority: The pruned SSCNet outperformed traditional and learning-based compression methods in terms of computational efficiency and memory usage, while maintaining competitive reconstruction quality.\nPreservation of Fidelity: Despite pruning, the model retained high-quality reconstructions with:\n\nPSNR ensuring strong pixel-level accuracy.\nSSIM confirming minimal degradation in structural similarity.\nLow spectral angle deviation, validating accurate spectral information preservation.",
    "crumbs": [
      "Leveraging Deep Neural Network Compression Techniques for Real-Time Hyperspectral Image Processing in Edge AI"
    ]
  },
  {
    "objectID": "Poster_Pruning.html#conclusion",
    "href": "Poster_Pruning.html#conclusion",
    "title": "Leveraging Deep Neural Network Compression Techniques for Real-Time Hyperspectral Image Processing in Edge AI",
    "section": "Conclusion",
    "text": "Conclusion\n\nEffective reduction in memory footprint and computational demands for real-time edge AI deployment.\nEnables practical deployment of hyperspectral models in resource-constrained environments.\nSupports scalable analysis for large datasets like HySpecNet-11k.",
    "crumbs": [
      "Leveraging Deep Neural Network Compression Techniques for Real-Time Hyperspectral Image Processing in Edge AI"
    ]
  },
  {
    "objectID": "Poster_Pruning.html#future-work",
    "href": "Poster_Pruning.html#future-work",
    "title": "Leveraging Deep Neural Network Compression Techniques for Real-Time Hyperspectral Image Processing in Edge AI",
    "section": "Future Work",
    "text": "Future Work\n\nTest FasterAI compression on additional hyperspectral models.\nExplore dynamic pruning strategies.\nApply other model compression techniques.",
    "crumbs": [
      "Leveraging Deep Neural Network Compression Techniques for Real-Time Hyperspectral Image Processing in Edge AI"
    ]
  },
  {
    "objectID": "Poster_Pruning.html#references",
    "href": "Poster_Pruning.html#references",
    "title": "Leveraging Deep Neural Network Compression Techniques for Real-Time Hyperspectral Image Processing in Edge AI",
    "section": "References",
    "text": "References\n\n[1] M. H. P. Fuchs and B. Demir, “Hyspecnet-11k: A large-scale hyperspectral dataset for benchmarking learning-based hyperspectral image compression methods,” in IGARSS 2023-2023 IEEE International Geoscience and Remote Sensing Symposium, IEEE, 2023, pp. 1779–1782.\n[2] M. H. P. Fuchs and B. Demir, “HySpecNet-11k: A large-scale hyperspectral benchmark dataset.” Dryad, p. 63608947808 bytes, Jun. 26, 2023. doi: 10.5061/DRYAD.FTTDZ08ZH.\n[3] R. La Grassa, C. Re, G. Cremonese, and I. Gallo, “Hyperspectral data compression using fully convolutional autoencoder,” Remote Sensing, vol. 14, no. 10, p. 2472, 2022.\n[4] “FasterAI,” fasterai. Available: https://nathanhubens.github.io/fasterai/\n[5] Fuchs, M. H. P., & Demir, B. (2023). HySpecNet-11k: A Large-Scale Hyperspectral Dataset for Benchmarking Learning-Based Hyperspectral Image Compression Methods. arXiv preprint arXiv:2306.00385v2.",
    "crumbs": [
      "Leveraging Deep Neural Network Compression Techniques for Real-Time Hyperspectral Image Processing in Edge AI"
    ]
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "core",
    "section": "",
    "text": "source\n\nfoo\n\n foo ()",
    "crumbs": [
      "core"
    ]
  },
  {
    "objectID": "arad1k_setup.html",
    "href": "arad1k_setup.html",
    "title": "core",
    "section": "",
    "text": "source\n\nfoo\n\n foo ()",
    "crumbs": [
      "core"
    ]
  },
  {
    "objectID": "hyspecnet11k_quantization.html",
    "href": "hyspecnet11k_quantization.html",
    "title": "core",
    "section": "",
    "text": "source\n\nfoo\n\n foo ()",
    "crumbs": [
      "core"
    ]
  },
  {
    "objectID": "arad1k_mst++_inference.html",
    "href": "arad1k_mst++_inference.html",
    "title": "MST++: Multi-stage Spectral-wise Transformer for Efficient Spectral Reconstruction (CVPRW 2022)",
    "section": "",
    "text": "MST++: Multi-stage Spectral-wise Transformer\n\n\nSST: Single-stage Spectral-wise Transformer\n\n\nSAB: Spectral-wise Attention Block\n\n\nFFN: Feed Forward Network\n\n\nS-MSA: Spectral-wise Multi-head Self-attention",
    "crumbs": [
      "MST++: Multi-stage Spectral-wise Transformer for Efficient Spectral Reconstruction (CVPRW 2022)"
    ]
  },
  {
    "objectID": "arad1k_mst++_inference.html#pipeline-of-mst-model-architecture",
    "href": "arad1k_mst++_inference.html#pipeline-of-mst-model-architecture",
    "title": "MST++: Multi-stage Spectral-wise Transformer for Efficient Spectral Reconstruction (CVPRW 2022)",
    "section": "",
    "text": "MST++: Multi-stage Spectral-wise Transformer\n\n\nSST: Single-stage Spectral-wise Transformer\n\n\nSAB: Spectral-wise Attention Block\n\n\nFFN: Feed Forward Network\n\n\nS-MSA: Spectral-wise Multi-head Self-attention",
    "crumbs": [
      "MST++: Multi-stage Spectral-wise Transformer for Efficient Spectral Reconstruction (CVPRW 2022)"
    ]
  },
  {
    "objectID": "arad1k_mst++_inference.html#dataset",
    "href": "arad1k_mst++_inference.html#dataset",
    "title": "MST++: Multi-stage Spectral-wise Transformer for Efficient Spectral Reconstruction (CVPRW 2022)",
    "section": "Dataset:",
    "text": "Dataset:\nARAD_1K, The dataset provided by NTIRE 2022 Spectral Reconstruction Challenge contains 1000 RGB-HSI pairs. This dataset is split into train, valid, and test subsets in proportional to 18:1:1. Each HSI at size of 482×512 has 31 wavelengths from 400 nm to 700 nm.\nThis data set contains 950 publicly images and 50 “test” images which have only been released as RGB or Multi-Spectral-Filter-Array “RAW” files.\nPublicly available images have been published as:\n\n31 channel hyperspectral images in the 400-700nm range\n16 channel multi-spectral images in the 400-1000nm range",
    "crumbs": [
      "MST++: Multi-stage Spectral-wise Transformer for Efficient Spectral Reconstruction (CVPRW 2022)"
    ]
  },
  {
    "objectID": "arad1k_mst++_inference.html#create-envirement",
    "href": "arad1k_mst++_inference.html#create-envirement",
    "title": "MST++: Multi-stage Spectral-wise Transformer for Efficient Spectral Reconstruction (CVPRW 2022)",
    "section": "1. Create Envirement:",
    "text": "1. Create Envirement:\n\nPython 3 (Recommend to use Anaconda)\nNVIDIA GPU + CUDA\nPython packages : requirements.txt",
    "crumbs": [
      "MST++: Multi-stage Spectral-wise Transformer for Efficient Spectral Reconstruction (CVPRW 2022)"
    ]
  },
  {
    "objectID": "arad1k_mst++_inference.html#data-preparation",
    "href": "arad1k_mst++_inference.html#data-preparation",
    "title": "MST++: Multi-stage Spectral-wise Transformer for Efficient Spectral Reconstruction (CVPRW 2022)",
    "section": "2. Data Preparation:",
    "text": "2. Data Preparation:\nDownload datasets from the main GitHub repo. * training spectral images (Google Drive / Baidu Disk, code: mst1): Size 19G * training RGB images (Google Drive / Baidu Disk): Size 22M * validation spectral images (Google Drive / Baidu Disk): Size 1G * validation RGB images (Google Drive / Baidu Disk): Size 1M * testing RGB images (Google Drive / Baidu Disk): Size 1M",
    "crumbs": [
      "MST++: Multi-stage Spectral-wise Transformer for Efficient Spectral Reconstruction (CVPRW 2022)"
    ]
  },
  {
    "objectID": "arad1k_mst++_inference.html#directory-of-the-dataset",
    "href": "arad1k_mst++_inference.html#directory-of-the-dataset",
    "title": "MST++: Multi-stage Spectral-wise Transformer for Efficient Spectral Reconstruction (CVPRW 2022)",
    "section": "Directory of the dataset:",
    "text": "Directory of the dataset:\n\nPlace the training spectral images and validation spectral images to /MST-plus-plus/dataset/Train_Spec/.\nPlace the training RGB images and validation RGB images to /MST-plus-plus/dataset/Train_RGB/.\nPlace the testing RGB images to /MST-plus-plus/dataset/Test_RGB/.\n\nThen this repo is collected as the following form:\n|–MST-plus-plus |–test_challenge_code |–test_develop_code |–train_code\n|–dataset |–Train_Spec |–ARAD_1K_0001.mat |–ARAD_1K_0002.mat ： |–ARAD_1K_0950.mat |–Train_RGB |–ARAD_1K_0001.jpg |–ARAD_1K_0002.jpg ： |–ARAD_1K_0950.jpg |–Test_RGB |–ARAD_1K_0951.jpg |–ARAD_1K_0952.jpg ： |–ARAD_1K_1000.jpg |–split_txt |–train_list.txt |–valid_list.txt\n\n# create environment: conda create -n ostrack python=3.8\n# conda activate MST\n#cd MST-plus-plus/\n\n\n# !pip install -r requirements.txt",
    "crumbs": [
      "MST++: Multi-stage Spectral-wise Transformer for Efficient Spectral Reconstruction (CVPRW 2022)"
    ]
  },
  {
    "objectID": "arad1k_mst++_inference.html#different-methods",
    "href": "arad1k_mst++_inference.html#different-methods",
    "title": "MST++: Multi-stage Spectral-wise Transformer for Efficient Spectral Reconstruction (CVPRW 2022)",
    "section": "Different Methods:",
    "text": "Different Methods:\n\nHSCNN+\nHRNet\nEDSR\nAWAN\nHDNet\nHINet\nMIRNet\nRestormer\nMPRNet\nMST-L\nMST ++\n\n\nMST_Plus_Plus\n\nimport torch.nn as nn\nimport torch\nimport torch.nn.functional as F\nfrom einops import rearrange\nimport math\nimport warnings\nfrom torch.nn.init import _calculate_fan_in_and_fan_out\n\ndef _no_grad_trunc_normal_(tensor, mean, std, a, b):\n    def norm_cdf(x):\n        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n\n    if (mean &lt; a - 2 * std) or (mean &gt; b + 2 * std):\n        warnings.warn(\"mean is more than 2 std from [a, b] in nn.init.trunc_normal_. \"\n                      \"The distribution of values may be incorrect.\",\n                      stacklevel=2)\n    with torch.no_grad():\n        l = norm_cdf((a - mean) / std)\n        u = norm_cdf((b - mean) / std)\n        tensor.uniform_(2 * l - 1, 2 * u - 1)\n        tensor.erfinv_()\n        tensor.mul_(std * math.sqrt(2.))\n        tensor.add_(mean)\n        tensor.clamp_(min=a, max=b)\n        return tensor\n\n\ndef trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n    # type: (Tensor, float, float, float, float) -&gt; Tensor\n    return _no_grad_trunc_normal_(tensor, mean, std, a, b)\n\n\ndef variance_scaling_(tensor, scale=1.0, mode='fan_in', distribution='normal'):\n    fan_in, fan_out = _calculate_fan_in_and_fan_out(tensor)\n    if mode == 'fan_in':\n        denom = fan_in\n    elif mode == 'fan_out':\n        denom = fan_out\n    elif mode == 'fan_avg':\n        denom = (fan_in + fan_out) / 2\n    variance = scale / denom\n    if distribution == \"truncated_normal\":\n        trunc_normal_(tensor, std=math.sqrt(variance) / .87962566103423978)\n    elif distribution == \"normal\":\n        tensor.normal_(std=math.sqrt(variance))\n    elif distribution == \"uniform\":\n        bound = math.sqrt(3 * variance)\n        tensor.uniform_(-bound, bound)\n    else:\n        raise ValueError(f\"invalid distribution {distribution}\")\n\n\ndef lecun_normal_(tensor):\n    variance_scaling_(tensor, mode='fan_in', distribution='truncated_normal')\n\n\nclass PreNorm(nn.Module):\n    def __init__(self, dim, fn):\n        super().__init__()\n        self.fn = fn\n        self.norm = nn.LayerNorm(dim)\n\n    def forward(self, x, *args, **kwargs):\n        x = self.norm(x)\n        return self.fn(x, *args, **kwargs)\n\n\nclass GELU(nn.Module):\n    def forward(self, x):\n        return F.gelu(x)\n\ndef conv(in_channels, out_channels, kernel_size, bias=False, padding = 1, stride = 1):\n    return nn.Conv2d(\n        in_channels, out_channels, kernel_size,\n        padding=(kernel_size//2), bias=bias, stride=stride)\n\n\ndef shift_back(inputs,step=2):          # input [bs,28,256,310]  output [bs, 28, 256, 256]\n    [bs, nC, row, col] = inputs.shape\n    down_sample = 256//row\n    step = float(step)/float(down_sample*down_sample)\n    out_col = row\n    for i in range(nC):\n        inputs[:,i,:,:out_col] = \\\n            inputs[:,i,:,int(step*i):int(step*i)+out_col]\n    return inputs[:, :, :, :out_col]\n\nclass MS_MSA(nn.Module):\n    def __init__(\n            self,\n            dim,\n            dim_head,\n            heads,\n    ):\n        super().__init__()\n        self.num_heads = heads\n        self.dim_head = dim_head\n        self.to_q = nn.Linear(dim, dim_head * heads, bias=False)\n        self.to_k = nn.Linear(dim, dim_head * heads, bias=False)\n        self.to_v = nn.Linear(dim, dim_head * heads, bias=False)\n        self.rescale = nn.Parameter(torch.ones(heads, 1, 1))\n        self.proj = nn.Linear(dim_head * heads, dim, bias=True)\n        self.pos_emb = nn.Sequential(\n            nn.Conv2d(dim, dim, 3, 1, 1, bias=False, groups=dim),\n            GELU(),\n            nn.Conv2d(dim, dim, 3, 1, 1, bias=False, groups=dim),\n        )\n        self.dim = dim\n\n    def forward(self, x_in):\n        \"\"\"\n        x_in: [b,h,w,c]\n        return out: [b,h,w,c]\n        \"\"\"\n        b, h, w, c = x_in.shape\n        x = x_in.reshape(b,h*w,c)\n        q_inp = self.to_q(x)\n        k_inp = self.to_k(x)\n        v_inp = self.to_v(x)\n        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -&gt; b h n d', h=self.num_heads),\n                                (q_inp, k_inp, v_inp))\n        v = v\n        # q: b,heads,hw,c\n        q = q.transpose(-2, -1)\n        k = k.transpose(-2, -1)\n        v = v.transpose(-2, -1)\n        q = F.normalize(q, dim=-1, p=2)\n        k = F.normalize(k, dim=-1, p=2)\n        attn = (k @ q.transpose(-2, -1))   # A = K^T*Q\n        attn = attn * self.rescale\n        attn = attn.softmax(dim=-1)\n        x = attn @ v   # b,heads,d,hw\n        x = x.permute(0, 3, 1, 2)    # Transpose\n        x = x.reshape(b, h * w, self.num_heads * self.dim_head)\n        out_c = self.proj(x).view(b, h, w, c)\n        out_p = self.pos_emb(v_inp.reshape(b,h,w,c).permute(0, 3, 1, 2)).permute(0, 2, 3, 1)\n        out = out_c + out_p\n\n        return out\n\nclass FeedForward(nn.Module):\n    def __init__(self, dim, mult=4):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Conv2d(dim, dim * mult, 1, 1, bias=False),\n            GELU(),\n            nn.Conv2d(dim * mult, dim * mult, 3, 1, 1, bias=False, groups=dim * mult),\n            GELU(),\n            nn.Conv2d(dim * mult, dim, 1, 1, bias=False),\n        )\n\n    def forward(self, x):\n        \"\"\"\n        x: [b,h,w,c]\n        return out: [b,h,w,c]\n        \"\"\"\n        out = self.net(x.permute(0, 3, 1, 2))\n        return out.permute(0, 2, 3, 1)\n\nclass MSAB(nn.Module):\n    def __init__(\n            self,\n            dim,\n            dim_head,\n            heads,\n            num_blocks,\n    ):\n        super().__init__()\n        self.blocks = nn.ModuleList([])\n        for _ in range(num_blocks):\n            self.blocks.append(nn.ModuleList([\n                MS_MSA(dim=dim, dim_head=dim_head, heads=heads),\n                PreNorm(dim, FeedForward(dim=dim))\n            ]))\n\n    def forward(self, x):\n        \"\"\"\n        x: [b,c,h,w]\n        return out: [b,c,h,w]\n        \"\"\"\n        x = x.permute(0, 2, 3, 1)\n        for (attn, ff) in self.blocks:\n            x = attn(x) + x\n            x = ff(x) + x\n        out = x.permute(0, 3, 1, 2)\n        return out\n\nclass MST(nn.Module):\n    def __init__(self, in_dim=31, out_dim=31, dim=31, stage=2, num_blocks=[2,4,4]):\n        super(MST, self).__init__()\n        self.dim = dim\n        self.stage = stage\n\n        # Input projection\n        self.embedding = nn.Conv2d(in_dim, self.dim, 3, 1, 1, bias=False)\n\n        # Encoder\n        self.encoder_layers = nn.ModuleList([])\n        dim_stage = dim\n        for i in range(stage):\n            self.encoder_layers.append(nn.ModuleList([\n                MSAB(\n                    dim=dim_stage, num_blocks=num_blocks[i], dim_head=dim, heads=dim_stage // dim),\n                nn.Conv2d(dim_stage, dim_stage * 2, 4, 2, 1, bias=False),\n            ]))\n            dim_stage *= 2\n\n        # Bottleneck\n        self.bottleneck = MSAB(\n            dim=dim_stage, dim_head=dim, heads=dim_stage // dim, num_blocks=num_blocks[-1])\n\n        # Decoder\n        self.decoder_layers = nn.ModuleList([])\n        for i in range(stage):\n            self.decoder_layers.append(nn.ModuleList([\n                nn.ConvTranspose2d(dim_stage, dim_stage // 2, stride=2, kernel_size=2, padding=0, output_padding=0),\n                nn.Conv2d(dim_stage, dim_stage // 2, 1, 1, bias=False),\n                MSAB(\n                    dim=dim_stage // 2, num_blocks=num_blocks[stage - 1 - i], dim_head=dim,\n                    heads=(dim_stage // 2) // dim),\n            ]))\n            dim_stage //= 2\n\n        # Output projection\n        self.mapping = nn.Conv2d(self.dim, out_dim, 3, 1, 1, bias=False)\n\n        #### activation function\n        self.lrelu = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n\n    def forward(self, x):\n        \"\"\"\n        x: [b,c,h,w]\n        return out:[b,c,h,w]\n        \"\"\"\n\n        # Embedding\n        fea = self.embedding(x)\n\n        # Encoder\n        fea_encoder = []\n        for (MSAB, FeaDownSample) in self.encoder_layers:\n            fea = MSAB(fea)\n            fea_encoder.append(fea)\n            fea = FeaDownSample(fea)\n\n        # Bottleneck\n        fea = self.bottleneck(fea)\n\n        # Decoder\n        for i, (FeaUpSample, Fution, LeWinBlcok) in enumerate(self.decoder_layers):\n            fea = FeaUpSample(fea)\n            fea = Fution(torch.cat([fea, fea_encoder[self.stage-1-i]], dim=1))\n            fea = LeWinBlcok(fea)\n\n        # Mapping\n        out = self.mapping(fea) + x\n\n        return out\n\nclass MST_Plus_Plus(nn.Module):\n    def __init__(self, in_channels=3, out_channels=31, n_feat=31, stage=3):\n        super(MST_Plus_Plus, self).__init__()\n        self.stage = stage\n        self.conv_in = nn.Conv2d(in_channels, n_feat, kernel_size=3, padding=(3 - 1) // 2,bias=False)\n        modules_body = [MST(dim=31, stage=2, num_blocks=[1,1,1]) for _ in range(stage)]\n        self.body = nn.Sequential(*modules_body)\n        self.conv_out = nn.Conv2d(n_feat, out_channels, kernel_size=3, padding=(3 - 1) // 2,bias=False)\n\n    def forward(self, x):\n        \"\"\"\n        x: [b,c,h,w]\n        return out:[b,c,h,w]\n        \"\"\"\n        b, c, h_inp, w_inp = x.shape\n        hb, wb = 8, 8\n        pad_h = (hb - h_inp % hb) % hb\n        pad_w = (wb - w_inp % wb) % wb\n        x = F.pad(x, [0, pad_w, 0, pad_h], mode='reflect')\n        x = self.conv_in(x)\n        h = self.body(x)\n        h = self.conv_out(h)\n        h += x\n        return h[:, :, :h_inp, :w_inp]\n\n\n# from architecture.mst_plus_plus import MST_Plus_Plus\nimport architecture\nfrom utils import my_summary\nmy_summary(MST_Plus_Plus(), 256, 256, 3, 1)\n\nMST_Plus_Plus(\n  (conv_in): Conv2d(3, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (body): Sequential(\n    (0): MST(\n      (embedding): Conv2d(31, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (encoder_layers): ModuleList(\n        (0): ModuleList(\n          (0): MSAB(\n            (blocks): ModuleList(\n              (0): ModuleList(\n                (0): MS_MSA(\n                  (to_q): Linear(in_features=31, out_features=31, bias=False)\n                  (to_k): Linear(in_features=31, out_features=31, bias=False)\n                  (to_v): Linear(in_features=31, out_features=31, bias=False)\n                  (proj): Linear(in_features=31, out_features=31, bias=True)\n                  (pos_emb): Sequential(\n                    (0): Conv2d(31, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=31, bias=False)\n                    (1): GELU()\n                    (2): Conv2d(31, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=31, bias=False)\n                  )\n                )\n                (1): PreNorm(\n                  (fn): FeedForward(\n                    (net): Sequential(\n                      (0): Conv2d(31, 124, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                      (1): GELU()\n                      (2): Conv2d(124, 124, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=124, bias=False)\n                      (3): GELU()\n                      (4): Conv2d(124, 31, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                    )\n                  )\n                  (norm): LayerNorm((31,), eps=1e-05, elementwise_affine=True)\n                )\n              )\n            )\n          )\n          (1): Conv2d(31, 62, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n        )\n        (1): ModuleList(\n          (0): MSAB(\n            (blocks): ModuleList(\n              (0): ModuleList(\n                (0): MS_MSA(\n                  (to_q): Linear(in_features=62, out_features=62, bias=False)\n                  (to_k): Linear(in_features=62, out_features=62, bias=False)\n                  (to_v): Linear(in_features=62, out_features=62, bias=False)\n                  (proj): Linear(in_features=62, out_features=62, bias=True)\n                  (pos_emb): Sequential(\n                    (0): Conv2d(62, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=62, bias=False)\n                    (1): GELU()\n                    (2): Conv2d(62, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=62, bias=False)\n                  )\n                )\n                (1): PreNorm(\n                  (fn): FeedForward(\n                    (net): Sequential(\n                      (0): Conv2d(62, 248, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                      (1): GELU()\n                      (2): Conv2d(248, 248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=248, bias=False)\n                      (3): GELU()\n                      (4): Conv2d(248, 62, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                    )\n                  )\n                  (norm): LayerNorm((62,), eps=1e-05, elementwise_affine=True)\n                )\n              )\n            )\n          )\n          (1): Conv2d(62, 124, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n        )\n      )\n      (bottleneck): MSAB(\n        (blocks): ModuleList(\n          (0): ModuleList(\n            (0): MS_MSA(\n              (to_q): Linear(in_features=124, out_features=124, bias=False)\n              (to_k): Linear(in_features=124, out_features=124, bias=False)\n              (to_v): Linear(in_features=124, out_features=124, bias=False)\n              (proj): Linear(in_features=124, out_features=124, bias=True)\n              (pos_emb): Sequential(\n                (0): Conv2d(124, 124, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=124, bias=False)\n                (1): GELU()\n                (2): Conv2d(124, 124, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=124, bias=False)\n              )\n            )\n            (1): PreNorm(\n              (fn): FeedForward(\n                (net): Sequential(\n                  (0): Conv2d(124, 496, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                  (1): GELU()\n                  (2): Conv2d(496, 496, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=496, bias=False)\n                  (3): GELU()\n                  (4): Conv2d(496, 124, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                )\n              )\n              (norm): LayerNorm((124,), eps=1e-05, elementwise_affine=True)\n            )\n          )\n        )\n      )\n      (decoder_layers): ModuleList(\n        (0): ModuleList(\n          (0): ConvTranspose2d(124, 62, kernel_size=(2, 2), stride=(2, 2))\n          (1): Conv2d(124, 62, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (2): MSAB(\n            (blocks): ModuleList(\n              (0): ModuleList(\n                (0): MS_MSA(\n                  (to_q): Linear(in_features=62, out_features=62, bias=False)\n                  (to_k): Linear(in_features=62, out_features=62, bias=False)\n                  (to_v): Linear(in_features=62, out_features=62, bias=False)\n                  (proj): Linear(in_features=62, out_features=62, bias=True)\n                  (pos_emb): Sequential(\n                    (0): Conv2d(62, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=62, bias=False)\n                    (1): GELU()\n                    (2): Conv2d(62, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=62, bias=False)\n                  )\n                )\n                (1): PreNorm(\n                  (fn): FeedForward(\n                    (net): Sequential(\n                      (0): Conv2d(62, 248, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                      (1): GELU()\n                      (2): Conv2d(248, 248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=248, bias=False)\n                      (3): GELU()\n                      (4): Conv2d(248, 62, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                    )\n                  )\n                  (norm): LayerNorm((62,), eps=1e-05, elementwise_affine=True)\n                )\n              )\n            )\n          )\n        )\n        (1): ModuleList(\n          (0): ConvTranspose2d(62, 31, kernel_size=(2, 2), stride=(2, 2))\n          (1): Conv2d(62, 31, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (2): MSAB(\n            (blocks): ModuleList(\n              (0): ModuleList(\n                (0): MS_MSA(\n                  (to_q): Linear(in_features=31, out_features=31, bias=False)\n                  (to_k): Linear(in_features=31, out_features=31, bias=False)\n                  (to_v): Linear(in_features=31, out_features=31, bias=False)\n                  (proj): Linear(in_features=31, out_features=31, bias=True)\n                  (pos_emb): Sequential(\n                    (0): Conv2d(31, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=31, bias=False)\n                    (1): GELU()\n                    (2): Conv2d(31, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=31, bias=False)\n                  )\n                )\n                (1): PreNorm(\n                  (fn): FeedForward(\n                    (net): Sequential(\n                      (0): Conv2d(31, 124, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                      (1): GELU()\n                      (2): Conv2d(124, 124, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=124, bias=False)\n                      (3): GELU()\n                      (4): Conv2d(124, 31, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                    )\n                  )\n                  (norm): LayerNorm((31,), eps=1e-05, elementwise_affine=True)\n                )\n              )\n            )\n          )\n        )\n      )\n      (mapping): Conv2d(31, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (lrelu): LeakyReLU(negative_slope=0.1, inplace=True)\n    )\n    (1): MST(\n      (embedding): Conv2d(31, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (encoder_layers): ModuleList(\n        (0): ModuleList(\n          (0): MSAB(\n            (blocks): ModuleList(\n              (0): ModuleList(\n                (0): MS_MSA(\n                  (to_q): Linear(in_features=31, out_features=31, bias=False)\n                  (to_k): Linear(in_features=31, out_features=31, bias=False)\n                  (to_v): Linear(in_features=31, out_features=31, bias=False)\n                  (proj): Linear(in_features=31, out_features=31, bias=True)\n                  (pos_emb): Sequential(\n                    (0): Conv2d(31, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=31, bias=False)\n                    (1): GELU()\n                    (2): Conv2d(31, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=31, bias=False)\n                  )\n                )\n                (1): PreNorm(\n                  (fn): FeedForward(\n                    (net): Sequential(\n                      (0): Conv2d(31, 124, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                      (1): GELU()\n                      (2): Conv2d(124, 124, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=124, bias=False)\n                      (3): GELU()\n                      (4): Conv2d(124, 31, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                    )\n                  )\n                  (norm): LayerNorm((31,), eps=1e-05, elementwise_affine=True)\n                )\n              )\n            )\n          )\n          (1): Conv2d(31, 62, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n        )\n        (1): ModuleList(\n          (0): MSAB(\n            (blocks): ModuleList(\n              (0): ModuleList(\n                (0): MS_MSA(\n                  (to_q): Linear(in_features=62, out_features=62, bias=False)\n                  (to_k): Linear(in_features=62, out_features=62, bias=False)\n                  (to_v): Linear(in_features=62, out_features=62, bias=False)\n                  (proj): Linear(in_features=62, out_features=62, bias=True)\n                  (pos_emb): Sequential(\n                    (0): Conv2d(62, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=62, bias=False)\n                    (1): GELU()\n                    (2): Conv2d(62, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=62, bias=False)\n                  )\n                )\n                (1): PreNorm(\n                  (fn): FeedForward(\n                    (net): Sequential(\n                      (0): Conv2d(62, 248, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                      (1): GELU()\n                      (2): Conv2d(248, 248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=248, bias=False)\n                      (3): GELU()\n                      (4): Conv2d(248, 62, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                    )\n                  )\n                  (norm): LayerNorm((62,), eps=1e-05, elementwise_affine=True)\n                )\n              )\n            )\n          )\n          (1): Conv2d(62, 124, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n        )\n      )\n      (bottleneck): MSAB(\n        (blocks): ModuleList(\n          (0): ModuleList(\n            (0): MS_MSA(\n              (to_q): Linear(in_features=124, out_features=124, bias=False)\n              (to_k): Linear(in_features=124, out_features=124, bias=False)\n              (to_v): Linear(in_features=124, out_features=124, bias=False)\n              (proj): Linear(in_features=124, out_features=124, bias=True)\n              (pos_emb): Sequential(\n                (0): Conv2d(124, 124, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=124, bias=False)\n                (1): GELU()\n                (2): Conv2d(124, 124, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=124, bias=False)\n              )\n            )\n            (1): PreNorm(\n              (fn): FeedForward(\n                (net): Sequential(\n                  (0): Conv2d(124, 496, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                  (1): GELU()\n                  (2): Conv2d(496, 496, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=496, bias=False)\n                  (3): GELU()\n                  (4): Conv2d(496, 124, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                )\n              )\n              (norm): LayerNorm((124,), eps=1e-05, elementwise_affine=True)\n            )\n          )\n        )\n      )\n      (decoder_layers): ModuleList(\n        (0): ModuleList(\n          (0): ConvTranspose2d(124, 62, kernel_size=(2, 2), stride=(2, 2))\n          (1): Conv2d(124, 62, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (2): MSAB(\n            (blocks): ModuleList(\n              (0): ModuleList(\n                (0): MS_MSA(\n                  (to_q): Linear(in_features=62, out_features=62, bias=False)\n                  (to_k): Linear(in_features=62, out_features=62, bias=False)\n                  (to_v): Linear(in_features=62, out_features=62, bias=False)\n                  (proj): Linear(in_features=62, out_features=62, bias=True)\n                  (pos_emb): Sequential(\n                    (0): Conv2d(62, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=62, bias=False)\n                    (1): GELU()\n                    (2): Conv2d(62, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=62, bias=False)\n                  )\n                )\n                (1): PreNorm(\n                  (fn): FeedForward(\n                    (net): Sequential(\n                      (0): Conv2d(62, 248, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                      (1): GELU()\n                      (2): Conv2d(248, 248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=248, bias=False)\n                      (3): GELU()\n                      (4): Conv2d(248, 62, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                    )\n                  )\n                  (norm): LayerNorm((62,), eps=1e-05, elementwise_affine=True)\n                )\n              )\n            )\n          )\n        )\n        (1): ModuleList(\n          (0): ConvTranspose2d(62, 31, kernel_size=(2, 2), stride=(2, 2))\n          (1): Conv2d(62, 31, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (2): MSAB(\n            (blocks): ModuleList(\n              (0): ModuleList(\n                (0): MS_MSA(\n                  (to_q): Linear(in_features=31, out_features=31, bias=False)\n                  (to_k): Linear(in_features=31, out_features=31, bias=False)\n                  (to_v): Linear(in_features=31, out_features=31, bias=False)\n                  (proj): Linear(in_features=31, out_features=31, bias=True)\n                  (pos_emb): Sequential(\n                    (0): Conv2d(31, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=31, bias=False)\n                    (1): GELU()\n                    (2): Conv2d(31, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=31, bias=False)\n                  )\n                )\n                (1): PreNorm(\n                  (fn): FeedForward(\n                    (net): Sequential(\n                      (0): Conv2d(31, 124, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                      (1): GELU()\n                      (2): Conv2d(124, 124, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=124, bias=False)\n                      (3): GELU()\n                      (4): Conv2d(124, 31, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                    )\n                  )\n                  (norm): LayerNorm((31,), eps=1e-05, elementwise_affine=True)\n                )\n              )\n            )\n          )\n        )\n      )\n      (mapping): Conv2d(31, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (lrelu): LeakyReLU(negative_slope=0.1, inplace=True)\n    )\n    (2): MST(\n      (embedding): Conv2d(31, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (encoder_layers): ModuleList(\n        (0): ModuleList(\n          (0): MSAB(\n            (blocks): ModuleList(\n              (0): ModuleList(\n                (0): MS_MSA(\n                  (to_q): Linear(in_features=31, out_features=31, bias=False)\n                  (to_k): Linear(in_features=31, out_features=31, bias=False)\n                  (to_v): Linear(in_features=31, out_features=31, bias=False)\n                  (proj): Linear(in_features=31, out_features=31, bias=True)\n                  (pos_emb): Sequential(\n                    (0): Conv2d(31, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=31, bias=False)\n                    (1): GELU()\n                    (2): Conv2d(31, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=31, bias=False)\n                  )\n                )\n                (1): PreNorm(\n                  (fn): FeedForward(\n                    (net): Sequential(\n                      (0): Conv2d(31, 124, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                      (1): GELU()\n                      (2): Conv2d(124, 124, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=124, bias=False)\n                      (3): GELU()\n                      (4): Conv2d(124, 31, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                    )\n                  )\n                  (norm): LayerNorm((31,), eps=1e-05, elementwise_affine=True)\n                )\n              )\n            )\n          )\n          (1): Conv2d(31, 62, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n        )\n        (1): ModuleList(\n          (0): MSAB(\n            (blocks): ModuleList(\n              (0): ModuleList(\n                (0): MS_MSA(\n                  (to_q): Linear(in_features=62, out_features=62, bias=False)\n                  (to_k): Linear(in_features=62, out_features=62, bias=False)\n                  (to_v): Linear(in_features=62, out_features=62, bias=False)\n                  (proj): Linear(in_features=62, out_features=62, bias=True)\n                  (pos_emb): Sequential(\n                    (0): Conv2d(62, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=62, bias=False)\n                    (1): GELU()\n                    (2): Conv2d(62, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=62, bias=False)\n                  )\n                )\n                (1): PreNorm(\n                  (fn): FeedForward(\n                    (net): Sequential(\n                      (0): Conv2d(62, 248, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                      (1): GELU()\n                      (2): Conv2d(248, 248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=248, bias=False)\n                      (3): GELU()\n                      (4): Conv2d(248, 62, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                    )\n                  )\n                  (norm): LayerNorm((62,), eps=1e-05, elementwise_affine=True)\n                )\n              )\n            )\n          )\n          (1): Conv2d(62, 124, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n        )\n      )\n      (bottleneck): MSAB(\n        (blocks): ModuleList(\n          (0): ModuleList(\n            (0): MS_MSA(\n              (to_q): Linear(in_features=124, out_features=124, bias=False)\n              (to_k): Linear(in_features=124, out_features=124, bias=False)\n              (to_v): Linear(in_features=124, out_features=124, bias=False)\n              (proj): Linear(in_features=124, out_features=124, bias=True)\n              (pos_emb): Sequential(\n                (0): Conv2d(124, 124, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=124, bias=False)\n                (1): GELU()\n                (2): Conv2d(124, 124, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=124, bias=False)\n              )\n            )\n            (1): PreNorm(\n              (fn): FeedForward(\n                (net): Sequential(\n                  (0): Conv2d(124, 496, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                  (1): GELU()\n                  (2): Conv2d(496, 496, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=496, bias=False)\n                  (3): GELU()\n                  (4): Conv2d(496, 124, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                )\n              )\n              (norm): LayerNorm((124,), eps=1e-05, elementwise_affine=True)\n            )\n          )\n        )\n      )\n      (decoder_layers): ModuleList(\n        (0): ModuleList(\n          (0): ConvTranspose2d(124, 62, kernel_size=(2, 2), stride=(2, 2))\n          (1): Conv2d(124, 62, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (2): MSAB(\n            (blocks): ModuleList(\n              (0): ModuleList(\n                (0): MS_MSA(\n                  (to_q): Linear(in_features=62, out_features=62, bias=False)\n                  (to_k): Linear(in_features=62, out_features=62, bias=False)\n                  (to_v): Linear(in_features=62, out_features=62, bias=False)\n                  (proj): Linear(in_features=62, out_features=62, bias=True)\n                  (pos_emb): Sequential(\n                    (0): Conv2d(62, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=62, bias=False)\n                    (1): GELU()\n                    (2): Conv2d(62, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=62, bias=False)\n                  )\n                )\n                (1): PreNorm(\n                  (fn): FeedForward(\n                    (net): Sequential(\n                      (0): Conv2d(62, 248, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                      (1): GELU()\n                      (2): Conv2d(248, 248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=248, bias=False)\n                      (3): GELU()\n                      (4): Conv2d(248, 62, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                    )\n                  )\n                  (norm): LayerNorm((62,), eps=1e-05, elementwise_affine=True)\n                )\n              )\n            )\n          )\n        )\n        (1): ModuleList(\n          (0): ConvTranspose2d(62, 31, kernel_size=(2, 2), stride=(2, 2))\n          (1): Conv2d(62, 31, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (2): MSAB(\n            (blocks): ModuleList(\n              (0): ModuleList(\n                (0): MS_MSA(\n                  (to_q): Linear(in_features=31, out_features=31, bias=False)\n                  (to_k): Linear(in_features=31, out_features=31, bias=False)\n                  (to_v): Linear(in_features=31, out_features=31, bias=False)\n                  (proj): Linear(in_features=31, out_features=31, bias=True)\n                  (pos_emb): Sequential(\n                    (0): Conv2d(31, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=31, bias=False)\n                    (1): GELU()\n                    (2): Conv2d(31, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=31, bias=False)\n                  )\n                )\n                (1): PreNorm(\n                  (fn): FeedForward(\n                    (net): Sequential(\n                      (0): Conv2d(31, 124, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                      (1): GELU()\n                      (2): Conv2d(124, 124, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=124, bias=False)\n                      (3): GELU()\n                      (4): Conv2d(124, 31, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                    )\n                  )\n                  (norm): LayerNorm((31,), eps=1e-05, elementwise_affine=True)\n                )\n              )\n            )\n          )\n        )\n      )\n      (mapping): Conv2d(31, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (lrelu): LeakyReLU(negative_slope=0.1, inplace=True)\n    )\n  )\n  (conv_out): Conv2d(31, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n)\n\n\nUnsupported operator aten::rsub encountered 2 time(s)\nUnsupported operator aten::pad encountered 1 time(s)\nUnsupported operator aten::mul encountered 45 time(s)\nUnsupported operator aten::linalg_vector_norm encountered 30 time(s)\nUnsupported operator aten::clamp_min encountered 30 time(s)\nUnsupported operator aten::expand_as encountered 30 time(s)\nUnsupported operator aten::div encountered 30 time(s)\nUnsupported operator aten::softmax encountered 15 time(s)\nUnsupported operator aten::gelu encountered 45 time(s)\nUnsupported operator aten::add encountered 48 time(s)\nUnsupported operator aten::add_ encountered 1 time(s)\nThe following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\nbody.0.lrelu, body.1.lrelu, body.2.lrelu\n\n\nGMac:20.759536743164062\nParams:1619625\n\n\n\n# import torch\n\n# # Instantiate the model\n# model = MST_Plus_Plus()\n\n# # Load the entire checkpoint\n# checkpoint = torch.load('./test_develop_code/model_zoo/mst_plus_plus.pth')\n\n# # Extract the actual state_dict\n# model.load_state_dict(checkpoint['state_dict'])  # Load only the 'state_dict' part\n# print(model)\n\n\n\nHSCNN_Plus\n\nimport torch.nn as nn\nimport torch\nclass dfus_block(nn.Module):\n    def __init__(self, dim):\n        super(dfus_block, self).__init__()\n        self.conv1 = nn.Conv2d(dim, 128, 1, 1, 0, bias=False)\n\n        self.conv_up1 = nn.Conv2d(128, 32, 3, 1, 1, bias=False)\n        self.conv_up2 = nn.Conv2d(32, 16, 1, 1, 0, bias=False)\n\n        self.conv_down1 = nn.Conv2d(128, 32, 3, 1, 1, bias=False)\n        self.conv_down2 = nn.Conv2d(32, 16, 1, 1, 0, bias=False)\n\n        self.conv_fution = nn.Conv2d(96, 32, 1, 1, 0, bias=False)\n\n        #### activation function\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        \"\"\"\n        x: [b,c,h,w]\n        return out:[b,c,h,w]\n        \"\"\"\n        feat = self.relu(self.conv1(x))\n        feat_up1 = self.relu(self.conv_up1(feat))\n        feat_up2 = self.relu(self.conv_up2(feat_up1))\n        feat_down1 = self.relu(self.conv_down1(feat))\n        feat_down2 = self.relu(self.conv_down2(feat_down1))\n        feat_fution = torch.cat([feat_up1,feat_up2,feat_down1,feat_down2],dim=1)\n        feat_fution = self.relu(self.conv_fution(feat_fution))\n        out = torch.cat([x, feat_fution], dim=1)\n        return out\n\nclass ddfn(nn.Module):\n    def __init__(self, dim, num_blocks=78):\n        super(ddfn, self).__init__()\n\n        self.conv_up1 = nn.Conv2d(dim, 32, 3, 1, 1, bias=False)\n        self.conv_up2 = nn.Conv2d(32, 32, 1, 1, 0, bias=False)\n\n        self.conv_down1 = nn.Conv2d(dim, 32, 3, 1, 1, bias=False)\n        self.conv_down2 = nn.Conv2d(32, 32, 1, 1, 0, bias=False)\n\n        dfus_blocks = [dfus_block(dim=128+32*i) for i in range(num_blocks)]\n        self.dfus_blocks = nn.Sequential(*dfus_blocks)\n\n        #### activation function\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        \"\"\"\n        x: [b,c,h,w]\n        return out:[b,c,h,w]\n        \"\"\"\n        feat_up1 = self.relu(self.conv_up1(x))\n        feat_up2 = self.relu(self.conv_up2(feat_up1))\n        feat_down1 = self.relu(self.conv_down1(x))\n        feat_down2 = self.relu(self.conv_down2(feat_down1))\n        feat_fution = torch.cat([feat_up1,feat_up2,feat_down1,feat_down2],dim=1)\n        out = self.dfus_blocks(feat_fution)\n        return out\n\nclass HSCNN_Plus(nn.Module):\n    def __init__(self, in_channels=3, out_channels=31, num_blocks=30):\n        super(HSCNN_Plus, self).__init__()\n\n        self.ddfn = ddfn(dim=in_channels, num_blocks=num_blocks)\n        self.conv_out = nn.Conv2d(128+32*num_blocks, out_channels, 1, 1, 0, bias=False)\n\n    def forward(self, x):\n        \"\"\"\n        x: [b,c,h,w]\n        return out:[b,c,h,w]\n        \"\"\"\n        fea = self.ddfn(x)\n        out =  self.conv_out(fea)\n        return out\n\n\nfrom utils import my_summary\nmy_summary(HSCNN_Plus(), 256, 256, 3, 1)\n\nHSCNN_Plus(\n  (ddfn): ddfn(\n    (conv_up1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (conv_up2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (conv_down1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (conv_down2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (dfus_blocks): Sequential(\n      (0): dfus_block(\n        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_up1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_up2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_down1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_down2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_fution): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (relu): ReLU(inplace=True)\n      )\n      (1): dfus_block(\n        (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_up1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_up2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_down1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_down2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_fution): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (relu): ReLU(inplace=True)\n      )\n      (2): dfus_block(\n        (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_up1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_up2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_down1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_down2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_fution): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (relu): ReLU(inplace=True)\n      )\n      (3): dfus_block(\n        (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_up1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_up2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_down1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_down2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_fution): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (relu): ReLU(inplace=True)\n      )\n      (4): dfus_block(\n        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_up1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_up2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_down1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_down2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_fution): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (relu): ReLU(inplace=True)\n      )\n      (5): dfus_block(\n        (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_up1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_up2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_down1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_down2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_fution): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (relu): ReLU(inplace=True)\n      )\n      (6): dfus_block(\n        (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_up1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_up2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_down1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_down2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_fution): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (relu): ReLU(inplace=True)\n      )\n      (7): dfus_block(\n        (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_up1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_up2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_down1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_down2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_fution): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (relu): ReLU(inplace=True)\n      )\n      (8): dfus_block(\n        (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_up1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_up2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_down1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_down2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_fution): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (relu): ReLU(inplace=True)\n      )\n      (9): dfus_block(\n        (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_up1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_up2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_down1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_down2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_fution): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (relu): ReLU(inplace=True)\n      )\n      (10): dfus_block(\n        (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_up1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_up2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_down1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_down2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_fution): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (relu): ReLU(inplace=True)\n      )\n      (11): dfus_block(\n        (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_up1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_up2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_down1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_down2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_fution): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (relu): ReLU(inplace=True)\n      )\n      (12): dfus_block(\n        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_up1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_up2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_down1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_down2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_fution): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (relu): ReLU(inplace=True)\n      )\n      (13): dfus_block(\n        (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_up1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_up2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_down1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_down2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_fution): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (relu): ReLU(inplace=True)\n      )\n      (14): dfus_block(\n        (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_up1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_up2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_down1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_down2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_fution): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (relu): ReLU(inplace=True)\n      )\n      (15): dfus_block(\n        (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_up1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_up2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_down1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_down2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_fution): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (relu): ReLU(inplace=True)\n      )\n      (16): dfus_block(\n        (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_up1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_up2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_down1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_down2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_fution): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (relu): ReLU(inplace=True)\n      )\n      (17): dfus_block(\n        (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_up1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_up2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_down1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_down2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_fution): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (relu): ReLU(inplace=True)\n      )\n      (18): dfus_block(\n        (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_up1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_up2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_down1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_down2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_fution): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (relu): ReLU(inplace=True)\n      )\n      (19): dfus_block(\n        (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_up1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_up2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_down1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_down2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_fution): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (relu): ReLU(inplace=True)\n      )\n      (20): dfus_block(\n        (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_up1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_up2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_down1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_down2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_fution): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (relu): ReLU(inplace=True)\n      )\n      (21): dfus_block(\n        (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_up1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_up2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_down1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_down2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_fution): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (relu): ReLU(inplace=True)\n      )\n      (22): dfus_block(\n        (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_up1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_up2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_down1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_down2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_fution): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (relu): ReLU(inplace=True)\n      )\n      (23): dfus_block(\n        (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_up1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_up2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_down1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_down2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_fution): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (relu): ReLU(inplace=True)\n      )\n      (24): dfus_block(\n        (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_up1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_up2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_down1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_down2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_fution): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (relu): ReLU(inplace=True)\n      )\n      (25): dfus_block(\n        (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_up1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_up2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_down1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_down2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_fution): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (relu): ReLU(inplace=True)\n      )\n      (26): dfus_block(\n        (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_up1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_up2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_down1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_down2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_fution): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (relu): ReLU(inplace=True)\n      )\n      (27): dfus_block(\n        (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_up1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_up2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_down1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_down2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_fution): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (relu): ReLU(inplace=True)\n      )\n      (28): dfus_block(\n        (conv1): Conv2d(1024, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_up1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_up2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_down1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_down2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_fution): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (relu): ReLU(inplace=True)\n      )\n      (29): dfus_block(\n        (conv1): Conv2d(1056, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_up1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_up2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_down1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (conv_down2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (conv_fution): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (relu): ReLU(inplace=True)\n      )\n    )\n    (relu): ReLU(inplace=True)\n  )\n  (conv_out): Conv2d(1088, 31, kernel_size=(1, 1), stride=(1, 1), bias=False)\n)\nGMac:283.5390625\nParams:4645504",
    "crumbs": [
      "MST++: Multi-stage Spectral-wise Transformer for Efficient Spectral Reconstruction (CVPRW 2022)"
    ]
  },
  {
    "objectID": "arad1k_mst++_inference.html#evaluation-on-the-test-set",
    "href": "arad1k_mst++_inference.html#evaluation-on-the-test-set",
    "title": "MST++: Multi-stage Spectral-wise Transformer for Efficient Spectral Reconstruction (CVPRW 2022)",
    "section": "3. Evaluation on the Test Set:",
    "text": "3. Evaluation on the Test Set:\n\nDownload the pretrained model zoo from (Google Drive / Baidu Disk, code: mst1) and place them to /MST-plus-plus/test_challenge_code/model_zoo/.\nRun the following command to test the model on the testing RGB images.\n\n\ncd test_challenge_code\n\n/root/Ninjalabo/HSI/MST-plus-plus/MST-plus-plus/test_challenge_code\n\n\n\nTest MST++ (Transformer Model)\n\n!python test.py --data_root ../dataset/  --method mst_plus_plus --pretrained_model_path ./model_zoo/mst_plus_plus.pth --outf ./exp/mst_plus_plus/  --gpu_id 0\n\nload model from ./model_zoo/mst_plus_plus.pth\n/root/Ninjalabo/HSI/MST-plus-plus/MST-plus-plus/test_challenge_code/architecture/__init__.py:38: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(pretrained_model_path)\nARAD_1K_0951.jpg\nARAD_1K_0952.jpg\nARAD_1K_0953.jpg\nARAD_1K_0954.jpg\nARAD_1K_0955.jpg\nARAD_1K_0956.jpg\nARAD_1K_0957.jpg\nARAD_1K_0958.jpg\nARAD_1K_0959.jpg\nARAD_1K_0960.jpg\nARAD_1K_0961.jpg\nARAD_1K_0962.jpg\nARAD_1K_0963.jpg\nARAD_1K_0964.jpg\nARAD_1K_0965.jpg\nARAD_1K_0966.jpg\nARAD_1K_0967.jpg\nARAD_1K_0968.jpg\nARAD_1K_0969.jpg\nARAD_1K_0970.jpg\nARAD_1K_0971.jpg\nARAD_1K_0972.jpg\nARAD_1K_0973.jpg\nARAD_1K_0974.jpg\nARAD_1K_0975.jpg\nARAD_1K_0976.jpg\nARAD_1K_0977.jpg\nARAD_1K_0978.jpg\nARAD_1K_0979.jpg\nARAD_1K_0980.jpg\nARAD_1K_0981.jpg\nARAD_1K_0982.jpg\nARAD_1K_0983.jpg\nARAD_1K_0984.jpg\nARAD_1K_0985.jpg\nARAD_1K_0986.jpg\nARAD_1K_0987.jpg\nARAD_1K_0988.jpg\nARAD_1K_0989.jpg\nARAD_1K_0990.jpg\nARAD_1K_0991.jpg\nARAD_1K_0992.jpg\nARAD_1K_0993.jpg\nARAD_1K_0994.jpg\nARAD_1K_0995.jpg\nARAD_1K_0996.jpg\nARAD_1K_0997.jpg\nARAD_1K_0998.jpg\nARAD_1K_0999.jpg\nARAD_1K_1000.jpg\n./exp/mst_plus_plus/\n./exp/mst_plus_plus//submission\nCropping files from input directory\n100%|███████████████████████████████████████████| 50/50 [00:16&lt;00:00,  3.02it/s]\nCompressing submission\n100%|███████████████████████████████████████████| 50/50 [00:08&lt;00:00,  6.06it/s]\nRemoving temporary files\n100%|█████████████████████████████████████████| 50/50 [00:00&lt;00:00, 1098.49it/s]\nVerifying submission size -  SUCCESS!\nSubmission generated @ ./exp/mst_plus_plus//submission/submission.zip\n\n\n\n\nTest HSCNN+\n\n!python test.py --data_root ../dataset/  --method hscnn_plus --pretrained_model_path ./model_zoo/hscnn_plus.pth --outf ./exp/hscnn_plus/  --gpu_id 0\n\nload model from ./model_zoo/hscnn_plus.pth\n/root/Ninjalabo/HSI/MST-plus-plus/MST-plus-plus/test_challenge_code/architecture/__init__.py:38: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(pretrained_model_path)\nARAD_1K_0951.jpg\nARAD_1K_0952.jpg\nARAD_1K_0953.jpg\nARAD_1K_0954.jpg\nARAD_1K_0955.jpg\nARAD_1K_0956.jpg\nARAD_1K_0957.jpg\nARAD_1K_0958.jpg\nARAD_1K_0959.jpg\nARAD_1K_0960.jpg\nARAD_1K_0961.jpg\nARAD_1K_0962.jpg\nARAD_1K_0963.jpg\nARAD_1K_0964.jpg\nARAD_1K_0965.jpg\nARAD_1K_0966.jpg\nARAD_1K_0967.jpg\nARAD_1K_0968.jpg\nARAD_1K_0969.jpg\nARAD_1K_0970.jpg\nARAD_1K_0971.jpg\nARAD_1K_0972.jpg\nARAD_1K_0973.jpg\nARAD_1K_0974.jpg\nARAD_1K_0975.jpg\nARAD_1K_0976.jpg\nARAD_1K_0977.jpg\nARAD_1K_0978.jpg\nARAD_1K_0979.jpg\nARAD_1K_0980.jpg\nARAD_1K_0981.jpg\nARAD_1K_0982.jpg\nARAD_1K_0983.jpg\nARAD_1K_0984.jpg\nARAD_1K_0985.jpg\nARAD_1K_0986.jpg\nARAD_1K_0987.jpg\nARAD_1K_0988.jpg\nARAD_1K_0989.jpg\nARAD_1K_0990.jpg\nARAD_1K_0991.jpg\nARAD_1K_0992.jpg\nARAD_1K_0993.jpg\nARAD_1K_0994.jpg\nARAD_1K_0995.jpg\nARAD_1K_0996.jpg\nARAD_1K_0997.jpg\nARAD_1K_0998.jpg\nARAD_1K_0999.jpg\nARAD_1K_1000.jpg\n./exp/hscnn_plus/\n./exp/hscnn_plus//submission\nCropping files from input directory\n100%|███████████████████████████████████████████| 50/50 [00:16&lt;00:00,  3.06it/s]\nCompressing submission\n100%|███████████████████████████████████████████| 50/50 [00:08&lt;00:00,  5.94it/s]\nRemoving temporary files\n100%|█████████████████████████████████████████| 50/50 [00:00&lt;00:00, 1004.40it/s]\nVerifying submission size -  SUCCESS!\nSubmission generated @ ./exp/hscnn_plus//submission/submission.zip\n\n\nThe results and submission.zip will be saved in /MST-plus-plus/test_challenge_code/exp/.",
    "crumbs": [
      "MST++: Multi-stage Spectral-wise Transformer for Efficient Spectral Reconstruction (CVPRW 2022)"
    ]
  },
  {
    "objectID": "arad1k_mst++_inference.html#prediction",
    "href": "arad1k_mst++_inference.html#prediction",
    "title": "MST++: Multi-stage Spectral-wise Transformer for Efficient Spectral Reconstruction (CVPRW 2022)",
    "section": "4.Prediction:",
    "text": "4.Prediction:\n\nDownload the pretrained model zoo from (Google Drive / Baidu Disk, code: mst1) and place them to /MST-plus-plus/predict_code/model_zoo/.\nRun the following command to reconstruct your own RGB image.\n\n\ncd ..\n\n/root/Ninjalabo/HSI/MST-plus-plus/MST-plus-plus\n\n\n\ncd predict_code/\n\n/root/Ninjalabo/HSI/MST-plus-plus/MST-plus-plus/predict_code\n\n\n\nReconstruct by MST++\n\n!python test.py --rgb_path ./demo/ARAD_1K_0912.jpg  --method mst_plus_plus --pretrained_model_path ./model_zoo/mst_plus_plus.pth --outf ./exp/mst_plus_plus/  --gpu_id 0\n\nload model from ./model_zoo/mst_plus_plus.pth\n/root/Ninjalabo/HSI/MST-plus-plus/MST-plus-plus/predict_code/architecture/__init__.py:38: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(pretrained_model_path)\nReconstructing ./demo/ARAD_1K_0912.jpg\nThe reconstructed hyper spectral image are saved as ./exp/mst_plus_plus/ARAD_1K_0912.mat.\n\n\n\n\nReconstruct by HSCNN+\n\n!python test.py --rgb_path ./demo/ARAD_1K_1000.jpg  --method hscnn_plus --pretrained_model_path ./model_zoo/hscnn_plus.pth --outf ./exp/hscnn_plus/  --gpu_id 0\n\nload model from ./model_zoo/hscnn_plus.pth\n/root/Ninjalabo/HSI/MST-plus-plus/MST-plus-plus/predict_code/architecture/__init__.py:38: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(pretrained_model_path)\nReconstructing ./demo/ARAD_1K_1000.jpg\nThe reconstructed hyper spectral image are saved as ./exp/hscnn_plus/ARAD_1K_1000.mat.\n\n\nYou can replace ‘./demo/ARAD_1K_0912.jpg’ with your RGB image path. The reconstructed results will be saved in /MST-plus-plus/predict_code/exp/.",
    "crumbs": [
      "MST++: Multi-stage Spectral-wise Transformer for Efficient Spectral Reconstruction (CVPRW 2022)"
    ]
  },
  {
    "objectID": "arad1k_mst++_inference.html#visualization",
    "href": "arad1k_mst++_inference.html#visualization",
    "title": "MST++: Multi-stage Spectral-wise Transformer for Efficient Spectral Reconstruction (CVPRW 2022)",
    "section": "5. Visualization",
    "text": "5. Visualization\n\nPut the reconstruted HSI in visualization/simulation_results/results/.\nGenerate the RGB images of the reconstructed HSIs\n\nPlease put the reconstructed HSI from /MST-plus-plus/predict_code/exp/. to visualization/simulation_results/results/ and rename it as method.mat, e.g., mst_s.mat. (Please read README.mdfile in each folder)\n\n# cd visualization\n\n/root/Ninjalabo/HSI/MST-plus-plus/MST-plus-plus/visualization\n\n\n\n# Run show_simulation.m\n\n\ncd ..\n\n/root/Ninjalabo/HSI/MST-plus-plus/MST-plus-plus\n\n\n\nimport h5py\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef load_and_visualize_mat_v7_3(file_path):\n    try:\n        # Load the .mat file\n        with h5py.File(file_path, 'r') as mat_file:\n            # Extract the 'cube' dataset\n            cube = mat_file['cube'][()]  # Load the full dataset into a NumPy array\n            \n            # Check the shape of the data cube\n            print(f\"Data cube shape: {cube.shape}\")\n\n            # Visualize one of the spectral bands (e.g., the 15th band)\n            band_index = 15  # Choose a band to visualize\n            if band_index &lt; cube.shape[0]:\n                band_data = cube[band_index, :, :]\n                band_data_corrected = np.transpose(band_data)  # Correct rotation\n                \n                plt.imshow(band_data_corrected, cmap='gray')\n                plt.title(f'Spectral Band {band_index + 1}')\n                plt.colorbar()\n                plt.show()\n            else:\n                print(f\"Band index {band_index} is out of range for cube with shape {cube.shape}\")\n\n            # Visualize an average over all bands (create an RGB-like view)\n            average_band = np.mean(cube, axis=0)\n            average_band_corrected = np.transpose(average_band)  # Correct rotation\n            \n            plt.imshow(average_band_corrected, cmap='gray')\n            plt.title('Average Over All Bands')\n            plt.colorbar()\n            plt.show()\n\n    except FileNotFoundError:\n        print(\"The specified file was not found. Please check the file path.\")\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n\n\n# Replace with the path to your .mat file\nfile_path = \"./visualization/simulation_results/results/mst_plus_plus.mat\"\nload_and_visualize_mat_v7_3(file_path)\n\nData cube shape: (31, 512, 482)",
    "crumbs": [
      "MST++: Multi-stage Spectral-wise Transformer for Efficient Spectral Reconstruction (CVPRW 2022)"
    ]
  },
  {
    "objectID": "arad1k_mst++_inference.html#evaluation-on-the-validation-set",
    "href": "arad1k_mst++_inference.html#evaluation-on-the-validation-set",
    "title": "MST++: Multi-stage Spectral-wise Transformer for Efficient Spectral Reconstruction (CVPRW 2022)",
    "section": "6. Evaluation on the Validation Set:",
    "text": "6. Evaluation on the Validation Set:\n\nDownload the pretrained model zoo from (Google Drive / Baidu Disk, code: mst1) and place them to /MST-plus-plus/test_develop_code/model_zoo/.\nRun the following command to test the model on the validation RGB images.\n\n\ncd test_develop_code\n\n/root/Ninjalabo/HSI/MST-plus-plus/MST-plus-plus/test_develop_code\n\n\n\nls\n\n__pycache__/   exp/            model_zoo/  utils.py\narchitecture/  hsi_dataset.py  test.py\n\n\n\nTest MST++\n\n!python test.py --data_root ../dataset/  --method mst_plus_plus --pretrained_model_path ./model_zoo/mst_plus_plus.pth --outf ./exp/mst_plus_plus/  --gpu_id 0\n\nlen(hyper_valid) of ntire2022 dataset:50\nlen(bgr_valid) of ntire2022 dataset:50\nNtire2022 scene 0 is loaded.\nNtire2022 scene 1 is loaded.\nNtire2022 scene 2 is loaded.\nNtire2022 scene 3 is loaded.\nNtire2022 scene 4 is loaded.\nNtire2022 scene 5 is loaded.\nNtire2022 scene 6 is loaded.\nNtire2022 scene 7 is loaded.\nNtire2022 scene 8 is loaded.\nNtire2022 scene 9 is loaded.\nNtire2022 scene 10 is loaded.\nNtire2022 scene 11 is loaded.\nNtire2022 scene 12 is loaded.\nNtire2022 scene 13 is loaded.\nNtire2022 scene 14 is loaded.\nNtire2022 scene 15 is loaded.\nNtire2022 scene 16 is loaded.\nNtire2022 scene 17 is loaded.\nNtire2022 scene 18 is loaded.\nNtire2022 scene 19 is loaded.\nNtire2022 scene 20 is loaded.\nNtire2022 scene 21 is loaded.\nNtire2022 scene 22 is loaded.\nNtire2022 scene 23 is loaded.\nNtire2022 scene 24 is loaded.\nNtire2022 scene 25 is loaded.\nNtire2022 scene 26 is loaded.\nNtire2022 scene 27 is loaded.\nNtire2022 scene 28 is loaded.\nNtire2022 scene 29 is loaded.\nNtire2022 scene 30 is loaded.\nNtire2022 scene 31 is loaded.\nNtire2022 scene 32 is loaded.\nNtire2022 scene 33 is loaded.\nNtire2022 scene 34 is loaded.\nNtire2022 scene 35 is loaded.\nNtire2022 scene 36 is loaded.\nNtire2022 scene 37 is loaded.\nNtire2022 scene 38 is loaded.\nNtire2022 scene 39 is loaded.\nNtire2022 scene 40 is loaded.\nNtire2022 scene 41 is loaded.\nNtire2022 scene 42 is loaded.\nNtire2022 scene 43 is loaded.\nNtire2022 scene 44 is loaded.\nNtire2022 scene 45 is loaded.\nNtire2022 scene 46 is loaded.\nNtire2022 scene 47 is loaded.\nNtire2022 scene 48 is loaded.\nNtire2022 scene 49 is loaded.\nload model from ./model_zoo/mst_plus_plus.pth\nmethod:mst_plus_plus, mrae:0.1645723432302475, rmse:0.02476534992456436, psnr:18.959978103637695\n\n\n\n\nTest HSCNN+\n\n!python test.py --data_root ../dataset/  --method hscnn_plus --pretrained_model_path ./model_zoo/hscnn_plus.pth --outf ./exp/hscnn_plus/  --gpu_id 0\n\nlen(hyper_valid) of ntire2022 dataset:50\nlen(bgr_valid) of ntire2022 dataset:50\nNtire2022 scene 0 is loaded.\nNtire2022 scene 1 is loaded.\nNtire2022 scene 2 is loaded.\nNtire2022 scene 3 is loaded.\nNtire2022 scene 4 is loaded.\nNtire2022 scene 5 is loaded.\nNtire2022 scene 6 is loaded.\nNtire2022 scene 7 is loaded.\nNtire2022 scene 8 is loaded.\nNtire2022 scene 9 is loaded.\nNtire2022 scene 10 is loaded.\nNtire2022 scene 11 is loaded.\nNtire2022 scene 12 is loaded.\nNtire2022 scene 13 is loaded.\nNtire2022 scene 14 is loaded.\nNtire2022 scene 15 is loaded.\nNtire2022 scene 16 is loaded.\nNtire2022 scene 17 is loaded.\nNtire2022 scene 18 is loaded.\nNtire2022 scene 19 is loaded.\nNtire2022 scene 20 is loaded.\nNtire2022 scene 21 is loaded.\nNtire2022 scene 22 is loaded.\nNtire2022 scene 23 is loaded.\nNtire2022 scene 24 is loaded.\nNtire2022 scene 25 is loaded.\nNtire2022 scene 26 is loaded.\nNtire2022 scene 27 is loaded.\nNtire2022 scene 28 is loaded.\nNtire2022 scene 29 is loaded.\nNtire2022 scene 30 is loaded.\nNtire2022 scene 31 is loaded.\nNtire2022 scene 32 is loaded.\nNtire2022 scene 33 is loaded.\nNtire2022 scene 34 is loaded.\nNtire2022 scene 35 is loaded.\nNtire2022 scene 36 is loaded.\nNtire2022 scene 37 is loaded.\nNtire2022 scene 38 is loaded.\nNtire2022 scene 39 is loaded.\nNtire2022 scene 40 is loaded.\nNtire2022 scene 41 is loaded.\nNtire2022 scene 42 is loaded.\nNtire2022 scene 43 is loaded.\nNtire2022 scene 44 is loaded.\nNtire2022 scene 45 is loaded.\nNtire2022 scene 46 is loaded.\nNtire2022 scene 47 is loaded.\nNtire2022 scene 48 is loaded.\nNtire2022 scene 49 is loaded.\nload model from ./model_zoo/hscnn_plus.pth\nmethod:hscnn_plus, mrae:0.38142824172973633, rmse:0.05884117633104324, psnr:26.3621826171875\n\n\n\n\nTest HDNet\n\n!python test.py --data_root ../dataset/  --method hdnet --pretrained_model_path ./model_zoo/hdnet.pth --outf ./exp/hdnet/  --gpu_id 0\n\nlen(hyper_valid) of ntire2022 dataset:50\nlen(bgr_valid) of ntire2022 dataset:50\nNtire2022 scene 0 is loaded.\nNtire2022 scene 1 is loaded.\nNtire2022 scene 2 is loaded.\nNtire2022 scene 3 is loaded.\nNtire2022 scene 4 is loaded.\nNtire2022 scene 5 is loaded.\nNtire2022 scene 6 is loaded.\nNtire2022 scene 7 is loaded.\nNtire2022 scene 8 is loaded.\nNtire2022 scene 9 is loaded.\nNtire2022 scene 10 is loaded.\nNtire2022 scene 11 is loaded.\nNtire2022 scene 12 is loaded.\nNtire2022 scene 13 is loaded.\nNtire2022 scene 14 is loaded.\nNtire2022 scene 15 is loaded.\nNtire2022 scene 16 is loaded.\nNtire2022 scene 17 is loaded.\nNtire2022 scene 18 is loaded.\nNtire2022 scene 19 is loaded.\nNtire2022 scene 20 is loaded.\nNtire2022 scene 21 is loaded.\nNtire2022 scene 22 is loaded.\nNtire2022 scene 23 is loaded.\nNtire2022 scene 24 is loaded.\nNtire2022 scene 25 is loaded.\nNtire2022 scene 26 is loaded.\nNtire2022 scene 27 is loaded.\nNtire2022 scene 28 is loaded.\nNtire2022 scene 29 is loaded.\nNtire2022 scene 30 is loaded.\nNtire2022 scene 31 is loaded.\nNtire2022 scene 32 is loaded.\nNtire2022 scene 33 is loaded.\nNtire2022 scene 34 is loaded.\nNtire2022 scene 35 is loaded.\nNtire2022 scene 36 is loaded.\nNtire2022 scene 37 is loaded.\nNtire2022 scene 38 is loaded.\nNtire2022 scene 39 is loaded.\nNtire2022 scene 40 is loaded.\nNtire2022 scene 41 is loaded.\nNtire2022 scene 42 is loaded.\nNtire2022 scene 43 is loaded.\nNtire2022 scene 44 is loaded.\nNtire2022 scene 45 is loaded.\nNtire2022 scene 46 is loaded.\nNtire2022 scene 47 is loaded.\nNtire2022 scene 48 is loaded.\nNtire2022 scene 49 is loaded.\nload model from ./model_zoo/hdnet.pth\nmethod:hdnet, mrae:0.20484349131584167, rmse:0.03170507773756981, psnr:32.129539489746094",
    "crumbs": [
      "MST++: Multi-stage Spectral-wise Transformer for Efficient Spectral Reconstruction (CVPRW 2022)"
    ]
  },
  {
    "objectID": "arad1k_mst++_inference.html#traning",
    "href": "arad1k_mst++_inference.html#traning",
    "title": "MST++: Multi-stage Spectral-wise Transformer for Efficient Spectral Reconstruction (CVPRW 2022)",
    "section": "Traning",
    "text": "Traning\n\ncd train_code/\n\n/root/Ninjalabo/HSI/MST-plus-plus/MST-plus-plus/train_code\n\n\n\n# !python train.py --method mst_plus_plus  --batch_size 20 --end_epoch 300 --init_lr 4e-4 --outf ./exp/mst_plus_plus/ --data_root ../dataset/  --patch_size 128 --stride 8  --gpu_id 0",
    "crumbs": [
      "MST++: Multi-stage Spectral-wise Transformer for Efficient Spectral Reconstruction (CVPRW 2022)"
    ]
  },
  {
    "objectID": "arad1k_mst++_inference.html#supported-algorithms",
    "href": "arad1k_mst++_inference.html#supported-algorithms",
    "title": "MST++: Multi-stage Spectral-wise Transformer for Efficient Spectral Reconstruction (CVPRW 2022)",
    "section": "Supported Algorithms:",
    "text": "Supported Algorithms:\n\nFigure: PSNR-Params-FLOPS comparisons with other spectral reconstruction algorithms.\nFLOPS (Floating Point Operations Per Second): computational complexity\nPSNR ( Peak signal-to-noise ratio): performance\nThe circle radius is Params: memory cost\nMST++: surpasses other methods while requiring significantly cheaper FLOPS and Params.",
    "crumbs": [
      "MST++: Multi-stage Spectral-wise Transformer for Efficient Spectral Reconstruction (CVPRW 2022)"
    ]
  },
  {
    "objectID": "arad1k_mst++_inference.html#comparison-with-other-methods",
    "href": "arad1k_mst++_inference.html#comparison-with-other-methods",
    "title": "MST++: Multi-stage Spectral-wise Transformer for Efficient Spectral Reconstruction (CVPRW 2022)",
    "section": "Comparison with Other Methods",
    "text": "Comparison with Other Methods\n\nMST++ achieved the highest PSNR of 34.32 dB on the validation set, a significant improvement over previous methods.\nIn terms of efficiency, MST++ required only 23.05 FLOPS, making it one of the most lightweight models.\nMST++ achieved the lowest error rates- RMSE (root mean square error), and MRAE (mean relative absolute error).",
    "crumbs": [
      "MST++: Multi-stage Spectral-wise Transformer for Efficient Spectral Reconstruction (CVPRW 2022)"
    ]
  },
  {
    "objectID": "arad1k_mst++_inference.html#experiment-results",
    "href": "arad1k_mst++_inference.html#experiment-results",
    "title": "MST++: Multi-stage Spectral-wise Transformer for Efficient Spectral Reconstruction (CVPRW 2022)",
    "section": "Experiment Results",
    "text": "Experiment Results\nThe dataset for evaluation consisted of RGB-HSI pairs with 31 spectral bands. #### Quantitative Results: * MST++ outperformed all other models in metrics like PSNR, RMSE, and MRAE. #### Qualitative Results: * Visual comparisons show that MST++ produces sharper, more detailed hyperspectral reconstructions with fewer artifacts. * It excels in preserving spatial smoothness and restoring spectral consistency.",
    "crumbs": [
      "MST++: Multi-stage Spectral-wise Transformer for Efficient Spectral Reconstruction (CVPRW 2022)"
    ]
  }
]