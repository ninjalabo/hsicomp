{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixed_Precision_Training_Quantization\n",
    "\n",
    "> We will be implementing Mixed-Precision Training Quantization model compression here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp quantization_fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-image in /root/miniconda3/envs/jupyter_env/lib/python3.12/site-packages (0.24.0)\n",
      "Requirement already satisfied: numpy>=1.23 in /root/miniconda3/envs/jupyter_env/lib/python3.12/site-packages (from scikit-image) (2.0.2)\n",
      "Requirement already satisfied: scipy>=1.9 in /root/miniconda3/envs/jupyter_env/lib/python3.12/site-packages (from scikit-image) (1.14.1)\n",
      "Requirement already satisfied: networkx>=2.8 in /root/miniconda3/envs/jupyter_env/lib/python3.12/site-packages (from scikit-image) (3.4.2)\n",
      "Requirement already satisfied: pillow>=9.1 in /root/miniconda3/envs/jupyter_env/lib/python3.12/site-packages (from scikit-image) (11.0.0)\n",
      "Requirement already satisfied: imageio>=2.33 in /root/miniconda3/envs/jupyter_env/lib/python3.12/site-packages (from scikit-image) (2.36.0)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /root/miniconda3/envs/jupyter_env/lib/python3.12/site-packages (from scikit-image) (2024.9.20)\n",
      "Requirement already satisfied: packaging>=21 in /root/miniconda3/envs/jupyter_env/lib/python3.12/site-packages (from scikit-image) (24.1)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in /root/miniconda3/envs/jupyter_env/lib/python3.12/site-packages (from scikit-image) (0.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# List of required packages\n",
    "required_packages = [\n",
    "    \"fastai\",  # fastai includes torch, torchvision, and fastprogress\n",
    "    \"numpy\",\n",
    "    \"pandas\",\n",
    "    \"scikit-image\",\n",
    "    \"tqdm\"\n",
    "]\n",
    "# Install required packages\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        __import__(package)\n",
    "    except ImportError:\n",
    "        install(package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastai.vision.all import *  # Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import logging\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from tqdm import tqdm, trange\n",
    "import torch.quantization\n",
    "import torch.multiprocessing as mp\n",
    "sys.path.append('/root/hsi-compression/models/')\n",
    "from cae1d import ConvolutionalAutoencoder1D\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True,max_split_size_mb:64\"\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'  # Ensure device is set to GPU if available\n",
    "mp.set_start_method('spawn', force=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "filepath = \"/root/hsi-compression/results/weights/cae1d_8bpppc.pth.tar\"\n",
    "\n",
    "def load_model_with_weights(filepath):\n",
    "    \"\"\"Load a ConvolutionalAutoencoder1D model with pretrained weights.\"\"\"\n",
    "    model = ConvolutionalAutoencoder1D().to(device)\n",
    "    model.load_state_dict(torch.load(filepath, map_location=device, weights_only=True))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Setup logging to help with debugging\n",
    "logging.basicConfig(level=logging.ERROR, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Base directory where the .npy files are stored\n",
    "base_directory = '/root/hsi-compression/datasets/hyspecnet-11k/patches/'\n",
    "\n",
    "# Utility functions\n",
    "def load_paths(csv_file):\n",
    "    df = pd.read_csv(csv_file, header=None)\n",
    "    file_paths = [os.path.join(base_directory, x.strip()) for x in df[0]]\n",
    "    return file_paths\n",
    "\n",
    "def transform_sample(sample):\n",
    "    return (sample - np.mean(sample)) / np.std(sample)\n",
    "\n",
    "# Dataset class\n",
    "class NPYDataset(Dataset):\n",
    "    def __init__(self, file_paths, transform=None):\n",
    "        self.file_paths = file_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_paths[idx]\n",
    "        sample = np.load(file_path)\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return torch.from_numpy(sample).float().to(device), torch.from_numpy(sample).float().to(device)\n",
    "\n",
    "csv_file_path = '/root/hsi-compression/datasets/hyspecnet-11k/splits/easy/test.csv'\n",
    "file_paths = load_paths(csv_file_path) \n",
    "\n",
    "# Initialize the dataset and DataLoader\n",
    "dataset = NPYDataset(file_paths, transform=transform_sample)\n",
    "subset_size = int(len(dataset) * 0.5)\n",
    "indices = torch.randperm(len(dataset))[:subset_size]\n",
    "subset = Subset(dataset, indices)\n",
    "\n",
    "# Create DataLoader using the subset\n",
    "dataloader = DataLoader(subset, batch_size=1, shuffle=True, num_workers=4, pin_memory=True)  # pin_memory for efficient transfer to CUDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Mixed precision training\n",
    "def mixed_precision_training(model, dataloader):\n",
    "    scaler = GradScaler()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = torch.nn.MSELoss().to(device)\n",
    "    model.train()\n",
    "    for epoch in range(3):\n",
    "        for inputs, _ in tqdm(dataloader, desc=f\"Epoch {epoch+1}\"):\n",
    "            optimizer.zero_grad()\n",
    "            with autocast(device=device):\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, inputs)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    criterion = torch.nn.MSELoss().to(device)  # Ensure loss computation is on GPU\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader, desc=\"Evaluating model\", leave=False):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)  # Ensure labels are of the same shape as outputs\n",
    "            losses.append(loss.item())\n",
    "            total += labels.size(0)\n",
    "    mean_loss = sum(losses) / len(losses)\n",
    "    return mean_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def evaluate_ssim(model, dataloader):\n",
    "    model.eval()\n",
    "    ssim_scores = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in dataloader:\n",
    "            output = model(data)\n",
    "            output_np = output.cpu().detach().numpy()  # Transfer data back to CPU for SSIM computation\n",
    "            target_np = target.cpu().detach().numpy()\n",
    "            for o, t in zip(output_np, target_np):\n",
    "                score = ssim(o, t, data_range=t.max() - t.min())\n",
    "                ssim_scores.append(score)\n",
    "    average_ssim = np.mean(ssim_scores)\n",
    "    return average_ssim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def evaluate_latency(model, dataloader, num_iterations=100):\n",
    "    start_time = time.time()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (data, _) in enumerate(dataloader):\n",
    "            if i >= num_iterations:\n",
    "                break\n",
    "            _ = model(data)\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    average_time_per_batch = total_time / num_iterations\n",
    "    return average_time_per_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU memory\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def save_model(model, filepath):\n",
    "    \"\"\"Save model state to a specified file path.\"\"\"\n",
    "    torch.save(model.state_dict(), filepath)\n",
    "\n",
    "def get_model_size(filepath):\n",
    "    \"\"\"Calculate the size of the model file.\"\"\"\n",
    "    size_bytes = os.path.getsize(filepath)\n",
    "    return size_bytes / (1024 * 1024)  # Convert bytes to megabytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ConvolutionalAutoencoder1D:\n\tUnexpected key(s) in state_dict: \"encoder.0.activation_post_process.eps\", \"encoder.0.activation_post_process.histogram\", \"encoder.0.activation_post_process.min_val\", \"encoder.0.activation_post_process.max_val\", \"encoder.1.activation_post_process.eps\", \"encoder.1.activation_post_process.histogram\", \"encoder.1.activation_post_process.min_val\", \"encoder.1.activation_post_process.max_val\", \"encoder.3.activation_post_process.eps\", \"encoder.3.activation_post_process.histogram\", \"encoder.3.activation_post_process.min_val\", \"encoder.3.activation_post_process.max_val\", \"encoder.4.activation_post_process.eps\", \"encoder.4.activation_post_process.histogram\", \"encoder.4.activation_post_process.min_val\", \"encoder.4.activation_post_process.max_val\", \"encoder.6.activation_post_process.eps\", \"encoder.6.activation_post_process.histogram\", \"encoder.6.activation_post_process.min_val\", \"encoder.6.activation_post_process.max_val\", \"encoder.7.activation_post_process.eps\", \"encoder.7.activation_post_process.histogram\", \"encoder.7.activation_post_process.min_val\", \"encoder.7.activation_post_process.max_val\", \"encoder.8.activation_post_process.eps\", \"encoder.8.activation_post_process.histogram\", \"encoder.8.activation_post_process.min_val\", \"encoder.8.activation_post_process.max_val\", \"encoder.9.activation_post_process.eps\", \"encoder.9.activation_post_process.histogram\", \"encoder.9.activation_post_process.min_val\", \"encoder.9.activation_post_process.max_val\", \"decoder.0.activation_post_process.eps\", \"decoder.0.activation_post_process.histogram\", \"decoder.0.activation_post_process.min_val\", \"decoder.0.activation_post_process.max_val\", \"decoder.1.activation_post_process.eps\", \"decoder.1.activation_post_process.histogram\", \"decoder.1.activation_post_process.min_val\", \"decoder.1.activation_post_process.max_val\", \"decoder.2.activation_post_process.eps\", \"decoder.2.activation_post_process.histogram\", \"decoder.2.activation_post_process.min_val\", \"decoder.2.activation_post_process.max_val\", \"decoder.3.activation_post_process.eps\", \"decoder.3.activation_post_process.histogram\", \"decoder.3.activation_post_process.min_val\", \"decoder.3.activation_post_process.max_val\", \"decoder.5.activation_post_process.eps\", \"decoder.5.activation_post_process.histogram\", \"decoder.5.activation_post_process.min_val\", \"decoder.5.activation_post_process.max_val\", \"decoder.6.activation_post_process.eps\", \"decoder.6.activation_post_process.histogram\", \"decoder.6.activation_post_process.min_val\", \"decoder.6.activation_post_process.max_val\", \"decoder.8.activation_post_process.eps\", \"decoder.8.activation_post_process.histogram\", \"decoder.8.activation_post_process.min_val\", \"decoder.8.activation_post_process.max_val\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Load your model\u001b[39;00m\n\u001b[1;32m      8\u001b[0m filepath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/root/hsi-compression/results/weights/cae1d_8bpppc.pth.tar\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 9\u001b[0m model \u001b[38;5;241m=\u001b[39m load_model_with_weights(filepath)  \u001b[38;5;66;03m# Make sure this function is correctly defined\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Now evaluate the model\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal Model Accuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, evaluate_model(model, dataloader))\n",
      "Cell \u001b[0;32mIn[5], line 8\u001b[0m, in \u001b[0;36mload_model_with_weights\u001b[0;34m(filepath)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load a ConvolutionalAutoencoder1D model with pretrained weights.\"\"\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m ConvolutionalAutoencoder1D()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 8\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(filepath, map_location\u001b[38;5;241m=\u001b[39mdevice, weights_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[1;32m      9\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/miniconda3/envs/jupyter_env/lib/python3.12/site-packages/torch/nn/modules/module.py:2584\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2576\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2577\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   2578\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2579\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[1;32m   2580\u001b[0m             ),\n\u001b[1;32m   2581\u001b[0m         )\n\u001b[1;32m   2583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2584\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   2585\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2586\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[1;32m   2587\u001b[0m         )\n\u001b[1;32m   2588\u001b[0m     )\n\u001b[1;32m   2589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ConvolutionalAutoencoder1D:\n\tUnexpected key(s) in state_dict: \"encoder.0.activation_post_process.eps\", \"encoder.0.activation_post_process.histogram\", \"encoder.0.activation_post_process.min_val\", \"encoder.0.activation_post_process.max_val\", \"encoder.1.activation_post_process.eps\", \"encoder.1.activation_post_process.histogram\", \"encoder.1.activation_post_process.min_val\", \"encoder.1.activation_post_process.max_val\", \"encoder.3.activation_post_process.eps\", \"encoder.3.activation_post_process.histogram\", \"encoder.3.activation_post_process.min_val\", \"encoder.3.activation_post_process.max_val\", \"encoder.4.activation_post_process.eps\", \"encoder.4.activation_post_process.histogram\", \"encoder.4.activation_post_process.min_val\", \"encoder.4.activation_post_process.max_val\", \"encoder.6.activation_post_process.eps\", \"encoder.6.activation_post_process.histogram\", \"encoder.6.activation_post_process.min_val\", \"encoder.6.activation_post_process.max_val\", \"encoder.7.activation_post_process.eps\", \"encoder.7.activation_post_process.histogram\", \"encoder.7.activation_post_process.min_val\", \"encoder.7.activation_post_process.max_val\", \"encoder.8.activation_post_process.eps\", \"encoder.8.activation_post_process.histogram\", \"encoder.8.activation_post_process.min_val\", \"encoder.8.activation_post_process.max_val\", \"encoder.9.activation_post_process.eps\", \"encoder.9.activation_post_process.histogram\", \"encoder.9.activation_post_process.min_val\", \"encoder.9.activation_post_process.max_val\", \"decoder.0.activation_post_process.eps\", \"decoder.0.activation_post_process.histogram\", \"decoder.0.activation_post_process.min_val\", \"decoder.0.activation_post_process.max_val\", \"decoder.1.activation_post_process.eps\", \"decoder.1.activation_post_process.histogram\", \"decoder.1.activation_post_process.min_val\", \"decoder.1.activation_post_process.max_val\", \"decoder.2.activation_post_process.eps\", \"decoder.2.activation_post_process.histogram\", \"decoder.2.activation_post_process.min_val\", \"decoder.2.activation_post_process.max_val\", \"decoder.3.activation_post_process.eps\", \"decoder.3.activation_post_process.histogram\", \"decoder.3.activation_post_process.min_val\", \"decoder.3.activation_post_process.max_val\", \"decoder.5.activation_post_process.eps\", \"decoder.5.activation_post_process.histogram\", \"decoder.5.activation_post_process.min_val\", \"decoder.5.activation_post_process.max_val\", \"decoder.6.activation_post_process.eps\", \"decoder.6.activation_post_process.histogram\", \"decoder.6.activation_post_process.min_val\", \"decoder.6.activation_post_process.max_val\", \"decoder.8.activation_post_process.eps\", \"decoder.8.activation_post_process.histogram\", \"decoder.8.activation_post_process.min_val\", \"decoder.8.activation_post_process.max_val\". "
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "\n",
    "# Append the directory containing the cae1d.py file, not the file itself\n",
    "sys.path.append('/root/hsi-compression/models/')\n",
    "from cae1d import ConvolutionalAutoencoder1D\n",
    "\n",
    "# Load your model\n",
    "filepath = \"/root/hsi-compression/results/weights/cae1d_8bpppc.pth.tar\"\n",
    "model = load_model_with_weights(filepath)  # Make sure this function is correctly defined\n",
    "\n",
    "# Now evaluate the model\n",
    "print(\"Original Model Accuracy:\", evaluate_model(model, dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "\n",
    "# If you have a quantized model\n",
    "quantized_model = mixed_precision_training(model, dataloader)# Ensure this function is correctly defined and imported\n",
    "print(\"Quantized Model Accuracy:\", evaluate_model(quantized_model, dataloader))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "\n",
    "original_ssim = evaluate_ssim(model, dataloader)\n",
    "quantized_ssim = evaluate_ssim(quantized_model, dataloader)\n",
    "print(f\"Original SSIM: {original_ssim:.4f}, Quantized SSIM: {quantized_ssim:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "# Measure latency\n",
    "original_latency = evaluate_latency(model, dataloader)\n",
    "quantized_latency = evaluate_latency(quantized_model, dataloader)\n",
    "print(f\"Original Model Latency: {original_latency:.4f} seconds per batch\")\n",
    "print(f\"Quantized Model Latency: {quantized_latency:.4f} seconds per batch\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "\n",
    "# Size calculation\n",
    "# Paths for original and quantized models\n",
    "original_model_path = \"/root/hsi-compression/results/weights/cae1d_8bpppc.pth.tar\"\n",
    "quantized_model_path = \"/root/hsi-compression/compressed_model/mixed_precision_model.pth\"\n",
    "\n",
    "# Assuming 'model' is your original loaded model\n",
    "save_model(model, original_model_path)\n",
    "\n",
    "save_model(quantized_model, quantized_model_path)  \n",
    "\n",
    "# Calculate and print the model sizes\n",
    "original_size = get_model_size(original_model_path)\n",
    "quantized_size = get_model_size(quantized_model_path)  \n",
    "print(f\"Original Model Size: {original_size:.2f} MB\")\n",
    "print(f\"Quantized Model Size: {quantized_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def foo(): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
