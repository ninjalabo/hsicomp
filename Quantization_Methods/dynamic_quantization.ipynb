{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic_Quantization\n",
    "\n",
    "> We will be implementing Dynamic_Quantization model compression here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp quantization_fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastai.vision.all import *  # Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import logging\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from tqdm import tqdm, trange\n",
    "import torch.quantization\n",
    "sys.path.append('/root/hsi-compression/models/')\n",
    "from cae1d import ConvolutionalAutoencoder1D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#| export\n",
    "\n",
    "filepath = \"/root/hsi-compression/results/weights/cae1d_8bpppc.pth.tar\"\n",
    "\n",
    "def load_model_with_weights(filepath, quantized=False):\n",
    "    model = ConvolutionalAutoencoder1D()\n",
    "    if quantized:\n",
    "        qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "        model.qconfig = qconfig\n",
    "        model = torch.quantization.prepare(model, inplace=False)\n",
    "    \n",
    "    checkpoint = torch.load(filepath, map_location='cpu', weights_only=True)  # Ensure loading to CPU\n",
    "    if quantized:\n",
    "        model = torch.quantization.convert(model, inplace=False)  # Convert to quantized version\n",
    "\n",
    "    model_state_dict = {k: v for k, v in checkpoint.items() if 'activation_post_process' not in k}\n",
    "    model.load_state_dict(model_state_dict, strict=False)\n",
    "    model.to('cpu')  # Explicitly move the model to CPU\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "# Append the directory containing the cae1d.py file, not the file itself\n",
    "\n",
    "# Setup logging to help with debugging\n",
    "logging.basicConfig(level=logging.ERROR, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Base directory where the .npy files are stored\n",
    "base_directory = '/root/hsi-compression/datasets/hyspecnet-11k/patches/'\n",
    "\n",
    "# Function to load paths from a CSV file without headers\n",
    "def load_paths(csv_file):\n",
    "    df = pd.read_csv(csv_file, header=None)\n",
    "    file_paths = [os.path.join(base_directory, x.strip()) for x in df[0]]\n",
    "    return file_paths\n",
    "\n",
    "class NPYDataset(Dataset):\n",
    "    def __init__(self, file_paths, transform=None):\n",
    "        self.file_paths = file_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_paths[idx]\n",
    "        sample = np.load(file_path)\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        sample = torch.from_numpy(sample).float().to('cpu')  # Move data to CPU\n",
    "        return sample, sample\n",
    "\n",
    "# Apply any necessary transformations directly on the CPU\n",
    "def transform_sample(sample):\n",
    "    return (sample - np.mean(sample)) / np.std(sample)\n",
    "\n",
    "csv_file_path = '/root/hsi-compression/datasets/hyspecnet-11k/splits/easy/test.csv'\n",
    "file_paths = load_paths(csv_file_path)\n",
    "\n",
    "dataset = NPYDataset(file_paths, transform=transform_sample)\n",
    "print(\"Dataset initialized successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def apply_dynamic_quantization(model, save_path=\"/root/hsi-compression/compressed_model/quantized_model.pth\"):\n",
    "    \n",
    "    # Apply dynamic quantization to Conv1d layers\n",
    "    quantized_model = torch.quantization.quantize_dynamic(\n",
    "        model,\n",
    "        {nn.Conv1d},  # Target Conv1d layers\n",
    "        dtype=torch.qint8  # Quantize using 8-bit integers\n",
    "    )\n",
    "    \n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    \n",
    "    # Save the quantized model\n",
    "    torch.save(quantized_model.state_dict(), save_path)\n",
    "    print(f\"Quantized model saved at: {save_path}\")\n",
    "    \n",
    "    return quantized_model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader, desc=\"Evaluating model\", leave=False):\n",
    "            inputs, labels = inputs.to('cpu'), labels.to('cpu')  # Ensure tensors are on CPU\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            losses.append(loss.item())\n",
    "    mean_loss = sum(losses) / len(losses)\n",
    "    return mean_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def evaluate_ssim(model, dataloader):\n",
    "    model.eval()\n",
    "    ssim_scores = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in dataloader:\n",
    "            output = model(data)\n",
    "            output_np = output.cpu().detach().numpy()\n",
    "            target_np = target.cpu().detach().numpy()\n",
    "            for o, t in zip(output_np, target_np):\n",
    "                score = ssim(o, t, data_range=t.max() - t.min())\n",
    "                ssim_scores.append(score)\n",
    "    average_ssim = np.mean(ssim_scores)\n",
    "    return average_ssim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def evaluate_latency(model, dataloader, num_iterations=100):\n",
    "    start_time = time.time()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (data, _) in enumerate(dataloader):\n",
    "            if i >= num_iterations:\n",
    "                break\n",
    "            _ = model(data)\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    average_time_per_batch = total_time / num_iterations\n",
    "    return average_time_per_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader initialized successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Model Accuracy: 0.6015384942293167\n",
      "Quantized model saved at: /root/hsi-compression/compressed_model/quantized_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized Model Accuracy: 0.6033445373177528\n",
      "Original SSIM: 0.3540, Quantized SSIM: 0.3540\n",
      "Original Model Latency: 1.6883 seconds per batch\n",
      "Quantized Model Latency: 1.6928 seconds per batch\n",
      "Original Model Size: 0.22 MB\n",
      "Quantized Model Size: 0.22 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Append the directory containing the cae1d.py file, not the file itself\n",
    "sys.path.append('/root/hsi-compression/models/')\n",
    "\n",
    "from cae1d import ConvolutionalAutoencoder1D\n",
    "\n",
    "# Load your model\n",
    "filepath = \"/root/hsi-compression/results/weights/cae1d_8bpppc.pth.tar\"\n",
    "model = load_model_with_weights(filepath)  # Make sure this function is correctly defined\n",
    "\n",
    "subset_size = int(len(dataset) * 0.5)\n",
    "indices = torch.randperm(len(dataset))[:subset_size]\n",
    "subset = Subset(dataset, indices)\n",
    "dataloader = DataLoader(subset, batch_size=16, shuffle=True, num_workers=0)\n",
    "print(\"DataLoader initialized successfully.\")\n",
    "\n",
    "# Now evaluate the model\n",
    "print(\"Original Model Accuracy:\", evaluate_model(model, dataloader))\n",
    "\n",
    "# If you have a quantized model\n",
    "quantized_model = apply_dynamic_quantization(model)  # Ensure this function is correctly defined and imported\n",
    "print(\"Quantized Model Accuracy:\", evaluate_model(quantized_model, dataloader))\n",
    "\n",
    "\n",
    "original_ssim = evaluate_ssim(model, dataloader)\n",
    "quantized_ssim = evaluate_ssim(quantized_model, dataloader)\n",
    "print(f\"Original SSIM: {original_ssim:.4f}, Quantized SSIM: {quantized_ssim:.4f}\")\n",
    "\n",
    "\n",
    "# Measure latency\n",
    "original_latency = evaluate_latency(model, dataloader)\n",
    "quantized_latency = evaluate_latency(quantized_model, dataloader)\n",
    "print(f\"Original Model Latency: {original_latency:.4f} seconds per batch\")\n",
    "print(f\"Quantized Model Latency: {quantized_latency:.4f} seconds per batch\")\n",
    "\n",
    "# size evaluation\n",
    "\n",
    "def save_model(model, filepath):\n",
    "    torch.save(model.state_dict(), filepath)\n",
    "\n",
    "def get_model_size(filepath):\n",
    "    size_bytes = os.path.getsize(filepath)\n",
    "    return size_bytes / (1024 * 1024)  # Convert bytes to megabytes\n",
    "\n",
    "# Paths for original and quantized models\n",
    "original_model_path = \"/root/hsi-compression/results/weights/cae1d_8bpppc.pth.tar\"\n",
    "quantized_model_path = \"/root/hsi-compression/compressed_model/quantized_model.pth\"\n",
    "\n",
    "# Assuming 'model' is your original loaded model\n",
    "save_model(model, original_model_path)\n",
    "\n",
    "save_model(quantized_model, quantized_model_path)  \n",
    "\n",
    "# Calculate and print the model sizes\n",
    "original_size = get_model_size(original_model_path)\n",
    "quantized_size = get_model_size(quantized_model_path)  \n",
    "print(f\"Original Model Size: {original_size:.2f} MB\")\n",
    "print(f\"Quantized Model Size: {quantized_size:.2f} MB\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def foo(): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
