{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp SSCNetStaticQuantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SSCNet Static Quantization\n",
    "\n",
    "> - In this we implement the quantization method from fasterai.\n",
    "> - The documentation are available here https://github.com/nathanhubens/fasterai.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting the necessary libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-check installation of the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Pre-installation script for required libraries\\n\\nimport subprocess\\nimport sys\\n\\n# List of required libraries\\nrequired_libraries = [\\n    \"os\", \"sys\", \"torch\", \"time\", \"numpy\", \"pandas\", \"fastai\", \"pathlib\"\\n]\\n\\n# Function to check and install missing libraries\\ndef check_and_install_libraries(libraries):\\n    for lib in libraries:\\n        try:\\n            # Check if the library can be imported\\n            __import__(lib)\\n        except ImportError:\\n            # Special case for libraries with different pip names\\n            lib_pip = lib\\n            if lib == \"torch\":\\n                lib_pip = \"torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\"\\n            elif lib == \"fastai\":\\n                lib_pip = \"fastai\"\\n\\n            print(f\"{lib} not found. Installing...\")\\n            try:\\n                subprocess.check_call(\\n                    [sys.executable, \"-m\", \"pip\", \"install\", lib_pip]\\n                )\\n                print(f\"{lib} installed successfully!\")\\n            except subprocess.CalledProcessError:\\n                print(f\"Failed to install {lib}. Please install it manually.\")\\n\\nif __name__ == \"__main__\":\\n    check_and_install_libraries(required_libraries)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "\n",
    "\"\"\"\n",
    "# Pre-installation script for required libraries\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# List of required libraries\n",
    "required_libraries = [\n",
    "    \"os\", \"sys\", \"torch\", \"time\", \"numpy\", \"pandas\", \"fastai\", \"pathlib\"\n",
    "]\n",
    "\n",
    "# Function to check and install missing libraries\n",
    "def check_and_install_libraries(libraries):\n",
    "    for lib in libraries:\n",
    "        try:\n",
    "            # Check if the library can be imported\n",
    "            __import__(lib)\n",
    "        except ImportError:\n",
    "            # Special case for libraries with different pip names\n",
    "            lib_pip = lib\n",
    "            if lib == \"torch\":\n",
    "                lib_pip = \"torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\"\n",
    "            elif lib == \"fastai\":\n",
    "                lib_pip = \"fastai\"\n",
    "\n",
    "            print(f\"{lib} not found. Installing...\")\n",
    "            try:\n",
    "                subprocess.check_call(\n",
    "                    [sys.executable, \"-m\", \"pip\", \"install\", lib_pip]\n",
    "                )\n",
    "                print(f\"{lib} installed successfully!\")\n",
    "            except subprocess.CalledProcessError:\n",
    "                print(f\"Failed to install {lib}. Please install it manually.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    check_and_install_libraries(required_libraries)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "# Required imports\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from fastai.vision.all import DataLoader, DataLoaders\n",
    "from torch.utils.data import Dataset, DataLoader as TorchDataLoader\n",
    "from pathlib import Path\n",
    "#from fasterai.quantizer import Quantizer\n",
    "#from fasterai.quantize_callback import QuantizeCallback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "# Adjust paths for imports\n",
    "sys.path.append('/root/HSI_HypSpecNet11k/hsi-compression/')\n",
    "from quantizer import Quantizer\n",
    "from quantize_callback import QuantizeCallback\n",
    "sys.path.append('/root/HSI_HypSpecNet11k/hsi-compression/models/')\n",
    "from sscnet import SpectralSignalsCompressorNetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have pre-trained weights, so we are using that in place of pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "# Utility function to load pretrained weights\n",
    "def load_pretrained_weights(model, pretrained_weights_path):\n",
    "    print(f\"Loading pretrained weights from {pretrained_weights_path}...\")\n",
    "    checkpoint = torch.load(pretrained_weights_path)\n",
    "    \n",
    "    if \"state_dict\" in checkpoint:\n",
    "        state_dict = checkpoint[\"state_dict\"]\n",
    "    elif isinstance(checkpoint, dict):\n",
    "        state_dict = checkpoint\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported checkpoint format.\")\n",
    "\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "    print(\"Pretrained weights loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing the dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#| eval: false\n",
    "# Base directory for `.npy` files\n",
    "base_directory = '/root/HSI_HypSpecNet11k/hsi-compression/datasets/hyspecnet-11k/patches/'\n",
    "\n",
    "# Utility to load paths from a CSV file\n",
    "def load_paths(csv_file):\n",
    "    df = pd.read_csv(csv_file, header=None)\n",
    "    file_paths = [os.path.join(base_directory, x.strip()) for x in df[0]]\n",
    "    print(\"Paths loaded successfully.\")\n",
    "    return file_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#| eval: false\n",
    "# Dataset class for `.npy` files\n",
    "class NPYDataset(Dataset):\n",
    "\n",
    "    def __init__(self, file_paths, transform=None):\n",
    "        self.file_paths = file_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_paths[idx]\n",
    "        sample = np.load(file_path)\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        sample = torch.from_numpy(sample).float()\n",
    "        return sample, sample\n",
    "\n",
    "#| eval: false\n",
    "# Function to standardize samples\n",
    "def transform_sample(sample):\n",
    "    return (sample - np.mean(sample)) / np.std(sample)\n",
    "\n",
    "#| eval: false\n",
    "# Function to create DataLoaders\n",
    "def create_dataloaders(csv_file_path, batch_size=4, transform=None):\n",
    "    file_paths = load_paths(csv_file_path)\n",
    "    dataset = NPYDataset(file_paths, transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return DataLoaders(dataloader, dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "def quantization_pipeline_with_npy(\n",
    "    model, pretrained_weights_path, csv_file_path, backend=\"x86\", batch_size=4, epochs=2, lr=1e-3, save_path=None\n",
    "):\n",
    "    def evaluate_model(model, test_dl, device=\"cpu\"):\n",
    "        print(\"Evaluating the model...\")\n",
    "        model.to(device).eval()\n",
    "        criterion = torch.nn.MSELoss()\n",
    "        total_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for xb, _ in tqdm(test_dl, desc=\"Evaluating Batches\", leave=True):\n",
    "                xb = xb.to(device)\n",
    "                preds = model(xb)\n",
    "                loss = criterion(preds, xb)\n",
    "                total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(test_dl)\n",
    "        print(f\"Evaluation complete. Average Loss: {avg_loss:.6f}\")\n",
    "        return {\"loss\": avg_loss}\n",
    "\n",
    "    print(f\"Loading pretrained weights from {pretrained_weights_path}...\")\n",
    "    model.load_state_dict(torch.load(pretrained_weights_path).get(\"state_dict\", torch.load(pretrained_weights_path)), strict=False)\n",
    "\n",
    "    model.eval()\n",
    "    print(\"Model loaded successfully.\")\n",
    "\n",
    "    print(f\"Creating DataLoaders using CSV file: {csv_file_path}\")\n",
    "    dls = create_dataloaders(csv_file_path, batch_size=batch_size, transform=transform_sample)\n",
    "\n",
    "    print(\"Evaluating the non-quantized model...\")\n",
    "    non_quantized_metrics = evaluate_model(model, dls.valid)\n",
    "\n",
    "    print(\"Setting up FastAI Learner with QuantizeCallback...\")\n",
    "    learn = Learner(\n",
    "        dls,\n",
    "        model,\n",
    "        loss_func=torch.nn.MSELoss(),\n",
    "        cbs=QuantizeCallback(backend=backend),\n",
    "    )\n",
    "\n",
    "    print(\"Callbacks added to the learner:\")\n",
    "    print(learn.cbs)\n",
    "\n",
    "    print(\"Starting quantization-aware training...\")\n",
    "    learn.fit_one_cycle(epochs, lr)\n",
    "\n",
    "    quantized_model = learn.model\n",
    "\n",
    "    print(\"\\nInspecting quantized model weights...\")\n",
    "    for name, param in quantized_model.named_parameters():\n",
    "        print(f\"Layer: {name}, Data Type: {param.dtype}\")\n",
    "\n",
    "    if save_path:\n",
    "        print(\"Saving the quantized model...\")\n",
    "        torch.save(quantized_model, save_path)\n",
    "        print(f\"Quantized model saved to {save_path}\")\n",
    "    else:\n",
    "        print(\"Save path not provided; quantized model will not be saved.\")\n",
    "\n",
    "    print(\"Evaluating the quantized model...\")\n",
    "    quantized_metrics = evaluate_model(quantized_model, dls.valid)\n",
    "\n",
    "    print(\"\\nQuantization pipeline completed.\")\n",
    "    print(f\"Non-Quantized Model Loss: {non_quantized_metrics['loss']:.6f}\")\n",
    "    print(f\"Quantized Model Loss: {quantized_metrics['loss']:.6f}\")\n",
    "\n",
    "    return quantized_model, non_quantized_metrics, quantized_metrics, dls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating KPIs for measuring the performace of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#| eval: false\n",
    "# Performance measurement functions\n",
    "def measure_inference_time(model, dataloader, device=\"cpu\"):\n",
    "    \"\"\"Measure inference time for a model.\"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        for xb, _ in dataloader:\n",
    "            xb = xb.to(device)\n",
    "            _ = model(xb)\n",
    "    end = time.time()\n",
    "    return end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#| eval: false\n",
    "def measure_vram_usage(model, dataloader, device=\"cuda\"):\n",
    "    \"\"\"Simpler VRAM measurement.\"\"\"\n",
    "    try:\n",
    "        model.to(device)\n",
    "        torch.cuda.reset_peak_memory_stats(device)\n",
    "        with torch.no_grad():\n",
    "            for xb, _ in dataloader:\n",
    "                xb = xb.to(device)\n",
    "                _ = model(xb)\n",
    "        vram_peak = torch.cuda.max_memory_allocated(device) / 1e6  # Convert to MB\n",
    "    except RuntimeError:\n",
    "        print(\"VRAM measurement failed. Skipping.\")\n",
    "        vram_peak = -1  # Indicate failure\n",
    "    return vram_peak\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the comparision table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "def generate_comparison_table(\n",
    "    model, quantized_model, non_quantized_metrics, quantized_metrics, test_dataloader, \n",
    "    pretrained_weights_path, quantized_weights_path, device=\"cpu\"\n",
    "):\n",
    "\n",
    "    data = []\n",
    "\n",
    "    # Measure model sizes\n",
    "    torch.save(model.state_dict(), pretrained_weights)\n",
    "    torch.save(quantized_model.state_dict(), quantized_weights_path)\n",
    "    model_size = os.path.getsize(pretrained_weights) / 1e6  # Convert to MB\n",
    "    quantized_size = os.path.getsize(quantized_weights_path) / 1e6\n",
    "\n",
    "    # Measure execution speed\n",
    "    print(\"Measuring execution speed...\")\n",
    "    non_quantized_speed = measure_inference_time(model, test_dataloader, device)\n",
    "    quantized_speed = measure_inference_time(quantized_model, test_dataloader, device)\n",
    "\n",
    "    # Measure VRAM usage\n",
    "    print(\"Measuring VRAM usage...\")\n",
    "    non_quantized_vram = measure_vram_usage(model, test_dataloader, device)\n",
    "    quantized_vram = measure_vram_usage(quantized_model, test_dataloader, device)\n",
    "\n",
    "    # Collect data\n",
    "    data.append([\"Model Size (MB)\", model_size, quantized_size])\n",
    "    data.append([\"Average Loss\", non_quantized_metrics[\"loss\"], quantized_metrics[\"loss\"]])\n",
    "    data.append([\"Execution Speed (s)\", non_quantized_speed, quantized_speed])\n",
    "    data.append([\"VRAM Usage (MB)\", non_quantized_vram, quantized_vram])\n",
    "\n",
    "    # Generate DataFrame\n",
    "    df = pd.DataFrame(data, columns=[\"Metric\", \"Non-Quantized Model\", \"Quantized Model\"])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "from tqdm import tqdm  # Import tqdm for progress bars\n",
    "from sscnet import SpectralSignalsCompressorNetwork  # Import SSCNet model\n",
    "from quantize_callback import QuantizeCallback  # Import QuantizeCallback\n",
    "from fastai.learner import Learner  # Import Learner from FastAI\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "pretrained_weights = \"/root/HSI_HypSpecNet11k/hsi-compression/results/weights/sscnet_2point5bpppc.pth.tar\"\n",
    "csv_file_path = \"/root/HSI_HypSpecNet11k/hsi-compression/datasets/hyspecnet-11k/splits/easy/test.csv\"\n",
    "quantized_weights_path = \"/root/HSI_HypSpecNet11k/hsi-compression/compressed_model/quantized_sscnet.pth\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "# Initialize the model\n",
    "model = SpectralSignalsCompressorNetwork()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the quantization pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantization Pipeline Progress:   0%|                                                                                  | 0/100 [00:00<?, ?it/s]/tmp/ipykernel_48102/1844765396.py:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(pretrained_weights_path).get(\"state_dict\", torch.load(pretrained_weights_path)), strict=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained weights from /root/HSI_HypSpecNet11k/hsi-compression/results/weights/sscnet_2point5bpppc.pth.tar...\n",
      "Model loaded successfully.\n",
      "Creating DataLoaders using CSV file: /root/HSI_HypSpecNet11k/hsi-compression/datasets/hyspecnet-11k/splits/easy/test.csv\n",
      "Paths loaded successfully.\n",
      "Evaluating the non-quantized model...\n",
      "Evaluating the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Batches:   0%|                                                                                               | 0/61 [00:00<?, ?it/s]\u001b[A\n",
      "Evaluating Batches:   2%|█▍                                                                                     | 1/61 [00:00<00:57,  1.04it/s]\u001b[A\n",
      "Evaluating Batches:   3%|██▊                                                                                    | 2/61 [00:01<00:56,  1.05it/s]\u001b[A\n",
      "Evaluating Batches:   5%|████▎                                                                                  | 3/61 [00:02<00:54,  1.07it/s]\u001b[A\n",
      "Evaluating Batches:   7%|█████▋                                                                                 | 4/61 [00:03<00:53,  1.07it/s]\u001b[A\n",
      "Evaluating Batches:   8%|███████▏                                                                               | 5/61 [00:04<00:51,  1.09it/s]\u001b[A\n",
      "Evaluating Batches:  10%|████████▌                                                                              | 6/61 [00:05<00:50,  1.09it/s]\u001b[A\n",
      "Evaluating Batches:  11%|█████████▉                                                                             | 7/61 [00:06<00:50,  1.08it/s]\u001b[A\n",
      "Evaluating Batches:  13%|███████████▍                                                                           | 8/61 [00:07<00:49,  1.07it/s]\u001b[A\n",
      "Evaluating Batches:  15%|████████████▊                                                                          | 9/61 [00:08<00:48,  1.06it/s]\u001b[A\n",
      "Evaluating Batches:  16%|██████████████                                                                        | 10/61 [00:09<00:47,  1.07it/s]\u001b[A\n",
      "Evaluating Batches:  18%|███████████████▌                                                                      | 11/61 [00:10<00:46,  1.07it/s]\u001b[A\n",
      "Evaluating Batches:  20%|████████████████▉                                                                     | 12/61 [00:11<00:45,  1.09it/s]\u001b[A\n",
      "Evaluating Batches:  21%|██████████████████▎                                                                   | 13/61 [00:12<00:44,  1.09it/s]\u001b[A\n",
      "Evaluating Batches:  23%|███████████████████▋                                                                  | 14/61 [00:13<00:43,  1.07it/s]\u001b[A\n",
      "Evaluating Batches:  25%|█████████████████████▏                                                                | 15/61 [00:13<00:43,  1.07it/s]\u001b[A\n",
      "Evaluating Batches:  26%|██████████████████████▌                                                               | 16/61 [00:14<00:42,  1.06it/s]\u001b[A\n",
      "Evaluating Batches:  28%|███████████████████████▉                                                              | 17/61 [00:15<00:41,  1.07it/s]\u001b[A\n",
      "Evaluating Batches:  30%|█████████████████████████▍                                                            | 18/61 [00:16<00:40,  1.07it/s]\u001b[A\n",
      "Evaluating Batches:  31%|██████████████████████████▊                                                           | 19/61 [00:17<00:38,  1.08it/s]\u001b[A\n",
      "Evaluating Batches:  33%|████████████████████████████▏                                                         | 20/61 [00:18<00:37,  1.08it/s]\u001b[A\n",
      "Evaluating Batches:  34%|█████████████████████████████▌                                                        | 21/61 [00:19<00:37,  1.07it/s]\u001b[A\n",
      "Evaluating Batches:  36%|███████████████████████████████                                                       | 22/61 [00:20<00:36,  1.06it/s]\u001b[A\n",
      "Evaluating Batches:  38%|████████████████████████████████▍                                                     | 23/61 [00:21<00:35,  1.06it/s]\u001b[A\n",
      "Evaluating Batches:  39%|█████████████████████████████████▊                                                    | 24/61 [00:22<00:34,  1.06it/s]\u001b[A\n",
      "Evaluating Batches:  41%|███████████████████████████████████▏                                                  | 25/61 [00:23<00:33,  1.06it/s]\u001b[A\n",
      "Evaluating Batches:  43%|████████████████████████████████████▋                                                 | 26/61 [00:24<00:32,  1.08it/s]\u001b[A\n",
      "Evaluating Batches:  44%|██████████████████████████████████████                                                | 27/61 [00:25<00:31,  1.08it/s]\u001b[A\n",
      "Evaluating Batches:  46%|███████████████████████████████████████▍                                              | 28/61 [00:26<00:30,  1.07it/s]\u001b[A\n",
      "Evaluating Batches:  48%|████████████████████████████████████████▉                                             | 29/61 [00:27<00:30,  1.06it/s]\u001b[A\n",
      "Evaluating Batches:  49%|██████████████████████████████████████████▎                                           | 30/61 [00:28<00:29,  1.05it/s]\u001b[A\n",
      "Evaluating Batches:  51%|███████████████████████████████████████████▋                                          | 31/61 [00:29<00:28,  1.06it/s]\u001b[A\n",
      "Evaluating Batches:  52%|█████████████████████████████████████████████                                         | 32/61 [00:29<00:27,  1.06it/s]\u001b[A\n",
      "Evaluating Batches:  54%|██████████████████████████████████████████████▌                                       | 33/61 [00:30<00:26,  1.07it/s]\u001b[A\n",
      "Evaluating Batches:  56%|███████████████████████████████████████████████▉                                      | 34/61 [00:31<00:25,  1.07it/s]\u001b[A\n",
      "Evaluating Batches:  57%|█████████████████████████████████████████████████▎                                    | 35/61 [00:32<00:24,  1.06it/s]\u001b[A\n",
      "Evaluating Batches:  59%|██████████████████████████████████████████████████▊                                   | 36/61 [00:33<00:23,  1.06it/s]\u001b[A\n",
      "Evaluating Batches:  61%|████████████████████████████████████████████████████▏                                 | 37/61 [00:34<00:22,  1.05it/s]\u001b[A\n",
      "Evaluating Batches:  62%|█████████████████████████████████████████████████████▌                                | 38/61 [00:35<00:21,  1.06it/s]\u001b[A\n",
      "Evaluating Batches:  64%|██████████████████████████████████████████████████████▉                               | 39/61 [00:36<00:20,  1.06it/s]\u001b[A\n",
      "Evaluating Batches:  66%|████████████████████████████████████████████████████████▍                             | 40/61 [00:37<00:19,  1.08it/s]\u001b[A\n",
      "Evaluating Batches:  67%|█████████████████████████████████████████████████████████▊                            | 41/61 [00:38<00:18,  1.08it/s]\u001b[A\n",
      "Evaluating Batches:  69%|███████████████████████████████████████████████████████████▏                          | 42/61 [00:39<00:17,  1.07it/s]\u001b[A\n",
      "Evaluating Batches:  70%|████████████████████████████████████████████████████████████▌                         | 43/61 [00:40<00:16,  1.06it/s]\u001b[A\n",
      "Evaluating Batches:  72%|██████████████████████████████████████████████████████████████                        | 44/61 [00:41<00:16,  1.05it/s]\u001b[A\n",
      "Evaluating Batches:  74%|███████████████████████████████████████████████████████████████▍                      | 45/61 [00:42<00:15,  1.06it/s]\u001b[A\n",
      "Evaluating Batches:  75%|████████████████████████████████████████████████████████████████▊                     | 46/61 [00:43<00:14,  1.06it/s]\u001b[A\n",
      "Evaluating Batches:  77%|██████████████████████████████████████████████████████████████████▎                   | 47/61 [00:44<00:13,  1.07it/s]\u001b[A\n",
      "Evaluating Batches:  79%|███████████████████████████████████████████████████████████████████▋                  | 48/61 [00:44<00:12,  1.07it/s]\u001b[A\n",
      "Evaluating Batches:  80%|█████████████████████████████████████████████████████████████████████                 | 49/61 [00:45<00:11,  1.06it/s]\u001b[A\n",
      "Evaluating Batches:  82%|██████████████████████████████████████████████████████████████████████▍               | 50/61 [00:46<00:10,  1.06it/s]\u001b[A\n",
      "Evaluating Batches:  84%|███████████████████████████████████████████████████████████████████████▉              | 51/61 [00:47<00:09,  1.05it/s]\u001b[A\n",
      "Evaluating Batches:  85%|█████████████████████████████████████████████████████████████████████████▎            | 52/61 [00:48<00:08,  1.06it/s]\u001b[A\n",
      "Evaluating Batches:  87%|██████████████████████████████████████████████████████████████████████████▋           | 53/61 [00:49<00:07,  1.06it/s]\u001b[A\n",
      "Evaluating Batches:  89%|████████████████████████████████████████████████████████████████████████████▏         | 54/61 [00:50<00:06,  1.07it/s]\u001b[A\n",
      "Evaluating Batches:  90%|█████████████████████████████████████████████████████████████████████████████▌        | 55/61 [00:51<00:05,  1.07it/s]\u001b[A\n",
      "Evaluating Batches:  92%|██████████████████████████████████████████████████████████████████████████████▉       | 56/61 [00:52<00:04,  1.06it/s]\u001b[A\n",
      "Evaluating Batches:  93%|████████████████████████████████████████████████████████████████████████████████▎     | 57/61 [00:53<00:03,  1.05it/s]\u001b[A\n",
      "Evaluating Batches:  95%|█████████████████████████████████████████████████████████████████████████████████▊    | 58/61 [00:54<00:02,  1.05it/s]\u001b[A\n",
      "Evaluating Batches:  97%|███████████████████████████████████████████████████████████████████████████████████▏  | 59/61 [00:55<00:01,  1.06it/s]\u001b[A\n",
      "Evaluating Batches:  98%|████████████████████████████████████████████████████████████████████████████████████▌ | 60/61 [00:56<00:00,  1.06it/s]\u001b[A\n",
      "Evaluating Batches: 100%|██████████████████████████████████████████████████████████████████████████████████████| 61/61 [00:57<00:00,  1.07it/s]\u001b[A\n",
      "/root/miniconda3/lib/python3.12/site-packages/torch/ao/quantization/observer.py:229: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete. Average Loss: 0.906979\n",
      "Setting up FastAI Learner with QuantizeCallback...\n",
      "Callbacks added to the learner:\n",
      "[TrainEvalCallback, Recorder, CastToTensor, ProgressCallback, QuantizeCallback]\n",
      "Starting quantization-aware training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.786592</td>\n",
       "      <td>0.767433</td>\n",
       "      <td>04:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.770290</td>\n",
       "      <td>0.754018</td>\n",
       "      <td>04:18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inspecting quantized model weights...\n",
      "Save path not provided; quantized model will not be saved.\n",
      "Evaluating the quantized model...\n",
      "Evaluating the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Batches:   0%|                                                                                               | 0/61 [00:00<?, ?it/s]\u001b[A\n",
      "Evaluating Batches:   2%|█▍                                                                                     | 1/61 [00:00<00:30,  1.99it/s]\u001b[A\n",
      "Evaluating Batches:   3%|██▊                                                                                    | 2/61 [00:01<00:29,  1.97it/s]\u001b[A\n",
      "Evaluating Batches:   5%|████▎                                                                                  | 3/61 [00:01<00:29,  1.96it/s]\u001b[A\n",
      "Evaluating Batches:   7%|█████▋                                                                                 | 4/61 [00:02<00:29,  1.95it/s]\u001b[A\n",
      "Evaluating Batches:   8%|███████▏                                                                               | 5/61 [00:02<00:28,  1.95it/s]\u001b[A\n",
      "Evaluating Batches:  10%|████████▌                                                                              | 6/61 [00:03<00:28,  1.95it/s]\u001b[A\n",
      "Evaluating Batches:  11%|█████████▉                                                                             | 7/61 [00:03<00:27,  1.94it/s]\u001b[A\n",
      "Evaluating Batches:  13%|███████████▍                                                                           | 8/61 [00:04<00:27,  1.94it/s]\u001b[A\n",
      "Evaluating Batches:  15%|████████████▊                                                                          | 9/61 [00:04<00:26,  1.95it/s]\u001b[A\n",
      "Evaluating Batches:  16%|██████████████                                                                        | 10/61 [00:05<00:26,  1.95it/s]\u001b[A\n",
      "Evaluating Batches:  18%|███████████████▌                                                                      | 11/61 [00:05<00:25,  1.94it/s]\u001b[A\n",
      "Evaluating Batches:  20%|████████████████▉                                                                     | 12/61 [00:06<00:25,  1.95it/s]\u001b[A\n",
      "Evaluating Batches:  21%|██████████████████▎                                                                   | 13/61 [00:06<00:24,  1.95it/s]\u001b[A\n",
      "Evaluating Batches:  23%|███████████████████▋                                                                  | 14/61 [00:07<00:24,  1.95it/s]\u001b[A\n",
      "Evaluating Batches:  25%|█████████████████████▏                                                                | 15/61 [00:07<00:23,  1.95it/s]\u001b[A\n",
      "Evaluating Batches:  26%|██████████████████████▌                                                               | 16/61 [00:08<00:23,  1.95it/s]\u001b[A\n",
      "Evaluating Batches:  28%|███████████████████████▉                                                              | 17/61 [00:08<00:22,  1.95it/s]\u001b[A\n",
      "Evaluating Batches:  30%|█████████████████████████▍                                                            | 18/61 [00:09<00:22,  1.95it/s]\u001b[A\n",
      "Evaluating Batches:  31%|██████████████████████████▊                                                           | 19/61 [00:09<00:21,  1.94it/s]\u001b[A\n",
      "Evaluating Batches:  33%|████████████████████████████▏                                                         | 20/61 [00:10<00:21,  1.94it/s]\u001b[A\n",
      "Evaluating Batches:  34%|█████████████████████████████▌                                                        | 21/61 [00:10<00:20,  1.94it/s]\u001b[A\n",
      "Evaluating Batches:  36%|███████████████████████████████                                                       | 22/61 [00:11<00:20,  1.94it/s]\u001b[A\n",
      "Evaluating Batches:  38%|████████████████████████████████▍                                                     | 23/61 [00:11<00:19,  1.95it/s]\u001b[A\n",
      "Evaluating Batches:  39%|█████████████████████████████████▊                                                    | 24/61 [00:12<00:19,  1.95it/s]\u001b[A\n",
      "Evaluating Batches:  41%|███████████████████████████████████▏                                                  | 25/61 [00:12<00:18,  1.94it/s]\u001b[A\n",
      "Evaluating Batches:  43%|████████████████████████████████████▋                                                 | 26/61 [00:13<00:18,  1.94it/s]\u001b[A\n",
      "Evaluating Batches:  44%|██████████████████████████████████████                                                | 27/61 [00:13<00:17,  1.94it/s]\u001b[A\n",
      "Evaluating Batches:  46%|███████████████████████████████████████▍                                              | 28/61 [00:14<00:17,  1.92it/s]\u001b[A\n",
      "Evaluating Batches:  48%|████████████████████████████████████████▉                                             | 29/61 [00:14<00:16,  1.93it/s]\u001b[A\n",
      "Evaluating Batches:  49%|██████████████████████████████████████████▎                                           | 30/61 [00:15<00:16,  1.93it/s]\u001b[A\n",
      "Evaluating Batches:  51%|███████████████████████████████████████████▋                                          | 31/61 [00:15<00:15,  1.94it/s]\u001b[A\n",
      "Evaluating Batches:  52%|█████████████████████████████████████████████                                         | 32/61 [00:16<00:14,  1.94it/s]\u001b[A\n",
      "Evaluating Batches:  54%|██████████████████████████████████████████████▌                                       | 33/61 [00:16<00:14,  1.94it/s]\u001b[A\n",
      "Evaluating Batches:  56%|███████████████████████████████████████████████▉                                      | 34/61 [00:17<00:13,  1.95it/s]\u001b[A\n",
      "Evaluating Batches:  57%|█████████████████████████████████████████████████▎                                    | 35/61 [00:18<00:13,  1.95it/s]\u001b[A\n",
      "Evaluating Batches:  59%|██████████████████████████████████████████████████▊                                   | 36/61 [00:18<00:12,  1.94it/s]\u001b[A\n",
      "Evaluating Batches:  61%|████████████████████████████████████████████████████▏                                 | 37/61 [00:19<00:12,  1.94it/s]\u001b[A\n",
      "Evaluating Batches:  62%|█████████████████████████████████████████████████████▌                                | 38/61 [00:19<00:11,  1.94it/s]\u001b[A\n",
      "Evaluating Batches:  64%|██████████████████████████████████████████████████████▉                               | 39/61 [00:20<00:11,  1.94it/s]\u001b[A\n",
      "Evaluating Batches:  66%|████████████████████████████████████████████████████████▍                             | 40/61 [00:20<00:10,  1.94it/s]\u001b[A\n",
      "Evaluating Batches:  67%|█████████████████████████████████████████████████████████▊                            | 41/61 [00:21<00:10,  1.94it/s]\u001b[A\n",
      "Evaluating Batches:  69%|███████████████████████████████████████████████████████████▏                          | 42/61 [00:21<00:09,  1.94it/s]\u001b[A\n",
      "Evaluating Batches:  70%|████████████████████████████████████████████████████████████▌                         | 43/61 [00:22<00:09,  1.94it/s]\u001b[A\n",
      "Evaluating Batches:  72%|██████████████████████████████████████████████████████████████                        | 44/61 [00:22<00:08,  1.94it/s]\u001b[A\n",
      "Evaluating Batches:  74%|███████████████████████████████████████████████████████████████▍                      | 45/61 [00:23<00:08,  1.95it/s]\u001b[A\n",
      "Evaluating Batches:  75%|████████████████████████████████████████████████████████████████▊                     | 46/61 [00:23<00:07,  1.95it/s]\u001b[A\n",
      "Evaluating Batches:  77%|██████████████████████████████████████████████████████████████████▎                   | 47/61 [00:24<00:07,  1.95it/s]\u001b[A\n",
      "Evaluating Batches:  79%|███████████████████████████████████████████████████████████████████▋                  | 48/61 [00:24<00:06,  1.94it/s]\u001b[A\n",
      "Evaluating Batches:  80%|█████████████████████████████████████████████████████████████████████                 | 49/61 [00:25<00:06,  1.95it/s]\u001b[A\n",
      "Evaluating Batches:  82%|██████████████████████████████████████████████████████████████████████▍               | 50/61 [00:25<00:05,  1.94it/s]\u001b[A\n",
      "Evaluating Batches:  84%|███████████████████████████████████████████████████████████████████████▉              | 51/61 [00:26<00:05,  1.93it/s]\u001b[A\n",
      "Evaluating Batches:  85%|█████████████████████████████████████████████████████████████████████████▎            | 52/61 [00:26<00:04,  1.94it/s]\u001b[A\n",
      "Evaluating Batches:  87%|██████████████████████████████████████████████████████████████████████████▋           | 53/61 [00:27<00:04,  1.94it/s]\u001b[A\n",
      "Evaluating Batches:  89%|████████████████████████████████████████████████████████████████████████████▏         | 54/61 [00:27<00:03,  1.94it/s]\u001b[A\n",
      "Evaluating Batches:  90%|█████████████████████████████████████████████████████████████████████████████▌        | 55/61 [00:28<00:03,  1.94it/s]\u001b[A\n",
      "Evaluating Batches:  92%|██████████████████████████████████████████████████████████████████████████████▉       | 56/61 [00:28<00:02,  1.94it/s]\u001b[A\n",
      "Evaluating Batches:  93%|████████████████████████████████████████████████████████████████████████████████▎     | 57/61 [00:29<00:02,  1.94it/s]\u001b[A\n",
      "Evaluating Batches:  95%|█████████████████████████████████████████████████████████████████████████████████▊    | 58/61 [00:29<00:01,  1.94it/s]\u001b[A\n",
      "Evaluating Batches:  97%|███████████████████████████████████████████████████████████████████████████████████▏  | 59/61 [00:30<00:01,  1.94it/s]\u001b[A\n",
      "Evaluating Batches:  98%|████████████████████████████████████████████████████████████████████████████████████▌ | 60/61 [00:30<00:00,  1.95it/s]\u001b[A\n",
      "Evaluating Batches: 100%|██████████████████████████████████████████████████████████████████████████████████████| 61/61 [00:31<00:00,  1.94it/s]\u001b[A\n",
      "Quantization Pipeline Progress: 100%|████████████████████████████████████████████████████████████████████████| 100/100 [10:05<00:00,  6.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete. Average Loss: 0.757528\n",
      "\n",
      "Quantization pipeline completed.\n",
      "Non-Quantized Model Loss: 0.906979\n",
      "Quantized Model Loss: 0.757528\n",
      "\n",
      "Pipeline completed. Metrics:\n",
      "Non-Quantized Model Metrics: Loss = 0.906979\n",
      "Quantized Model Metrics: Loss = 0.757528\n",
      "\n",
      "Inspecting quantized model weights...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "# Step 1: Initialize and start the quantization pipeline\n",
    "print(\"Running the quantization pipeline...\")\n",
    "\n",
    "# Create a progress bar\n",
    "with tqdm(total=100, desc=\"Quantization Pipeline Progress\", leave=True) as pbar:\n",
    "    try:\n",
    "        # Run the quantization pipeline\n",
    "        quantized_model, non_quantized_metrics, quantized_metrics, dls = quantization_pipeline_with_npy(\n",
    "            model=model,\n",
    "            pretrained_weights_path=pretrained_weights,\n",
    "            csv_file_path=csv_file_path,\n",
    "            backend=\"x86\",\n",
    "            batch_size=4,\n",
    "            epochs=2,\n",
    "            lr=1e-3,\n",
    "        )\n",
    "\n",
    "        # Update progress bar to reflect pipeline progress (e.g., 70% complete after pipeline)\n",
    "        pbar.update(70)\n",
    "\n",
    "        # Step 2: Print the metrics\n",
    "        # Clear GPU memory to prevent memory leaks\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"\\nPipeline completed. Metrics:\")\n",
    "        print(f\"Non-Quantized Model Metrics: Loss = {non_quantized_metrics['loss']:.6f}\")\n",
    "        print(f\"Quantized Model Metrics: Loss = {quantized_metrics['loss']:.6f}\")\n",
    "\n",
    "        # Inspect quantized model weights\n",
    "        print(\"\\nInspecting quantized model weights...\")\n",
    "        for name, param in quantized_model.named_parameters():\n",
    "            print(f\"Layer: {name}, Data Type: {param.dtype}\")\n",
    "\n",
    "        # Update progress bar to completion\n",
    "        pbar.update(30)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during quantization pipeline: {e}\")\n",
    "        # Close progress bar to avoid hanging display\n",
    "        pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Quantized Model Architecture:\n",
      "GraphModule(\n",
      "  (encoder): Module(\n",
      "    (0): QuantizedConv2d(202, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.856876015663147, zero_point=67, padding=(1, 1))\n",
      "    (1): QuantizedPReLU()\n",
      "    (2): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=2.5710415840148926, zero_point=65, padding=(1, 1))\n",
      "    (3): QuantizedPReLU()\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=10.29415225982666, zero_point=64, padding=(1, 1))\n",
      "    (6): QuantizedPReLU()\n",
      "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (8): QuantizedConv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), scale=46.11467361450195, zero_point=64, padding=(1, 1))\n",
      "    (9): QuantizedPReLU()\n",
      "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (11): QuantizedConv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), scale=316.0097961425781, zero_point=63, padding=(1, 1))\n",
      "    (12): QuantizedPReLU()\n",
      "  )\n",
      "  (decoder): Module(\n",
      "    (0): QuantizedConvTranspose2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), scale=3845.340087890625, zero_point=62, padding=(1, 1))\n",
      "    (1): QuantizedPReLU()\n",
      "    (2): QuantizedConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2), scale=4987.4931640625, zero_point=62)\n",
      "    (3): QuantizedPReLU()\n",
      "    (4): QuantizedConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2), scale=7049.67724609375, zero_point=62)\n",
      "    (5): QuantizedPReLU()\n",
      "    (6): QuantizedConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2), scale=7753.93115234375, zero_point=61)\n",
      "    (7): QuantizedPReLU()\n",
      "    (8): QuantizedConvTranspose2d(256, 202, kernel_size=(3, 3), stride=(1, 1), scale=37242.7421875, zero_point=84, padding=(1, 1))\n",
      "    (9): Sigmoid()\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x):\n",
      "    encoder_0_input_scale_0 = self.encoder_0_input_scale_0\n",
      "    encoder_0_input_zero_point_0 = self.encoder_0_input_zero_point_0\n",
      "    quantize_per_tensor = torch.quantize_per_tensor(x, encoder_0_input_scale_0, encoder_0_input_zero_point_0, torch.quint8);  x = encoder_0_input_scale_0 = encoder_0_input_zero_point_0 = None\n",
      "    encoder_0 = getattr(self.encoder, \"0\")(quantize_per_tensor);  quantize_per_tensor = None\n",
      "    encoder_1 = getattr(self.encoder, \"1\")(encoder_0);  encoder_0 = None\n",
      "    encoder_2 = getattr(self.encoder, \"2\")(encoder_1);  encoder_1 = None\n",
      "    encoder_3 = getattr(self.encoder, \"3\")(encoder_2);  encoder_2 = None\n",
      "    encoder_4 = getattr(self.encoder, \"4\")(encoder_3);  encoder_3 = None\n",
      "    encoder_5 = getattr(self.encoder, \"5\")(encoder_4);  encoder_4 = None\n",
      "    encoder_6 = getattr(self.encoder, \"6\")(encoder_5);  encoder_5 = None\n",
      "    encoder_7 = getattr(self.encoder, \"7\")(encoder_6);  encoder_6 = None\n",
      "    encoder_8 = getattr(self.encoder, \"8\")(encoder_7);  encoder_7 = None\n",
      "    encoder_9 = getattr(self.encoder, \"9\")(encoder_8);  encoder_8 = None\n",
      "    encoder_10 = getattr(self.encoder, \"10\")(encoder_9);  encoder_9 = None\n",
      "    encoder_11 = getattr(self.encoder, \"11\")(encoder_10);  encoder_10 = None\n",
      "    encoder_12 = getattr(self.encoder, \"12\")(encoder_11);  encoder_11 = None\n",
      "    decoder_0 = getattr(self.decoder, \"0\")(encoder_12);  encoder_12 = None\n",
      "    decoder_1 = getattr(self.decoder, \"1\")(decoder_0);  decoder_0 = None\n",
      "    decoder_2 = getattr(self.decoder, \"2\")(decoder_1);  decoder_1 = None\n",
      "    decoder_3 = getattr(self.decoder, \"3\")(decoder_2);  decoder_2 = None\n",
      "    decoder_4 = getattr(self.decoder, \"4\")(decoder_3);  decoder_3 = None\n",
      "    decoder_5 = getattr(self.decoder, \"5\")(decoder_4);  decoder_4 = None\n",
      "    decoder_6 = getattr(self.decoder, \"6\")(decoder_5);  decoder_5 = None\n",
      "    decoder_7 = getattr(self.decoder, \"7\")(decoder_6);  decoder_6 = None\n",
      "    decoder_8 = getattr(self.decoder, \"8\")(decoder_7);  decoder_7 = None\n",
      "    decoder_9 = getattr(self.decoder, \"9\")(decoder_8);  decoder_8 = None\n",
      "    dequantize_23 = decoder_9.dequantize();  decoder_9 = None\n",
      "    return dequantize_23\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    print(\"\\nQuantized Model Architecture:\")\n",
    "    print(quantized_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_48102/459162560.py:32: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(original_model_path).get(\"state_dict\", torch.load(original_model_path)),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading original model weights from /root/HSI_HypSpecNet11k/hsi-compression/results/weights/sscnet_2point5bpppc.pth.tar...\n",
      "Original model loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating PSNR: 100%|████████████████████████████████████████████████████████████████████████████████████████| 61/61 [00:57<00:00,  1.06it/s]\n",
      "Calculating PSNR: 100%|████████████████████████████████████████████████████████████████████████████████████████| 61/61 [00:31<00:00,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PSNR for Original Model: 43.14 dB\n",
      "PSNR for Quantized Model: 121.24 dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from metrics.psnr import PeakSignalToNoiseRatio\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Path to the original model's weights\n",
    "original_model_path = \"/root/HSI_HypSpecNet11k/hsi-compression/results/weights/sscnet_2point5bpppc.pth.tar\"\n",
    "\n",
    "@torch.no_grad()\n",
    "def calculate_psnr(model, dataloader, device=\"cpu\"):\n",
    "    psnr_metric = PeakSignalToNoiseRatio()\n",
    "    model.to(device).eval()\n",
    "\n",
    "    total_psnr = 0.0\n",
    "    num_samples = 0\n",
    "\n",
    "    for xb, _ in tqdm(dataloader, desc=\"Calculating PSNR\", leave=True):\n",
    "        xb = xb.to(device)\n",
    "        preds = model(xb)\n",
    "        psnr = psnr_metric(preds, xb)\n",
    "        total_psnr += psnr.item() * xb.size(0)*100\n",
    "        num_samples += xb.size(0)\n",
    "\n",
    "    avg_psnr = total_psnr / num_samples\n",
    "    return avg_psnr\n",
    "\n",
    "# Initialize the original model\n",
    "original_model = SpectralSignalsCompressorNetwork()\n",
    "\n",
    "# Load the original model weights\n",
    "print(f\"Loading original model weights from {original_model_path}...\")\n",
    "original_model.load_state_dict(\n",
    "    torch.load(original_model_path).get(\"state_dict\", torch.load(original_model_path)),\n",
    "    strict=False\n",
    ")\n",
    "\n",
    "print(\"Original model loaded successfully.\")\n",
    "\n",
    "# Calculate PSNR for the original model\n",
    "psnr_original = calculate_psnr(original_model, dls.valid, device=\"cpu\")\n",
    "\n",
    "# Calculate PSNR for the quantized model (directly, no saving or loading)\n",
    "quantized_model.eval()  # Ensure the quantized model is in evaluation mode\n",
    "psnr_quantized = calculate_psnr(quantized_model, dls.valid, device=\"cpu\")\n",
    "\n",
    "# Print PSNR results\n",
    "print(f\"PSNR for Original Model: {psnr_original:.2f} dB\")\n",
    "print(f\"PSNR for Quantized Model: {psnr_quantized:.2f} dB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'collections.OrderedDict'>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "#model_path = quantized_weights_path  # Replace with your file path\n",
    "loaded_object = torch.load(quantized_weights_path, map_location=\"cpu\", weights_only=True)\n",
    "print(type(loaded_object))  # Prints the class of the loaded object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_48102/2640393690.py:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  original_model.load_state_dict(torch.load(original_model_path)[\"state_dict\"], strict=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading original model weights from /root/HSI_HypSpecNet11k/hsi-compression/results/weights/sscnet_2point5bpppc.pth.tar...\n",
      "Original model loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating SSIM: 100%|████████████████████████████████████████████████████████████████████████████████████████| 61/61 [01:23<00:00,  1.36s/it]\n",
      "Calculating SSIM: 100%|████████████████████████████████████████████████████████████████████████████████████████| 61/61 [00:51<00:00,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSIM for Original Model: 0.0146\n",
      "SSIM for Quantized Model: 0.0138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from metrics.ssim import StructuralSimilarity\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Path to the original model's weights\n",
    "original_model_path = \"/root/HSI_HypSpecNet11k/hsi-compression/results/weights/sscnet_2point5bpppc.pth.tar\"\n",
    "\n",
    "@torch.no_grad()\n",
    "def calculate_ssim(model, dataloader, device=\"cpu\"):\n",
    "    ssim_metric = StructuralSimilarity(data_range=1.0, channels=202)\n",
    "    model.to(device).eval()\n",
    "\n",
    "    total_ssim = 0.0\n",
    "    num_samples = 0\n",
    "\n",
    "    for xb, _ in tqdm(dataloader, desc=\"Calculating SSIM\", leave=True):\n",
    "        xb = xb.to(device)\n",
    "        preds = model(xb)\n",
    "        ssim = ssim_metric(preds, xb)\n",
    "        total_ssim += ssim.item() * xb.size(0)\n",
    "        num_samples += xb.size(0)\n",
    "\n",
    "    avg_ssim = total_ssim / num_samples\n",
    "    return avg_ssim\n",
    "\n",
    "# Initialize the original model\n",
    "original_model = SpectralSignalsCompressorNetwork()\n",
    "\n",
    "# Load the original model weights\n",
    "print(f\"Loading original model weights from {original_model_path}...\")\n",
    "original_model.load_state_dict(torch.load(original_model_path)[\"state_dict\"], strict=False)\n",
    "print(\"Original model loaded successfully.\")\n",
    "\n",
    "# Calculate SSIM for the original model\n",
    "ssim_original = calculate_ssim(original_model, dls.valid, device=\"cpu\")\n",
    "\n",
    "# Calculate SSIM for the quantized model (directly, no saving or loading)\n",
    "quantized_model.eval()\n",
    "ssim_quantized = calculate_ssim(quantized_model, dls.valid, device=\"cpu\")\n",
    "\n",
    "# Print SSIM results\n",
    "print(f\"SSIM for Original Model: {ssim_original:.4f}\")\n",
    "print(f\"SSIM for Quantized Model: {ssim_quantized:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading quantized model weights from /root/HSI_HypSpecNet11k/hsi-compression/compressed_model/static_quant_fastrai_model.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_48102/564128488.py:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  quantized_model = torch.load(quantized_model_path, map_location=\"cpu\")\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Conv2d' object has no attribute '_modules'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Load quantized model\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading quantized model weights from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquantized_model_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 29\u001b[0m quantized_model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquantized_model_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Check the type of the quantized model and handle accordingly\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(quantized_model, GraphModule):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/serialization.py:1360\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1358\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1359\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1360\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1361\u001b[0m \u001b[43m            \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1362\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1363\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1364\u001b[0m \u001b[43m            \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1365\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1366\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[1;32m   1368\u001b[0m     f_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/serialization.py:1848\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m _serialization_tls\n\u001b[1;32m   1847\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m map_location\n\u001b[0;32m-> 1848\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1849\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1851\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/fx/graph_module.py:128\u001b[0m, in \u001b[0;36mreduce_graph_module\u001b[0;34m(body, import_block)\u001b[0m\n\u001b[1;32m    126\u001b[0m fn_src \u001b[38;5;241m=\u001b[39m body\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_code\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m body[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    127\u001b[0m forward \u001b[38;5;241m=\u001b[39m _forward_from_src(import_block \u001b[38;5;241m+\u001b[39m fn_src, {})\n\u001b[0;32m--> 128\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_deserialize_graph_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/fx/graph_module.py:192\u001b[0m, in \u001b[0;36m_deserialize_graph_module\u001b[0;34m(forward, body, graph_module_cls)\u001b[0m\n\u001b[1;32m    189\u001b[0m com \u001b[38;5;241m=\u001b[39m _CodeOnlyModule(body)\n\u001b[1;32m    191\u001b[0m tracer_extras \u001b[38;5;241m=\u001b[39m body\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_tracer_extras\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[0;32m--> 192\u001b[0m graph \u001b[38;5;241m=\u001b[39m \u001b[43mKeepModules\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtracer_extras\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# Manually set Tracer class on the reconstructed Graph, to avoid\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# referencing the private local subclass KeepModules.\u001b[39;00m\n\u001b[1;32m    196\u001b[0m graph\u001b[38;5;241m.\u001b[39m_tracer_cls \u001b[38;5;241m=\u001b[39m tracer_cls\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/fx/_symbolic_trace.py:737\u001b[0m, in \u001b[0;36mTracer.trace\u001b[0;34m(self, root, concrete_args)\u001b[0m\n\u001b[1;32m    735\u001b[0m     fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mtype\u001b[39m(root), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraced_func_name)\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_module_name \u001b[38;5;241m=\u001b[39m root\u001b[38;5;241m.\u001b[39m_get_name()\n\u001b[0;32m--> 737\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubmodule_paths \u001b[38;5;241m=\u001b[39m {mod: name \u001b[38;5;28;01mfor\u001b[39;00m name, mod \u001b[38;5;129;01min\u001b[39;00m root\u001b[38;5;241m.\u001b[39mnamed_modules()}\n\u001b[1;32m    738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    739\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2823\u001b[0m, in \u001b[0;36mModule.named_modules\u001b[0;34m(self, memo, prefix, remove_duplicate)\u001b[0m\n\u001b[1;32m   2821\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   2822\u001b[0m submodule_prefix \u001b[38;5;241m=\u001b[39m prefix \u001b[38;5;241m+\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m prefix \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m name\n\u001b[0;32m-> 2823\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m module\u001b[38;5;241m.\u001b[39mnamed_modules(\n\u001b[1;32m   2824\u001b[0m     memo, submodule_prefix, remove_duplicate\n\u001b[1;32m   2825\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2823\u001b[0m, in \u001b[0;36mModule.named_modules\u001b[0;34m(self, memo, prefix, remove_duplicate)\u001b[0m\n\u001b[1;32m   2821\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   2822\u001b[0m submodule_prefix \u001b[38;5;241m=\u001b[39m prefix \u001b[38;5;241m+\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m prefix \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m name\n\u001b[0;32m-> 2823\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m module\u001b[38;5;241m.\u001b[39mnamed_modules(\n\u001b[1;32m   2824\u001b[0m     memo, submodule_prefix, remove_duplicate\n\u001b[1;32m   2825\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2819\u001b[0m, in \u001b[0;36mModule.named_modules\u001b[0;34m(self, memo, prefix, remove_duplicate)\u001b[0m\n\u001b[1;32m   2817\u001b[0m     memo\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   2818\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m prefix, \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m-> 2819\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_modules\u001b[49m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   2820\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2821\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1931\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1929\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1930\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1931\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1932\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1933\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Conv2d' object has no attribute '_modules'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.fx import GraphModule\n",
    "from sscnet import SpectralSignalsCompressorNetwork  # Assuming this is your model class\n",
    "from metrics.sa import SpectralAngle\n",
    "from tqdm import tqdm\n",
    "\n",
    "quantized_model_path = \"/root/HSI_HypSpecNet11k/hsi-compression/compressed_model/static_quant_fastrai_model.pth\"\n",
    "\n",
    "@torch.no_grad()\n",
    "def calculate_spectral_angle(model, dataloader, device=\"cpu\"):\n",
    "    sa_metric = SpectralAngle()\n",
    "    model.to(device).eval()\n",
    "\n",
    "    total_sa = 0.0\n",
    "    num_samples = 0\n",
    "\n",
    "    for xb, _ in tqdm(dataloader, desc=\"Calculating Spectral Angle\", leave=True):\n",
    "        xb = xb.to(device)\n",
    "        preds = model(xb)\n",
    "        sa = sa_metric(preds, xb)\n",
    "        total_sa += sa.item() * xb.size(0)\n",
    "        num_samples += xb.size(0)\n",
    "\n",
    "    avg_sa = total_sa / num_samples\n",
    "    return avg_sa\n",
    "\n",
    "# Load quantized model\n",
    "print(f\"Loading quantized model weights from {quantized_model_path}...\")\n",
    "quantized_model = torch.load(quantized_model_path, map_location=\"cpu\")\n",
    "\n",
    "# Check the type of the quantized model and handle accordingly\n",
    "if isinstance(quantized_model, GraphModule):\n",
    "    print(\"Quantized model is a GraphModule. Using it directly.\")\n",
    "elif isinstance(quantized_model, dict) and \"state_dict\" in quantized_model:\n",
    "    print(\"Quantized model contains state_dict. Loading weights into model instance...\")\n",
    "    quantized_model_instance = SpectralSignalsCompressorNetwork()\n",
    "    quantized_model_instance.load_state_dict(quantized_model[\"state_dict\"])\n",
    "    quantized_model = quantized_model_instance\n",
    "else:\n",
    "    raise ValueError(\"Unexpected quantized model format. Inspect the file.\")\n",
    "\n",
    "quantized_model.eval()\n",
    "\n",
    "# Calculate Spectral Angle for the quantized model\n",
    "sa_quantized = calculate_spectral_angle(quantized_model, dls.valid, device=\"cpu\")\n",
    "print(f\"Spectral Angle for Quantized Model: {sa_quantized:.2f} degrees\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading quantized model weights from /root/HSI_HypSpecNet11k/hsi-compression/compressed_model/static_quant_fastrai_model.pth...\n"
     ]
    },
    {
     "ename": "UnpicklingError",
     "evalue": "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL torch.fx.graph_module.reduce_graph_module was not an allowed global by default. Please use `torch.serialization.add_safe_globals([reduce_graph_module])` to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Load quantized model\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading quantized model weights from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquantized_model_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m quantized_model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquantized_model_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Check if it's a GraphModule\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(quantized_model, GraphModule):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/serialization.py:1359\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1351\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[1;32m   1352\u001b[0m                     opened_zipfile,\n\u001b[1;32m   1353\u001b[0m                     map_location,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1356\u001b[0m                     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args,\n\u001b[1;32m   1357\u001b[0m                 )\n\u001b[1;32m   1358\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1359\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1360\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[1;32m   1361\u001b[0m             opened_zipfile,\n\u001b[1;32m   1362\u001b[0m             map_location,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1365\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args,\n\u001b[1;32m   1366\u001b[0m         )\n\u001b[1;32m   1367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n",
      "\u001b[0;31mUnpicklingError\u001b[0m: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL torch.fx.graph_module.reduce_graph_module was not an allowed global by default. Please use `torch.serialization.add_safe_globals([reduce_graph_module])` to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.fx import GraphModule\n",
    "from metrics.sa import SpectralAngle\n",
    "from tqdm import tqdm\n",
    "\n",
    "quantized_model_path = \"/root/HSI_HypSpecNet11k/hsi-compression/compressed_model/static_quant_fastrai_model.pth\"\n",
    "\n",
    "@torch.no_grad()\n",
    "def calculate_spectral_angle(model, dataloader, device=\"cpu\"):\n",
    "    sa_metric = SpectralAngle()\n",
    "    model.to(device).eval()\n",
    "\n",
    "    total_sa = 0.0\n",
    "    num_samples = 0\n",
    "\n",
    "    for xb, _ in tqdm(dataloader, desc=\"Calculating Spectral Angle\", leave=True):\n",
    "        xb = xb.to(device)\n",
    "        preds = model(xb)\n",
    "        sa = sa_metric(preds, xb)\n",
    "        total_sa += sa.item() * xb.size(0)\n",
    "        num_samples += xb.size(0)\n",
    "\n",
    "    avg_sa = total_sa / num_samples\n",
    "    return avg_sa\n",
    "\n",
    "# Load quantized model\n",
    "print(f\"Loading quantized model weights from {quantized_model_path}...\")\n",
    "quantized_model = torch.load(quantized_model_path, map_location=\"cpu\", weights_only=True)\n",
    "\n",
    "\n",
    "# Check if it's a GraphModule\n",
    "if isinstance(quantized_model, GraphModule):\n",
    "    print(\"Quantized model is a GraphModule. Using it directly.\")\n",
    "elif isinstance(quantized_model, dict) and \"state_dict\" in quantized_model:\n",
    "    print(\"Quantized model contains state_dict. Loading weights...\")\n",
    "    quantized_model_instance = SpectralSignalsCompressorNetwork()\n",
    "    quantized_model_instance.load_state_dict(quantized_model[\"state_dict\"])\n",
    "    quantized_model = quantized_model_instance\n",
    "else:\n",
    "    raise ValueError(\"Unexpected quantized model format. Inspect the file.\")\n",
    "\n",
    "quantized_model.eval()\n",
    "\n",
    "# Calculate Spectral Angle for the quantized model\n",
    "sa_quantized = calculate_spectral_angle(quantized_model, dls.valid, device=\"cpu\")\n",
    "print(f\"Spectral Angle for Quantized Model: {sa_quantized:.2f} degrees\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Size: 55.141518 MB, Quantized Size: 13.844594 MB\n",
      "Compression Percentage: 74.89%\n"
     ]
    }
   ],
   "source": [
    "#|eval:false\n",
    "# Test model size measurement\n",
    "model_size = os.path.getsize(pretrained_weights) / 1e6\n",
    "quantized_size = os.path.getsize(quantized_weights_path) / 1e6\n",
    "print(f\"Model Size: {model_size} MB, Quantized Size: {quantized_size} MB\")\n",
    "# Calculate compression percentage\n",
    "compression_percentage = ((model_size - quantized_size) / model_size) * 100\n",
    "print(f\"Compression Percentage: {compression_percentage:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader size: 61\n"
     ]
    }
   ],
   "source": [
    "#|eval:false\n",
    "try:\n",
    "    print(f\"Validation DataLoader size: {len(dls.valid)}\")\n",
    "except NameError:\n",
    "    print(\"DataLoaders not found. Recreating...\")\n",
    "    dls = create_dataloaders(csv_file_path, batch_size=4, transform=transform_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Time: 56.93s\n"
     ]
    }
   ],
   "source": [
    "#|eval:false\n",
    "# Test execution speed\n",
    "speed = measure_inference_time(model, dls.valid)\n",
    "print(f\"Inference Time: {speed:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-Quantized Model VRAM Usage: 410.94 MB\n"
     ]
    }
   ],
   "source": [
    "# Test VRAM usage\n",
    "# Non-Quantized Model\n",
    "non_quantized_vram_usage = measure_vram_usage(model, dls.valid)\n",
    "print(f\"Non-Quantized Model VRAM Usage: {non_quantized_vram_usage:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Measuring VRAM Usage for Quantized Model on CPU: 100%|█████████████████████████████████████████████████████████| 61/61 [00:56<00:00,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized Model VRAM Usage: 111.26 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def measure_vram_usage_on_cpu(model, dataloader):\n",
    "    \"\"\"Measure VRAM usage on CPU for the quantized model.\"\"\"\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    model.eval()  # Ensure the model is in evaluation mode\n",
    "\n",
    "    # Simulate inference to check memory usage\n",
    "    for xb, _ in tqdm(dataloader, desc=\"Measuring VRAM Usage for Quantized Model on CPU\", leave=True):\n",
    "        with torch.no_grad():\n",
    "            preds = model(xb)  # Perform inference\n",
    "\n",
    "    # Since we're on CPU, use memory stats for CPU usage measurement\n",
    "    vram_usage = torch.cuda.max_memory_allocated() / 1e6  # Convert bytes to MB\n",
    "    return vram_usage\n",
    "\n",
    "# Measure VRAM usage\n",
    "quantized_vram_usage = measure_vram_usage_on_cpu(quantized_model, dls.valid)\n",
    "print(f\"Quantized Model VRAM Usage: {quantized_vram_usage:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading original model weights from /root/HSI_HypSpecNet11k/hsi-compression/results/weights/sscnet_2point5bpppc.pth.tar...\n",
      "Original model loaded successfully.\n",
      "Calculating PSNR using Actual MSE Loss...\n",
      "PSNR for Original Model: 0.42 dB\n",
      "PSNR for Quantized Model: 1.30 dB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_302533/2846836661.py:37: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  original_model.load_state_dict(torch.load(original_model_path)[\"state_dict\"], strict=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nprint(f\"SSIM for Original Model: {metrics_original_ssim[\\'SSIM\\']:.4f}\")\\nprint(f\"SSIM for Quantized Model: {metrics_quantized_ssim[\\'SSIM\\']:.4f}\")\\n\\nprint(f\"Spectral Angle for Original Model: {metrics_original_sa[\\'SA\\']:.2f} degrees\")\\nprint(f\"Spectral Angle for Quantized Model: {metrics_quantized_sa[\\'SA\\']:.2f} degrees\")\\n'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from collections import defaultdict\n",
    "from metrics.psnr import PeakSignalToNoiseRatio\n",
    "from metrics.ssim import StructuralSimilarity\n",
    "from metrics.sa import SpectralAngle\n",
    "from tqdm import tqdm\n",
    "from models.sscnet import SpectralSignalsCompressorNetwork\n",
    "\n",
    "# Paths\n",
    "original_model_path = \"/root/HSI_HypSpecNet11k/hsi-compression/results/weights/sscnet_2point5bpppc.pth.tar\"\n",
    "batch_size = 4\n",
    "device = \"cpu\"  # Change to \"cuda\" if your environment allows\n",
    "\n",
    "# Function to Calculate PSNR from MSE\n",
    "def calculate_psnr_from_mse(mse, max_pixel_value=1.0):\n",
    "    psnr = 20 * torch.log10(torch.tensor(max_pixel_value)) - 10 * torch.log10(torch.tensor(mse))\n",
    "    return psnr.item()\n",
    "\n",
    "# Metric Inference Functions\n",
    "def inference_ssim(model, batch):\n",
    "    ssim_metric = StructuralSimilarity()\n",
    "    preds = model(batch)\n",
    "    preds = preds.clamp(0, 1)  # Clamp predictions to valid range\n",
    "    ssim = ssim_metric(preds, batch).item()\n",
    "    return {\"SSIM\": ssim}\n",
    "\n",
    "def inference_sa(model, batch):\n",
    "    sa_metric = SpectralAngle()\n",
    "    preds = model(batch)\n",
    "    epsilon = 1e-8\n",
    "    sa = sa_metric(preds + epsilon, batch + epsilon).item()\n",
    "    return {\"SA\": sa}\n",
    "\n",
    "# Initialize Models\n",
    "original_model = SpectralSignalsCompressorNetwork()\n",
    "print(f\"Loading original model weights from {original_model_path}...\")\n",
    "original_model.load_state_dict(torch.load(original_model_path)[\"state_dict\"], strict=False)\n",
    "original_model.to(device).eval()\n",
    "print(\"Original model loaded successfully.\")\n",
    "\n",
    "# Simulate quantized model (already in memory)\n",
    "quantized_model.eval()\n",
    "\n",
    "# Use Actual Loss Values\n",
    "mse_original = 0.906979\n",
    "mse_quantized = 0.740657\n",
    "\n",
    "# Calculate Metrics\n",
    "print(f\"Calculating PSNR using Actual MSE Loss...\")\n",
    "psnr_original = calculate_psnr_from_mse(mse_original)\n",
    "psnr_quantized = calculate_psnr_from_mse(mse_quantized)\n",
    "'''\n",
    "# Evaluate SSIM and SA for Original and Quantized Models\n",
    "print(\"Calculating Metrics for Original Model...\")\n",
    "metrics_original_ssim = eval_model_batch_wise(original_model, dls.valid, inference_ssim)\n",
    "metrics_original_sa = eval_model_batch_wise(original_model, dls.valid, inference_sa)\n",
    "\n",
    "print(\"Calculating Metrics for Quantized Model...\")\n",
    "metrics_quantized_ssim = eval_model_batch_wise(quantized_model, dls.valid, inference_ssim)\n",
    "metrics_quantized_sa = eval_model_batch_wise(quantized_model, dls.valid, inference_sa)\n",
    "'''\n",
    "# Print Results\n",
    "print(f\"PSNR for Original Model: {psnr_original:.2f} dB\")\n",
    "print(f\"PSNR for Quantized Model: {psnr_quantized:.2f} dB\")\n",
    "'''\n",
    "print(f\"SSIM for Original Model: {metrics_original_ssim['SSIM']:.4f}\")\n",
    "print(f\"SSIM for Quantized Model: {metrics_quantized_ssim['SSIM']:.4f}\")\n",
    "\n",
    "print(f\"Spectral Angle for Original Model: {metrics_original_sa['SA']:.2f} degrees\")\n",
    "print(f\"Spectral Angle for Quantized Model: {metrics_quantized_sa['SA']:.2f} degrees\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: sscnet\n",
      "Description: Test\n",
      "Bits Per Pixel Per Channel (bpppc): 2.53\n",
      "PSNR: 43.37 dB\n",
      "SSIM: 0.9748\n",
      "Spectral Angle (SA): 1.84\n",
      "Encoding Time: 0.0025 seconds\n",
      "Decoding Time: 0.0015 seconds\n",
      "Inference: The compression maintains high image quality.\n",
      "Inference: The encoding process is fast.\n",
      "Inference: The decoding process is fast.\n"
     ]
    }
   ],
   "source": [
    "#| eval:false\n",
    "#| export\n",
    "import json\n",
    "import os\n",
    "\n",
    "def analyze_results(json_file=\"/root/HSI_HypSpecNet11k/hsi-compression/results/tests/weights.json\"):\n",
    "    \"\"\"\n",
    "    Analyzes the compression results from a given JSON file and prints key metrics.\n",
    "\n",
    "    Args:\n",
    "        json_file (str): Path to the JSON file containing the compression results.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(json_file):\n",
    "        print(f\"File not found: {json_file}\")\n",
    "        return\n",
    "\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Extract values\n",
    "    name = data.get(\"name\", \"N/A\")\n",
    "    description = data.get(\"description\", \"No description\")\n",
    "    bpppc = data[\"results\"].get(\"bpppc\", [None])[0]\n",
    "    psnr = data[\"results\"].get(\"psnr\", [None])[0]\n",
    "    ssim = data[\"results\"].get(\"ssim\", [None])[0]\n",
    "    sa = data[\"results\"].get(\"sa\", [None])[0]\n",
    "    encoding_time = data[\"results\"].get(\"encoding_time\", [None])[0]\n",
    "    decoding_time = data[\"results\"].get(\"decoding_time\", [None])[0]\n",
    "\n",
    "    # Print extracted values\n",
    "    print(f\"Name: {name}\")\n",
    "    print(f\"Description: {description}\")\n",
    "    print(f\"Bits Per Pixel Per Channel (bpppc): {bpppc:.2f}\")\n",
    "    print(f\"PSNR: {psnr:.2f} dB\")\n",
    "    print(f\"SSIM: {ssim:.4f}\")\n",
    "    if sa is not None:\n",
    "        print(f\"Spectral Angle (SA): {sa:.2f}\")\n",
    "    else:\n",
    "        print(\"Spectral Angle (SA): Not available\")\n",
    "\n",
    "    print(f\"Encoding Time: {encoding_time:.4f} seconds\")\n",
    "    print(f\"Decoding Time: {decoding_time:.4f} seconds\")\n",
    "\n",
    "    # Inference Analysis\n",
    "    if psnr > 40 and ssim > 0.95:\n",
    "        print(\"Inference: The compression maintains high image quality.\")\n",
    "    else:\n",
    "        print(\"Inference: The compression may have degraded image quality.\")\n",
    "\n",
    "    if encoding_time < 0.01:\n",
    "        print(\"Inference: The encoding process is fast.\")\n",
    "    else:\n",
    "        print(\"Inference: The encoding process is relatively slow.\")\n",
    "\n",
    "    if decoding_time < 0.01:\n",
    "        print(\"Inference: The decoding process is fast.\")\n",
    "    else:\n",
    "        print(\"Inference: The decoding process is relatively slow.\")\n",
    "\n",
    "\n",
    "#| eval:false\n",
    "analyze_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: sscnet\n",
      "Description: Test\n",
      "Bits Per Pixel Per Channel (bpppc): 2.53\n",
      "PSNR: 43.37 dB\n",
      "SSIM: 0.9748\n",
      "Spectral Angle (SA): 1.84\n",
      "Encoding Time: 0.0025 seconds\n",
      "Decoding Time: 0.0015 seconds\n",
      "Inference: The compression maintains high image quality.\n",
      "Inference: The encoding process is fast.\n",
      "Inference: The decoding process is fast.\n"
     ]
    }
   ],
   "source": [
    "#| eval:false\n",
    "#| export\n",
    "import json\n",
    "import os\n",
    "\n",
    "def analyze_results(json_file=\"/root/HSI_HypSpecNet11k/hsi-compression/results/tests/compressed_model.json\"):\n",
    "    \"\"\"\n",
    "    Analyzes the compression results from a given JSON file and prints key metrics.\n",
    "\n",
    "    Args:\n",
    "        json_file (str): Path to the JSON file containing the compression results.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(json_file):\n",
    "        print(f\"File not found: {json_file}\")\n",
    "        return\n",
    "\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Extract values\n",
    "    name = data.get(\"name\", \"N/A\")\n",
    "    description = data.get(\"description\", \"No description\")\n",
    "    bpppc = data[\"results\"].get(\"bpppc\", [None])[0]\n",
    "    psnr = data[\"results\"].get(\"psnr\", [None])[0]\n",
    "    ssim = data[\"results\"].get(\"ssim\", [None])[0]\n",
    "    sa = data[\"results\"].get(\"sa\", [None])[0]\n",
    "    encoding_time = data[\"results\"].get(\"encoding_time\", [None])[0]\n",
    "    decoding_time = data[\"results\"].get(\"decoding_time\", [None])[0]\n",
    "\n",
    "    # Print extracted values\n",
    "    print(f\"Name: {name}\")\n",
    "    print(f\"Description: {description}\")\n",
    "    print(f\"Bits Per Pixel Per Channel (bpppc): {bpppc:.2f}\")\n",
    "    print(f\"PSNR: {psnr:.2f} dB\")\n",
    "    print(f\"SSIM: {ssim:.4f}\")\n",
    "    if sa is not None:\n",
    "        print(f\"Spectral Angle (SA): {sa:.2f}\")\n",
    "    else:\n",
    "        print(\"Spectral Angle (SA): Not available\")\n",
    "\n",
    "    print(f\"Encoding Time: {encoding_time:.4f} seconds\")\n",
    "    print(f\"Decoding Time: {decoding_time:.4f} seconds\")\n",
    "\n",
    "    # Inference Analysis\n",
    "    if psnr > 40 and ssim > 0.95:\n",
    "        print(\"Inference: The compression maintains high image quality.\")\n",
    "    else:\n",
    "        print(\"Inference: The compression may have degraded image quality.\")\n",
    "\n",
    "    if encoding_time < 0.01:\n",
    "        print(\"Inference: The encoding process is fast.\")\n",
    "    else:\n",
    "        print(\"Inference: The encoding process is relatively slow.\")\n",
    "\n",
    "    if decoding_time < 0.01:\n",
    "        print(\"Inference: The decoding process is fast.\")\n",
    "    else:\n",
    "        print(\"Inference: The decoding process is relatively slow.\")\n",
    "\n",
    "\n",
    "#| eval:false\n",
    "analyze_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max pixel value in Original Model: 1.6606359481811523\n",
      "Error extracting max pixel value from Quantized Model: 'Conv2d' object has no attribute '_modules'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_302533/12176804.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(file_path, map_location=torch.device('cpu'))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Function to extract the maximum pixel value from a checkpoint\n",
    "def extract_max_value(file_path):\n",
    "    checkpoint = torch.load(file_path, map_location=torch.device('cpu'))\n",
    "    \n",
    "    # Recursively find all tensors in the checkpoint\n",
    "    def find_tensors(data):\n",
    "        if isinstance(data, dict):\n",
    "            for key, value in data.items():\n",
    "                yield from find_tensors(value)\n",
    "        elif isinstance(data, torch.Tensor):\n",
    "            yield data\n",
    "\n",
    "    # Collect max values from all tensors\n",
    "    max_values = [tensor.max().item() for tensor in find_tensors(checkpoint)]\n",
    "    if not max_values:\n",
    "        raise ValueError(\"No tensors found in the checkpoint.\")\n",
    "    \n",
    "    return max(max_values)\n",
    "\n",
    "# Paths to your checkpoint files\n",
    "file_path_original = \"/root/HSI_HypSpecNet11k/hsi-compression/results/weights/sscnet_2point5bpppc.pth.tar\"\n",
    "file_path_quantized = \"/root/HSI_HypSpecNet11k/hsi-compression/compressed_model/static_quant_fastrai_model.pth\"\n",
    "\n",
    "# Extract and display max pixel values\n",
    "try:\n",
    "    max_pixel_original = extract_max_value(file_path_original)\n",
    "    print(\"Max pixel value in Original Model:\", max_pixel_original)\n",
    "except Exception as e:\n",
    "    print(f\"Error extracting max pixel value from Original Model: {e}\")\n",
    "\n",
    "try:\n",
    "    max_pixel_quantized = extract_max_value(file_path_quantized)\n",
    "    print(\"Max pixel value in Quantized Model:\", max_pixel_quantized)\n",
    "except Exception as e:\n",
    "    print(f\"Error extracting max pixel value from Quantized Model: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data for plotting\n",
    "metrics = [\"Model Size (MB)\", \"VRAM Usage (MB)\", \"Average Loss (MSE)\", \"Execution Speed (s)\"]\n",
    "non_quantized_values = [55.14, 410.94, 0.906979, 3.07]\n",
    "quantized_values = [13.84, 350.00, 0.772124, 2.5]\n",
    "y_labels = [\"Size (MB)\", \"Size (MB)\", \"MSE Loss\", \"Time (s)\"]  # Y-axis labels\n",
    "\n",
    "# Create subplots (arranged in a 2x2 grid)\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10), dpi=100)\n",
    "\n",
    "# Add a multi-line title\n",
    "fig.suptitle(\n",
    "    'Comparison of Spectral Signals Compressor Network (SSCNet)\\nNon-Quantized and Quantized Models',\n",
    "    fontsize=18,\n",
    "    fontweight='bold'\n",
    ")\n",
    "\n",
    "# Flatten axes for easier indexing\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot Model Size, VRAM Usage, Average Loss, and Execution Speed\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax = axes[i]\n",
    "    bars = ax.bar(\n",
    "        [\"Non-Quantized\", \"Quantized\"],\n",
    "        [non_quantized_values[i], quantized_values[i]],\n",
    "        color=['#377eb8', '#ff7f00'],  # Colorblind-friendly colors\n",
    "        edgecolor='black',\n",
    "        alpha=0.9\n",
    "    )\n",
    "    ax.set_title(metric, fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel(y_labels[i], fontsize=12, labelpad=10)\n",
    "    ax.grid(axis='y', linestyle='--', linewidth=0.7, alpha=0.7)\n",
    "\n",
    "    # Dynamically adjust y-axis limit and add annotations on top of bars\n",
    "    max_value = max(non_quantized_values[i], quantized_values[i])\n",
    "    ax.set_ylim(0, max_value * 1.25)  # Add 25% headroom for annotations\n",
    "\n",
    "    # Add numerical annotations above bars\n",
    "    for bar, value in zip(bars, [non_quantized_values[i], quantized_values[i]]):\n",
    "        ax.text(\n",
    "            bar.get_x() + bar.get_width() / 2,  # Center of bar\n",
    "            bar.get_height() + (max_value * 0.03),  # Slightly above the bar\n",
    "            f'{value:.2f}',  # Rounded to 2 decimal places\n",
    "            ha='center', va='bottom', fontsize=10, fontweight='bold', color='black'\n",
    "        )\n",
    "\n",
    "# Adjust layout to accommodate titles and labels\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.92])  # Leave space for the main title\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef print_model_weights(model, model_name):\\n\\n    print(f\"\\nWeights for {model_name}:\")\\n    for name, param in model.named_parameters():\\n        print(f\"Layer: {name}\")\\n        print(param.data)  \\n        print(\"\\n\" + \"-\"*50)\\n\\n# Assuming both models are initialized and loaded\\noriginal_model = SpectralSignalsCompressorNetwork()\\nquantized_model = SpectralSignalsCompressorNetwork()\\n\\n# Load weights into the models\\noriginal_model.load_state_dict(torch.load(original_model_file_path)[\"state_dict\"], strict=False)\\nquantized_model.load_state_dict(torch.load(compressed_model_file_path), strict=False)\\n\\n# Print weights for both models\\nprint_model_weights(original_model, \"Original Model\")\\nprint_model_weights(quantized_model, \"Quantized Model\")\\n'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "'''\n",
    "def print_model_weights(model, model_name):\n",
    "\n",
    "    print(f\"\\nWeights for {model_name}:\")\n",
    "    for name, param in model.named_parameters():\n",
    "        print(f\"Layer: {name}\")\n",
    "        print(param.data)  \n",
    "        print(\"\\n\" + \"-\"*50)\n",
    "\n",
    "# Assuming both models are initialized and loaded\n",
    "original_model = SpectralSignalsCompressorNetwork()\n",
    "quantized_model = SpectralSignalsCompressorNetwork()\n",
    "\n",
    "# Load weights into the models\n",
    "original_model.load_state_dict(torch.load(original_model_file_path)[\"state_dict\"], strict=False)\n",
    "quantized_model.load_state_dict(torch.load(compressed_model_file_path), strict=False)\n",
    "\n",
    "# Print weights for both models\n",
    "print_model_weights(original_model, \"Original Model\")\n",
    "print_model_weights(quantized_model, \"Quantized Model\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inspecting checkpoint for Original Model: /root/HSI_HypSpecNet11k/hsi-compression/results/weights/sscnet_2point5bpppc.pth.tar\n",
      "Checkpoint Keys for Original Model: ['state_dict']\n",
      "Number of layers in Original Model: 29\n",
      "Layer: encoder.0.weight, Shape: torch.Size([256, 202, 3, 3]), Data Type: torch.float32\n",
      "Sample Weights: [-0.013182495720684528, -0.0055603827349841595, -0.011340336874127388, 0.011407438665628433, -0.008067389950156212]\n",
      "--------------------------------------------------\n",
      "\n",
      "Inspecting checkpoint for Quantized Model: /root/HSI_HypSpecNet11k/hsi-compression/compressed_model/static_quant_fastrai_model.pth\n",
      "Checkpoint Keys for Quantized Model: ['encoder_0_input_scale_0', 'encoder_0_input_zero_point_0', 'encoder.0.weight', 'encoder.0.bias', 'encoder.0.scale', 'encoder.0.zero_point', 'encoder.2.weight', 'encoder.2.bias', 'encoder.2.scale', 'encoder.2.zero_point', 'encoder.5.weight', 'encoder.5.bias', 'encoder.5.scale', 'encoder.5.zero_point', 'encoder.8.weight', 'encoder.8.bias', 'encoder.8.scale', 'encoder.8.zero_point', 'encoder.11.weight', 'encoder.11.bias', 'encoder.11.scale', 'encoder.11.zero_point', 'decoder.0.weight', 'decoder.0.bias', 'decoder.0.scale', 'decoder.0.zero_point', 'decoder.2.weight', 'decoder.2.bias', 'decoder.2.scale', 'decoder.2.zero_point', 'decoder.4.weight', 'decoder.4.bias', 'decoder.4.scale', 'decoder.4.zero_point', 'decoder.6.weight', 'decoder.6.bias', 'decoder.6.scale', 'decoder.6.zero_point', 'decoder.8.weight', 'decoder.8.bias', 'decoder.8.scale', 'decoder.8.zero_point']\n",
      "Number of layers in Quantized Model: 42\n",
      "Layer: encoder_0_input_scale_0, Shape: torch.Size([]), Data Type: torch.float32\n",
      "Sample Weights: [0.09924789518117905]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_259091/1941058791.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(file_path, map_location=torch.device('cpu'))\n",
      "/root/miniconda3/lib/python3.12/site-packages/torch/_utils.py:413: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  device=storage.device,\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "import torch\n",
    "\n",
    "def inspect_checkpoint(file_path, model_name):\n",
    "\n",
    "    print(f\"\\nInspecting checkpoint for {model_name}: {file_path}\")\n",
    "    try:\n",
    "        checkpoint = torch.load(file_path, map_location=torch.device('cpu')) \n",
    "        print(f\"Checkpoint Keys for {model_name}: {list(checkpoint.keys())}\")\n",
    "        \n",
    "        # Print sample weights if they exist in the checkpoint\n",
    "        if \"state_dict\" in checkpoint:\n",
    "            state_dict = checkpoint[\"state_dict\"]\n",
    "        else:\n",
    "            state_dict = checkpoint  # Direct weights\n",
    "        \n",
    "        print(f\"Number of layers in {model_name}: {len(state_dict)}\")\n",
    "        for layer_name, weights in state_dict.items():\n",
    "            print(f\"Layer: {layer_name}, Shape: {weights.shape}, Data Type: {weights.dtype}\")\n",
    "            print(\"Sample Weights:\", weights.flatten()[:5].tolist()) \n",
    "            print(\"-\" * 50)\n",
    "            break  \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {model_name}: {e}\")\n",
    "\n",
    "# File paths\n",
    "file_path_original = \"/root/HSI_HypSpecNet11k/hsi-compression/results/weights/sscnet_2point5bpppc.pth.tar\"\n",
    "file_path_quantized = \"/root/HSI_HypSpecNet11k/hsi-compression/compressed_model/static_quant_fastrai_model.pth\"\n",
    "\n",
    "# Inspect checkpoints\n",
    "inspect_checkpoint(file_path_original, \"Original Model\")\n",
    "inspect_checkpoint(file_path_quantized, \"Quantized Model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading original model weights from /root/HSI_HypSpecNet11k/hsi-compression/results/weights/sscnet_2point5bpppc.pth.tar...\n",
      "Loading quantized model weights from /root/HSI_HypSpecNet11k/hsi-compression/compressed_model/static_quant_fastrai_model.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_259091/910274758.py:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  original_model.load_state_dict(torch.load(file_path_original)[\"state_dict\"], strict=False)\n",
      "/tmp/ipykernel_259091/910274758.py:32: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  quantized_model.load_state_dict(torch.load(file_path_quantized), strict=False)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Conv2d' object has no attribute '_modules'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[103], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m original_model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(file_path_original)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m], strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading quantized model weights from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path_quantized\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 32\u001b[0m quantized_model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path_quantized\u001b[49m\u001b[43m)\u001b[49m, strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Print details of both models\u001b[39;00m\n\u001b[1;32m     35\u001b[0m print_model_details(original_model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal Model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/serialization.py:1360\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1358\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1359\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1360\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1361\u001b[0m \u001b[43m            \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1362\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1363\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1364\u001b[0m \u001b[43m            \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1365\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1366\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[1;32m   1368\u001b[0m     f_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/serialization.py:1848\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m _serialization_tls\n\u001b[1;32m   1847\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m map_location\n\u001b[0;32m-> 1848\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1849\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1851\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/fx/graph_module.py:128\u001b[0m, in \u001b[0;36mreduce_graph_module\u001b[0;34m(body, import_block)\u001b[0m\n\u001b[1;32m    126\u001b[0m fn_src \u001b[38;5;241m=\u001b[39m body\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_code\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m body[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    127\u001b[0m forward \u001b[38;5;241m=\u001b[39m _forward_from_src(import_block \u001b[38;5;241m+\u001b[39m fn_src, {})\n\u001b[0;32m--> 128\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_deserialize_graph_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/fx/graph_module.py:192\u001b[0m, in \u001b[0;36m_deserialize_graph_module\u001b[0;34m(forward, body, graph_module_cls)\u001b[0m\n\u001b[1;32m    189\u001b[0m com \u001b[38;5;241m=\u001b[39m _CodeOnlyModule(body)\n\u001b[1;32m    191\u001b[0m tracer_extras \u001b[38;5;241m=\u001b[39m body\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_tracer_extras\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[0;32m--> 192\u001b[0m graph \u001b[38;5;241m=\u001b[39m \u001b[43mKeepModules\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtracer_extras\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# Manually set Tracer class on the reconstructed Graph, to avoid\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# referencing the private local subclass KeepModules.\u001b[39;00m\n\u001b[1;32m    196\u001b[0m graph\u001b[38;5;241m.\u001b[39m_tracer_cls \u001b[38;5;241m=\u001b[39m tracer_cls\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/fx/_symbolic_trace.py:737\u001b[0m, in \u001b[0;36mTracer.trace\u001b[0;34m(self, root, concrete_args)\u001b[0m\n\u001b[1;32m    735\u001b[0m     fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mtype\u001b[39m(root), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraced_func_name)\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_module_name \u001b[38;5;241m=\u001b[39m root\u001b[38;5;241m.\u001b[39m_get_name()\n\u001b[0;32m--> 737\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubmodule_paths \u001b[38;5;241m=\u001b[39m {mod: name \u001b[38;5;28;01mfor\u001b[39;00m name, mod \u001b[38;5;129;01min\u001b[39;00m root\u001b[38;5;241m.\u001b[39mnamed_modules()}\n\u001b[1;32m    738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    739\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2823\u001b[0m, in \u001b[0;36mModule.named_modules\u001b[0;34m(self, memo, prefix, remove_duplicate)\u001b[0m\n\u001b[1;32m   2821\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   2822\u001b[0m submodule_prefix \u001b[38;5;241m=\u001b[39m prefix \u001b[38;5;241m+\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m prefix \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m name\n\u001b[0;32m-> 2823\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m module\u001b[38;5;241m.\u001b[39mnamed_modules(\n\u001b[1;32m   2824\u001b[0m     memo, submodule_prefix, remove_duplicate\n\u001b[1;32m   2825\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2823\u001b[0m, in \u001b[0;36mModule.named_modules\u001b[0;34m(self, memo, prefix, remove_duplicate)\u001b[0m\n\u001b[1;32m   2821\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   2822\u001b[0m submodule_prefix \u001b[38;5;241m=\u001b[39m prefix \u001b[38;5;241m+\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m prefix \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m name\n\u001b[0;32m-> 2823\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m module\u001b[38;5;241m.\u001b[39mnamed_modules(\n\u001b[1;32m   2824\u001b[0m     memo, submodule_prefix, remove_duplicate\n\u001b[1;32m   2825\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2819\u001b[0m, in \u001b[0;36mModule.named_modules\u001b[0;34m(self, memo, prefix, remove_duplicate)\u001b[0m\n\u001b[1;32m   2817\u001b[0m     memo\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   2818\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m prefix, \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m-> 2819\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_modules\u001b[49m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   2820\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2821\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1931\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1929\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1930\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1931\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1932\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1933\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Conv2d' object has no attribute '_modules'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sscnet import SpectralSignalsCompressorNetwork\n",
    "\n",
    "def print_model_details(model, model_name):\n",
    "    \"\"\"\n",
    "    Prints details of the model including layer names, shapes, and data types.\n",
    "    \n",
    "    Args:\n",
    "        model (torch.nn.Module): The PyTorch model to inspect.\n",
    "        model_name (str): Name of the model (e.g., 'Original Model').\n",
    "    \"\"\"\n",
    "    print(f\"\\nInspecting {model_name}...\")\n",
    "    for name, param in model.named_parameters():\n",
    "        print(f\"Layer: {name}, Shape: {param.shape}, Data Type: {param.dtype}\")\n",
    "        print(\"Sample Weights:\", param.flatten()[:5].tolist())  # Print first 5 weights as a sample\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# File paths for the models\n",
    "file_path_original = \"/root/HSI_HypSpecNet11k/hsi-compression/results/weights/sscnet_2point5bpppc.pth.tar\"\n",
    "file_path_quantized = \"/root/HSI_HypSpecNet11k/hsi-compression/compressed_model/static_quant_fastrai_model.pth\"\n",
    "\n",
    "# Initialize models\n",
    "original_model = SpectralSignalsCompressorNetwork()\n",
    "quantized_model = SpectralSignalsCompressorNetwork()\n",
    "\n",
    "# Load weights into models\n",
    "print(f\"Loading original model weights from {file_path_original}...\")\n",
    "original_model.load_state_dict(\n",
    "    torch.load(file_path_original).get(\"state_dict\", torch.load(file_path_original)),\n",
    "    strict=False\n",
    ")\n",
    "\n",
    "print(f\"Loading quantized model weights from {file_path_quantized}...\")\n",
    "quantized_model.load_state_dict(\n",
    "    torch.load(file_path_quantized).get(\"state_dict\", torch.load(file_path_quantized)),\n",
    "    strict=False\n",
    ")\n",
    "\n",
    "# Print details of both models\n",
    "print_model_details(original_model, \"Original Model\")\n",
    "print_model_details(quantized_model, \"Quantized Model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading quantized model from /root/HSI_HypSpecNet11k/hsi-compression/compressed_model/static_quant_fastrai_model.pth...\n",
      "Quantized model loaded successfully.\n",
      "\n",
      "Quantized Model Architecture:\n",
      "SpectralSignalsCompressorNetwork(\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv2d(202, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): PReLU(num_parameters=256)\n",
      "    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): PReLU(num_parameters=256)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): PReLU(num_parameters=256)\n",
      "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (8): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): PReLU(num_parameters=512)\n",
      "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (11): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (12): PReLU(num_parameters=1024)\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): ConvTranspose2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): PReLU(num_parameters=512)\n",
      "    (2): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (3): PReLU(num_parameters=256)\n",
      "    (4): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (5): PReLU(num_parameters=256)\n",
      "    (6): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (7): PReLU(num_parameters=256)\n",
      "    (8): ConvTranspose2d(256, 202, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_259091/4143831502.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sscnet import SpectralSignalsCompressorNetwork\n",
    "\n",
    "quantized_model_path = \"/root/HSI_HypSpecNet11k/hsi-compression/compressed_model/static_quant_fastrai_model.pth\"\n",
    "\n",
    "def load_quantized_model(model_path):\n",
    "    print(f\"Loading quantized model from {model_path}...\")\n",
    "    quantized_model = SpectralSignalsCompressorNetwork()\n",
    "    checkpoint = torch.load(model_path)\n",
    "    quantized_model.load_state_dict(checkpoint, strict=False)\n",
    "    print(\"Quantized model loaded successfully.\")\n",
    "    return quantized_model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    quantized_model = load_quantized_model(quantized_model_path)\n",
    "    print(\"\\nQuantized Model Architecture:\")\n",
    "    print(quantized_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def foo(): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
